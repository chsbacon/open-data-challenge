{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, LeakyReLU, Input, concatenate\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.regularizers import l1_l2\n",
    "from livelossplot import PlotLossesKeras\n",
    "\n",
    "from glob import glob\n",
    "from os.path import join\n",
    "\n",
    "from scipy.stats import zscore\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_path = join('..','data','weather','precip_temp.csv')\n",
    "weather_df = pd.read_csv(weather_path)\n",
    "clients_path = join('..','data','wifi','**','Clients per day.csv')\n",
    "clients_df = pd.concat(map(lambda csv: pd.read_csv(csv, parse_dates=[0]),\n",
    "                sorted(glob(clients_path),reverse=True)), ignore_index=True)\n",
    "sessions_path = join('..','data','wifi','**','Number of sessions over time.csv')\n",
    "sessions_df = pd.concat(map(lambda csv: pd.read_csv(csv, parse_dates=[0]),\n",
    "                sorted(glob(sessions_path), reverse=True)), ignore_index=True)\n",
    "usage_path = join('..','data','wifi','**','Usage over time.csv')\n",
    "usage_df = pd.concat(map(lambda csv: pd.read_csv(csv, parse_dates=[0]),\n",
    "                sorted(glob(usage_path), reverse=True)), ignore_index=True)\n",
    "# Interpolate zeros in usage and sessions data\n",
    "usage_df.loc[usage_df['Total (B)'] == 0, 'Total (B)'] = np.NaN\n",
    "usage_df['Total (B)'] = usage_df['Total (B)'].interpolate()\n",
    "# Remove implausibly small values\n",
    "usage_df['filtered Total (B)'] = usage_df['Total (B)']\n",
    "usage_df.loc[usage_df['filtered Total (B)'] < 1000, 'filtered Total (B)'] = np.NaN\n",
    "usage_df['filtered Total (B)'] = usage_df['filtered Total (B)'].interpolate()\n",
    "\n",
    "# Remove redundant 06-30-17 data point\n",
    "usage_df.drop(0, inplace=True)\n",
    "sessions_df.loc[sessions_df['# Sessions'] == 0, '# Sessions'] = np.NaN\n",
    "sessions_df['# Sessions'] = sessions_df['# Sessions'].interpolate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add day of the week as a one-hot\n",
    "weekday_labels = []\n",
    "for d in range(7):\n",
    "    label = 'weekday-%i' % d\n",
    "    weekday_labels.append(label)\n",
    "    clients_df[label] = [int(dt.weekday()==d) for dt in clients_df['Time']]\n",
    "    \n",
    "# Put wifi date in the same format as it is in the weather data\n",
    "clients_df['Date'] = [dt.strftime('%Y-%m-%d') for dt in clients_df['Time']]\n",
    "clients_df['Year %'] = [dt.timetuple()[7] / 365 for dt in clients_df['Time']]\n",
    "sessions_df['Date'] = [dt.strftime('%Y-%m-%d') for dt in sessions_df['Time']]\n",
    "usage_df['Date'] = [dt.strftime('%Y-%m-%d') for dt in usage_df['Time']]\n",
    "usage_df['Hour'] = [dt.hour for dt in usage_df['Time']]\n",
    "\n",
    "\n",
    "all_data = clients_df.merge(weather_df, left_on='Date', right_on='DATE') \\\n",
    "    .merge(sessions_df, left_on='Date', right_on='Date')\n",
    "\n",
    "# Put 4-hour chunks together into rows by day\n",
    "usage_labels = set()\n",
    "val_usage_labels = set()\n",
    "for index, row in usage_df.iterrows():\n",
    "    total_label = 'total-%i' % row['Hour']\n",
    "    val_label = 'val-total-%i' % row['Hour']\n",
    "    all_data.loc[all_data['DATE'] == row['Date'], total_label] \\\n",
    "        = row['filtered Total (B)']\n",
    "    all_data.loc[all_data['DATE'] == row['Date'], val_label] \\\n",
    "        = row['Total (B)']\n",
    "    usage_labels.add(total_label)\n",
    "    val_usage_labels.add(val_label)\n",
    "usage_labels = list(usage_labels)\n",
    "val_usage_labels = list(val_usage_labels)\n",
    "\n",
    "# Normalize weather data\n",
    "all_data[['TMIN','TMAX']] -= (all_data[['TMIN', 'TMAX']].sum().sum() \\\n",
    "                              / all_data[['TMIN', 'TMAX']].size)\n",
    "all_data[['TMIN','TMAX']] /= (all_data[['TMIN', 'TMAX']].abs().sum().sum() \\\n",
    "                              / all_data[['TMIN', 'TMAX']].size)\n",
    "all_data['PRCP'] = zscore(all_data['PRCP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_data = all_data.sample(frac=0.05)\n",
    "training_data = all_data.drop(holdout_data.index)\n",
    "\n",
    "# Separate inputs into categories for encoding\n",
    "weather_x = training_data[['PRCP', 'TMAX', 'TMIN']].values\n",
    "day_x = training_data[weekday_labels].values\n",
    "date_x = training_data[['Year %']]\n",
    "\n",
    "usage_y = training_data[usage_labels].values\n",
    "clients_y = training_data[['# Clients']].values\n",
    "sessions_y = training_data[['# Sessions']].values\n",
    "\n",
    "validation_data = ([holdout_data[['PRCP','TMAX','TMIN']].values,\n",
    "                    holdout_data[weekday_labels].values,\n",
    "                    holdout_data[['Year %']].values],\n",
    "                   # the next line *should* use val_usage_labels, but it causes loss to explode\n",
    "                   [holdout_data[usage_labels].values,\n",
    "                    holdout_data[['# Clients']].values,\n",
    "                    holdout_data[['# Sessions']].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "reg = None # l1_l2(0.01,0.02)\n",
    "\n",
    "stack_layers = lambda layers: reduce(lambda stack, e: e(stack), layers)\n",
    "\n",
    "weather_layers = \\\n",
    "  [Input((3,), name='weather')] \\\n",
    "+ [Dense(10, activation='relu',kernel_regularizer=reg)] \\\n",
    "+ [Dropout(0.5)] \\\n",
    "+ [Dense(10, activation='relu',kernel_regularizer=reg)] \\\n",
    "+ [Dropout(0.5)] \\\n",
    "+ [Dense(5, activation='relu',kernel_regularizer=reg)] \\\n",
    "+ [Dropout(0.5)] \\\n",
    "+ [Dense(4, activation='relu',kernel_regularizer=reg)]\n",
    "\n",
    "weather_out = stack_layers(weather_layers)\n",
    "\n",
    "day_layers = \\\n",
    "  [Input((7,), name='day')] \\\n",
    "+ [Dense(10, activation='relu',kernel_regularizer=reg)] \\\n",
    "+ [Dropout(0.5)] \\\n",
    "+ [Dense(10, activation='relu',kernel_regularizer=reg)] \\\n",
    "+ [Dropout(0.5)] \\\n",
    "+ [Dense(10, activation='relu',kernel_regularizer=reg)] \\\n",
    "+ [Dropout(0.5)] \\\n",
    "+ [Dense(5, activation='relu',kernel_regularizer=reg)]\n",
    "\n",
    "day_out = stack_layers(day_layers)\n",
    "\n",
    "date_layers = \\\n",
    "  [Input((1,), name='date')] \\\n",
    "+ [Dense(10, activation='relu',kernel_regularizer=reg)] \\\n",
    "+ [Dropout(0.5)] \\\n",
    "+ [Dense(10, activation='relu',kernel_regularizer=reg)] \\\n",
    "+ [Dropout(0.5)] \\\n",
    "+ [Dense(10, activation='relu',kernel_regularizer=reg)] \\\n",
    "+ [Dropout(0.5)] \\\n",
    "+ [Dense(5, activation='relu',kernel_regularizer=reg)]\n",
    "\n",
    "date_out = stack_layers(date_layers)\n",
    "\n",
    "main_layers = \\\n",
    "  [concatenate([weather_out, day_out, date_out])] \\\n",
    "+ [Dense(20, activation='relu',kernel_regularizer=reg)] \\\n",
    "+ [Dropout(0.5)] \\\n",
    "+ [Dense(20, activation='relu',kernel_regularizer=reg)] \\\n",
    "+ [Dropout(0.5)] \\\n",
    "+ [Dense(20, activation='relu',kernel_regularizer=reg)] \\\n",
    "+ [Dropout(0.5)] \n",
    "\n",
    "main_out = stack_layers(main_layers)\n",
    "\n",
    "usage_layers = \\\n",
    "  [main_out] \\\n",
    "+ [Dense(15, activation='relu',kernel_regularizer=reg)] \\\n",
    "+ [Dropout(0.5)] \\\n",
    "+ [Dense(10, activation='relu',kernel_regularizer=reg)] \\\n",
    "+ [Dropout(0.5)] \\\n",
    "+ [Dense(10, activation='relu',kernel_regularizer=reg)] \\\n",
    "+ [Dropout(0.5)] \\\n",
    "+ [Dense(6, activation='relu',kernel_regularizer=reg, name='usage')]\n",
    "\n",
    "usage_out=stack_layers(usage_layers)\n",
    "\n",
    "clients_layers = \\\n",
    "  [main_out] \\\n",
    "+ [Dense(15, activation='relu',kernel_regularizer=reg)] \\\n",
    "+ [Dropout(0.5)] \\\n",
    "+ [Dense(10, activation='relu',kernel_regularizer=reg)] \\\n",
    "+ [Dropout(0.5)] \\\n",
    "+ [Dense(10, activation='relu',kernel_regularizer=reg)] \\\n",
    "+ [Dropout(0.5)] \\\n",
    "+ [Dense(1, activation='relu',kernel_regularizer=reg, name='clients')]\n",
    "\n",
    "clients_out = stack_layers(clients_layers)\n",
    "\n",
    "sessions_layers = \\\n",
    "  [main_out] \\\n",
    "+ [Dense(15, activation='relu',kernel_regularizer=reg)] \\\n",
    "+ [Dropout(0.5)] \\\n",
    "+ [Dense(10, activation='relu',kernel_regularizer=reg)] \\\n",
    "+ [Dropout(0.5)] \\\n",
    "+ [Dense(10, activation='relu',kernel_regularizer=reg)] \\\n",
    "+ [Dropout(0.5)] \\\n",
    "+ [Dense(1, activation='relu',kernel_regularizer=reg, name='sessions')]\n",
    "\n",
    "sessions_out = stack_layers(sessions_layers)\n",
    "\n",
    "model = Model(inputs=[weather_layers[0], day_layers[0], date_layers[0]], outputs=[usage_out,clients_out,sessions_out])\n",
    "model.compile(loss='mean_absolute_percentage_error',\n",
    "              optimizer=Adam(0.0001))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "early_stopping = EarlyStopping(patience=50, verbose=1)\n",
    "history = model.fit([weather_x,day_x,date_x], [usage_y,clients_y,sessions_y],\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=validation_data,\n",
    "                    callbacks=[early_stopping, PlotLossesKeras()])\n",
    "model_path = join('..','models','model.h5')\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
