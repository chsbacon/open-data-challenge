{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
<<<<<<< Updated upstream
=======
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
>>>>>>> Stashed changes
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, LeakyReLU, Input, concatenate, Activation\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.regularizers import l1_l2\n",
    "\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from scipy.stats import zscore\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "weather_path = os.path.join('..','data','weather','precip_temp.csv')\n",
    "weather_df = pd.read_csv(weather_path)\n",
    "clients_path = os.path.join('..','data','wifi','**','Clients per day.csv')\n",
    "clients_df = pd.concat(map(lambda csv: pd.read_csv(csv, parse_dates=[0]),\n",
    "                           glob.glob(clients_path)), ignore_index=True)\n",
    "sessions_path = os.path.join('..','data','wifi','**','Number of sessions over time.csv')\n",
    "sessions_df = pd.concat(map(lambda csv: pd.read_csv(csv, parse_dates=[0]),\n",
    "                               glob.glob(sessions_path)), ignore_index=True)\n",
    "usage_path = os.path.join('..','data','wifi','**','Usage over time.csv')\n",
    "usage_df = pd.concat(map(lambda csv: pd.read_csv(csv, parse_dates=[0]),\n",
    "                               glob.glob(usage_path)), ignore_index=True)\n",
    "# Interpolate zeros\n",
    "\n",
    "usage_df.loc[usage_df['Total (B)'] == 0, 'Total (B)'] = np.NaN\n",
    "usage_df['Total (B)'] = usage_df['Total (B)'].interpolate()\n",
    "# Remove redundant 06-30-17 data point\n",
    "usage_df.drop(0, inplace=True)\n",
    "\n",
    "\n",
    "sessions_df.loc[sessions_df['# Sessions'] == 0, '# Sessions'] = np.NaN\n",
    "from math import isnan\n",
    "if isnan(sessions_df['# Sessions'][0]):\n",
    "    sessions_df['# Sessions'][0]=(sessions_df['# Sessions'][351]*31+sessions_df['# Sessions'][32])/32\n",
    "sessions_df['# Sessions'] = sessions_df['# Sessions'].interpolate()\n",
    "#with pd.option_context('display.max_rows', None, 'display.max_columns', 3):\n",
    "#    print(sessions_df['# Sessions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th># Clients</th>\n",
       "      <th>weekday-0</th>\n",
       "      <th>weekday-1</th>\n",
       "      <th>weekday-2</th>\n",
       "      <th>weekday-3</th>\n",
       "      <th>weekday-4</th>\n",
       "      <th>weekday-5</th>\n",
       "      <th>weekday-6</th>\n",
       "      <th>DATE</th>\n",
       "      <th>PRCP</th>\n",
       "      <th>TMAX</th>\n",
       "      <th>TMIN</th>\n",
       "      <th># Sessions</th>\n",
       "      <th>total-2</th>\n",
       "      <th>total-6</th>\n",
       "      <th>total-10</th>\n",
       "      <th>total-14</th>\n",
       "      <th>total-18</th>\n",
       "      <th>total-22</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>2017-09-13</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-09-13</td>\n",
       "      <td>-0.243394</td>\n",
       "      <td>0.616034</td>\n",
       "      <td>0.784998</td>\n",
       "      <td>869.000000</td>\n",
       "      <td>2.061653e+03</td>\n",
       "      <td>7.248270e+04</td>\n",
       "      <td>8.216997e+05</td>\n",
       "      <td>2.122316e+06</td>\n",
       "      <td>5.042313e+05</td>\n",
       "      <td>4.702453e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>2017-09-08</td>\n",
       "      <td>127</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-09-08</td>\n",
       "      <td>-0.343154</td>\n",
       "      <td>0.345305</td>\n",
       "      <td>0.208310</td>\n",
       "      <td>987.000000</td>\n",
       "      <td>5.280364e+05</td>\n",
       "      <td>7.202702e+04</td>\n",
       "      <td>1.008288e+06</td>\n",
       "      <td>3.663622e+05</td>\n",
       "      <td>1.162125e+06</td>\n",
       "      <td>3.605407e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>2017-08-21</td>\n",
       "      <td>117</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-08-21</td>\n",
       "      <td>-0.343154</td>\n",
       "      <td>1.345826</td>\n",
       "      <td>1.291071</td>\n",
       "      <td>883.000000</td>\n",
       "      <td>7.320212e+05</td>\n",
       "      <td>1.045060e+05</td>\n",
       "      <td>8.574180e+04</td>\n",
       "      <td>7.164888e+05</td>\n",
       "      <td>6.817377e+05</td>\n",
       "      <td>8.522286e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>2017-03-03</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-03-03</td>\n",
       "      <td>-0.343154</td>\n",
       "      <td>-1.502714</td>\n",
       "      <td>-1.498214</td>\n",
       "      <td>574.000000</td>\n",
       "      <td>3.339378e+03</td>\n",
       "      <td>6.826325e+04</td>\n",
       "      <td>3.796537e+04</td>\n",
       "      <td>6.943698e+05</td>\n",
       "      <td>9.439255e+05</td>\n",
       "      <td>1.269333e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2017-02-20</td>\n",
       "      <td>138</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-02-20</td>\n",
       "      <td>-0.343154</td>\n",
       "      <td>0.098118</td>\n",
       "      <td>-0.168302</td>\n",
       "      <td>926.000000</td>\n",
       "      <td>4.573867e+02</td>\n",
       "      <td>1.130951e+03</td>\n",
       "      <td>1.579048e+05</td>\n",
       "      <td>1.284398e+06</td>\n",
       "      <td>5.700033e+05</td>\n",
       "      <td>4.262172e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>2017-04-21</td>\n",
       "      <td>264</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-04-21</td>\n",
       "      <td>-0.280804</td>\n",
       "      <td>1.016242</td>\n",
       "      <td>1.090995</td>\n",
       "      <td>2459.000000</td>\n",
       "      <td>5.088535e+05</td>\n",
       "      <td>4.257508e+04</td>\n",
       "      <td>1.735464e+05</td>\n",
       "      <td>1.723384e+06</td>\n",
       "      <td>2.324479e+06</td>\n",
       "      <td>5.998552e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>2017-03-29</td>\n",
       "      <td>156</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-03-29</td>\n",
       "      <td>-0.343154</td>\n",
       "      <td>-0.007819</td>\n",
       "      <td>0.114157</td>\n",
       "      <td>1316.000000</td>\n",
       "      <td>3.387904e+04</td>\n",
       "      <td>2.506866e+04</td>\n",
       "      <td>3.288530e+05</td>\n",
       "      <td>2.541641e+06</td>\n",
       "      <td>3.280004e+06</td>\n",
       "      <td>1.776759e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>2017-09-23</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-09-23</td>\n",
       "      <td>-0.343154</td>\n",
       "      <td>0.980930</td>\n",
       "      <td>0.890920</td>\n",
       "      <td>2224.000000</td>\n",
       "      <td>4.472263e+04</td>\n",
       "      <td>5.658112e+04</td>\n",
       "      <td>8.956780e+05</td>\n",
       "      <td>3.328403e+06</td>\n",
       "      <td>1.815322e+06</td>\n",
       "      <td>1.112104e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>2017-07-04</td>\n",
       "      <td>89</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-07-04</td>\n",
       "      <td>-0.318214</td>\n",
       "      <td>1.110409</td>\n",
       "      <td>1.326378</td>\n",
       "      <td>796.000000</td>\n",
       "      <td>5.145771e+04</td>\n",
       "      <td>1.008924e+04</td>\n",
       "      <td>6.868144e+05</td>\n",
       "      <td>2.157694e+06</td>\n",
       "      <td>2.640716e+06</td>\n",
       "      <td>1.951693e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>2017-07-20</td>\n",
       "      <td>134</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-07-20</td>\n",
       "      <td>-0.343154</td>\n",
       "      <td>1.675409</td>\n",
       "      <td>1.526453</td>\n",
       "      <td>1014.000000</td>\n",
       "      <td>2.674284e+05</td>\n",
       "      <td>2.334987e+05</td>\n",
       "      <td>4.066998e+05</td>\n",
       "      <td>9.483463e+05</td>\n",
       "      <td>1.891738e+06</td>\n",
       "      <td>1.491976e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>2017-02-25</td>\n",
       "      <td>116</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-02-25</td>\n",
       "      <td>0.305285</td>\n",
       "      <td>0.286451</td>\n",
       "      <td>-0.850912</td>\n",
       "      <td>899.000000</td>\n",
       "      <td>4.869746e+04</td>\n",
       "      <td>1.918780e+05</td>\n",
       "      <td>3.350585e+05</td>\n",
       "      <td>9.589180e+05</td>\n",
       "      <td>5.099315e+05</td>\n",
       "      <td>1.498573e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>2017-10-12</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-10-12</td>\n",
       "      <td>0.118235</td>\n",
       "      <td>0.227597</td>\n",
       "      <td>0.361309</td>\n",
       "      <td>505.000000</td>\n",
       "      <td>4.455367e+04</td>\n",
       "      <td>1.641910e+05</td>\n",
       "      <td>3.726791e+05</td>\n",
       "      <td>6.912848e+05</td>\n",
       "      <td>3.074765e+05</td>\n",
       "      <td>3.147031e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>2017-12-14</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-12-14</td>\n",
       "      <td>-0.343154</td>\n",
       "      <td>-1.549798</td>\n",
       "      <td>-1.557060</td>\n",
       "      <td>600.000000</td>\n",
       "      <td>2.392804e+04</td>\n",
       "      <td>2.973241e+04</td>\n",
       "      <td>4.062686e+05</td>\n",
       "      <td>1.897109e+06</td>\n",
       "      <td>8.650359e+05</td>\n",
       "      <td>1.944161e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>73</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>0.579624</td>\n",
       "      <td>-1.667506</td>\n",
       "      <td>-0.792066</td>\n",
       "      <td>644.957031</td>\n",
       "      <td>4.494222e+01</td>\n",
       "      <td>7.420587e+03</td>\n",
       "      <td>7.166350e+04</td>\n",
       "      <td>2.816171e+05</td>\n",
       "      <td>2.650556e+05</td>\n",
       "      <td>4.505202e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>2017-09-30</td>\n",
       "      <td>116</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-09-30</td>\n",
       "      <td>-0.343154</td>\n",
       "      <td>0.015722</td>\n",
       "      <td>-0.027072</td>\n",
       "      <td>858.000000</td>\n",
       "      <td>2.590458e+05</td>\n",
       "      <td>5.688889e+03</td>\n",
       "      <td>6.162478e+05</td>\n",
       "      <td>2.428831e+06</td>\n",
       "      <td>1.092194e+06</td>\n",
       "      <td>2.152516e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>2017-05-03</td>\n",
       "      <td>170</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-05-03</td>\n",
       "      <td>-0.343154</td>\n",
       "      <td>0.003951</td>\n",
       "      <td>0.137696</td>\n",
       "      <td>1610.000000</td>\n",
       "      <td>2.347099e+05</td>\n",
       "      <td>4.573867e+03</td>\n",
       "      <td>4.199902e+05</td>\n",
       "      <td>2.069174e+06</td>\n",
       "      <td>2.058882e+06</td>\n",
       "      <td>1.673921e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>2017-08-09</td>\n",
       "      <td>159</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-08-09</td>\n",
       "      <td>-0.343154</td>\n",
       "      <td>0.769055</td>\n",
       "      <td>0.667307</td>\n",
       "      <td>1247.000000</td>\n",
       "      <td>1.564217e+04</td>\n",
       "      <td>3.234702e+04</td>\n",
       "      <td>3.877962e+05</td>\n",
       "      <td>2.050097e+06</td>\n",
       "      <td>2.692654e+06</td>\n",
       "      <td>9.368957e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>2017-10-07</td>\n",
       "      <td>134</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-10-07</td>\n",
       "      <td>0.105765</td>\n",
       "      <td>0.992701</td>\n",
       "      <td>0.796767</td>\n",
       "      <td>1021.000000</td>\n",
       "      <td>1.483042e+05</td>\n",
       "      <td>5.206016e+04</td>\n",
       "      <td>1.142469e+06</td>\n",
       "      <td>3.790628e+06</td>\n",
       "      <td>2.224056e+06</td>\n",
       "      <td>1.315133e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>2017-07-22</td>\n",
       "      <td>105</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-07-22</td>\n",
       "      <td>0.567154</td>\n",
       "      <td>1.710721</td>\n",
       "      <td>1.373455</td>\n",
       "      <td>1162.000000</td>\n",
       "      <td>2.587961e+05</td>\n",
       "      <td>1.455924e+06</td>\n",
       "      <td>4.648914e+05</td>\n",
       "      <td>1.569263e+06</td>\n",
       "      <td>2.975002e+06</td>\n",
       "      <td>6.359603e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>2017-06-16</td>\n",
       "      <td>156</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-06-16</td>\n",
       "      <td>0.966193</td>\n",
       "      <td>0.721972</td>\n",
       "      <td>1.008611</td>\n",
       "      <td>1391.000000</td>\n",
       "      <td>1.114283e+04</td>\n",
       "      <td>2.061596e+04</td>\n",
       "      <td>7.369279e+05</td>\n",
       "      <td>1.176380e+06</td>\n",
       "      <td>5.769330e+05</td>\n",
       "      <td>3.826130e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>2017-09-09</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-09-09</td>\n",
       "      <td>-0.343154</td>\n",
       "      <td>0.109889</td>\n",
       "      <td>0.208310</td>\n",
       "      <td>744.000000</td>\n",
       "      <td>1.979011e+05</td>\n",
       "      <td>1.353557e+04</td>\n",
       "      <td>1.317712e+05</td>\n",
       "      <td>1.002985e+05</td>\n",
       "      <td>4.584329e+05</td>\n",
       "      <td>6.251275e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>2017-06-04</td>\n",
       "      <td>130</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-06-04</td>\n",
       "      <td>-0.343154</td>\n",
       "      <td>0.980930</td>\n",
       "      <td>1.043919</td>\n",
       "      <td>1377.000000</td>\n",
       "      <td>4.573184e+04</td>\n",
       "      <td>1.409536e+05</td>\n",
       "      <td>1.087124e+05</td>\n",
       "      <td>1.504274e+06</td>\n",
       "      <td>2.337648e+06</td>\n",
       "      <td>1.801657e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>2017-09-26</td>\n",
       "      <td>143</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-09-26</td>\n",
       "      <td>-0.343154</td>\n",
       "      <td>0.980930</td>\n",
       "      <td>1.008611</td>\n",
       "      <td>1280.000000</td>\n",
       "      <td>7.151047e+04</td>\n",
       "      <td>5.756700e+04</td>\n",
       "      <td>5.321563e+05</td>\n",
       "      <td>1.395713e+06</td>\n",
       "      <td>1.706582e+06</td>\n",
       "      <td>2.463875e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2017-02-19</td>\n",
       "      <td>147</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-02-19</td>\n",
       "      <td>-0.343154</td>\n",
       "      <td>0.333535</td>\n",
       "      <td>0.090619</td>\n",
       "      <td>1095.000000</td>\n",
       "      <td>1.105977e+04</td>\n",
       "      <td>8.791609e+03</td>\n",
       "      <td>2.003524e+05</td>\n",
       "      <td>7.972346e+05</td>\n",
       "      <td>4.024519e+05</td>\n",
       "      <td>1.222650e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>2017-10-03</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-10-03</td>\n",
       "      <td>-0.343154</td>\n",
       "      <td>0.309993</td>\n",
       "      <td>0.114157</td>\n",
       "      <td>1280.000000</td>\n",
       "      <td>1.818442e+05</td>\n",
       "      <td>3.645895e+04</td>\n",
       "      <td>7.627270e+05</td>\n",
       "      <td>2.999414e+06</td>\n",
       "      <td>2.532742e+06</td>\n",
       "      <td>1.015587e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2017-02-17</td>\n",
       "      <td>140</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-02-17</td>\n",
       "      <td>-0.343154</td>\n",
       "      <td>-0.384486</td>\n",
       "      <td>-1.192217</td>\n",
       "      <td>1140.000000</td>\n",
       "      <td>1.820444e+01</td>\n",
       "      <td>1.601422e+03</td>\n",
       "      <td>2.518682e+05</td>\n",
       "      <td>1.312908e+06</td>\n",
       "      <td>8.753283e+05</td>\n",
       "      <td>1.799179e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>2017-04-28</td>\n",
       "      <td>317</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-04-28</td>\n",
       "      <td>-0.343154</td>\n",
       "      <td>0.898534</td>\n",
       "      <td>0.749691</td>\n",
       "      <td>2868.000000</td>\n",
       "      <td>1.902780e+05</td>\n",
       "      <td>8.381713e+05</td>\n",
       "      <td>6.445039e+05</td>\n",
       "      <td>1.368909e+06</td>\n",
       "      <td>2.399233e+06</td>\n",
       "      <td>1.352837e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>97</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>-0.343154</td>\n",
       "      <td>-0.725840</td>\n",
       "      <td>-1.262831</td>\n",
       "      <td>632.121094</td>\n",
       "      <td>1.755022e+03</td>\n",
       "      <td>7.644729e+03</td>\n",
       "      <td>3.313021e+05</td>\n",
       "      <td>1.094120e+06</td>\n",
       "      <td>1.694071e+05</td>\n",
       "      <td>1.264754e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>2017-11-25</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-11-25</td>\n",
       "      <td>-0.343154</td>\n",
       "      <td>-0.419798</td>\n",
       "      <td>-0.827373</td>\n",
       "      <td>382.000000</td>\n",
       "      <td>1.289728e+04</td>\n",
       "      <td>1.336661e+04</td>\n",
       "      <td>1.383595e+04</td>\n",
       "      <td>2.902539e+05</td>\n",
       "      <td>4.869473e+05</td>\n",
       "      <td>1.209822e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>2017-08-28</td>\n",
       "      <td>104</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-08-28</td>\n",
       "      <td>-0.343154</td>\n",
       "      <td>0.345305</td>\n",
       "      <td>0.431924</td>\n",
       "      <td>832.000000</td>\n",
       "      <td>7.387711e+05</td>\n",
       "      <td>2.437404e+05</td>\n",
       "      <td>2.326795e+05</td>\n",
       "      <td>3.392243e+06</td>\n",
       "      <td>8.217145e+05</td>\n",
       "      <td>4.764723e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2017-01-31</td>\n",
       "      <td>138</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-31</td>\n",
       "      <td>-0.343154</td>\n",
       "      <td>-0.502194</td>\n",
       "      <td>-1.804212</td>\n",
       "      <td>458.835938</td>\n",
       "      <td>1.200128e+04</td>\n",
       "      <td>1.559324e+03</td>\n",
       "      <td>5.441872e+05</td>\n",
       "      <td>1.554773e+06</td>\n",
       "      <td>2.769618e+05</td>\n",
       "      <td>8.624981e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2017-01-29</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-01-29</td>\n",
       "      <td>-0.230924</td>\n",
       "      <td>-1.691048</td>\n",
       "      <td>-1.286370</td>\n",
       "      <td>471.671875</td>\n",
       "      <td>2.650624e+04</td>\n",
       "      <td>6.554169e+03</td>\n",
       "      <td>1.403449e+04</td>\n",
       "      <td>9.518763e+05</td>\n",
       "      <td>2.153432e+05</td>\n",
       "      <td>8.008021e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>2017-05-01</td>\n",
       "      <td>116</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-05-01</td>\n",
       "      <td>0.380104</td>\n",
       "      <td>0.474784</td>\n",
       "      <td>0.937997</td>\n",
       "      <td>765.000000</td>\n",
       "      <td>1.365316e+05</td>\n",
       "      <td>2.044359e+04</td>\n",
       "      <td>3.866704e+05</td>\n",
       "      <td>1.302502e+06</td>\n",
       "      <td>1.833318e+05</td>\n",
       "      <td>1.794897e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>2017-07-12</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-07-12</td>\n",
       "      <td>-0.343154</td>\n",
       "      <td>1.569471</td>\n",
       "      <td>1.797143</td>\n",
       "      <td>974.000000</td>\n",
       "      <td>6.666411e+04</td>\n",
       "      <td>5.997579e+05</td>\n",
       "      <td>6.138840e+05</td>\n",
       "      <td>7.740160e+05</td>\n",
       "      <td>7.628231e+05</td>\n",
       "      <td>3.302400e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>2017-04-30</td>\n",
       "      <td>148</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-04-30</td>\n",
       "      <td>-0.343154</td>\n",
       "      <td>0.874993</td>\n",
       "      <td>1.102765</td>\n",
       "      <td>1241.000000</td>\n",
       "      <td>5.033415e+04</td>\n",
       "      <td>2.551689e+05</td>\n",
       "      <td>1.323785e+06</td>\n",
       "      <td>1.379593e+06</td>\n",
       "      <td>2.139571e+06</td>\n",
       "      <td>7.459709e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2017-01-06</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-06</td>\n",
       "      <td>-0.218454</td>\n",
       "      <td>-2.208964</td>\n",
       "      <td>-1.757135</td>\n",
       "      <td>619.285156</td>\n",
       "      <td>8.192569e+03</td>\n",
       "      <td>1.422222e+02</td>\n",
       "      <td>1.798542e+04</td>\n",
       "      <td>1.443812e+05</td>\n",
       "      <td>3.356718e+05</td>\n",
       "      <td>2.263950e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>2017-06-23</td>\n",
       "      <td>188</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-06-23</td>\n",
       "      <td>0.006005</td>\n",
       "      <td>0.992701</td>\n",
       "      <td>1.090995</td>\n",
       "      <td>1719.000000</td>\n",
       "      <td>2.982707e+05</td>\n",
       "      <td>2.919708e+04</td>\n",
       "      <td>6.915715e+05</td>\n",
       "      <td>1.474821e+06</td>\n",
       "      <td>1.621261e+06</td>\n",
       "      <td>4.887609e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>2017-04-19</td>\n",
       "      <td>103</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-04-19</td>\n",
       "      <td>-0.131164</td>\n",
       "      <td>-0.820006</td>\n",
       "      <td>-0.203609</td>\n",
       "      <td>901.000000</td>\n",
       "      <td>6.081746e+05</td>\n",
       "      <td>2.078458e+05</td>\n",
       "      <td>7.140978e+04</td>\n",
       "      <td>1.118495e+06</td>\n",
       "      <td>1.016773e+06</td>\n",
       "      <td>7.712068e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>2017-05-19</td>\n",
       "      <td>245</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-05-19</td>\n",
       "      <td>-0.343154</td>\n",
       "      <td>1.051555</td>\n",
       "      <td>1.255763</td>\n",
       "      <td>2194.000000</td>\n",
       "      <td>8.657351e+03</td>\n",
       "      <td>5.272064e+04</td>\n",
       "      <td>3.418112e+05</td>\n",
       "      <td>1.891841e+06</td>\n",
       "      <td>1.404619e+06</td>\n",
       "      <td>2.026595e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>2017-10-29</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-10-29</td>\n",
       "      <td>8.535465</td>\n",
       "      <td>-0.466882</td>\n",
       "      <td>-0.733220</td>\n",
       "      <td>344.000000</td>\n",
       "      <td>1.129779e+05</td>\n",
       "      <td>1.354337e+05</td>\n",
       "      <td>4.474550e+05</td>\n",
       "      <td>3.186022e+05</td>\n",
       "      <td>5.274169e+03</td>\n",
       "      <td>1.466368e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>2017-04-17</td>\n",
       "      <td>140</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-04-17</td>\n",
       "      <td>-0.343154</td>\n",
       "      <td>0.321764</td>\n",
       "      <td>0.490770</td>\n",
       "      <td>1149.000000</td>\n",
       "      <td>2.912785e+05</td>\n",
       "      <td>4.760804e+04</td>\n",
       "      <td>1.976420e+06</td>\n",
       "      <td>1.549802e+06</td>\n",
       "      <td>1.538233e+06</td>\n",
       "      <td>1.940584e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>2017-04-05</td>\n",
       "      <td>157</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-04-05</td>\n",
       "      <td>-0.343154</td>\n",
       "      <td>0.439472</td>\n",
       "      <td>0.479000</td>\n",
       "      <td>1540.000000</td>\n",
       "      <td>1.358399e+05</td>\n",
       "      <td>1.746648e+05</td>\n",
       "      <td>1.229151e+06</td>\n",
       "      <td>1.387449e+06</td>\n",
       "      <td>2.449376e+06</td>\n",
       "      <td>9.449341e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>2017-11-17</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-11-17</td>\n",
       "      <td>-0.343154</td>\n",
       "      <td>-0.902402</td>\n",
       "      <td>-1.109833</td>\n",
       "      <td>302.000000</td>\n",
       "      <td>1.251556e+01</td>\n",
       "      <td>1.639538e+03</td>\n",
       "      <td>4.736512e+04</td>\n",
       "      <td>4.355925e+04</td>\n",
       "      <td>5.668921e+04</td>\n",
       "      <td>1.337947e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2017-01-14</td>\n",
       "      <td>94</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-14</td>\n",
       "      <td>-0.031405</td>\n",
       "      <td>-1.785214</td>\n",
       "      <td>-1.239293</td>\n",
       "      <td>567.941406</td>\n",
       "      <td>1.228919e+05</td>\n",
       "      <td>6.468267e+02</td>\n",
       "      <td>1.738638e+04</td>\n",
       "      <td>3.042241e+05</td>\n",
       "      <td>2.020466e+05</td>\n",
       "      <td>8.550172e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>2017-08-20</td>\n",
       "      <td>98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-08-20</td>\n",
       "      <td>-0.343154</td>\n",
       "      <td>1.204576</td>\n",
       "      <td>1.196918</td>\n",
       "      <td>889.000000</td>\n",
       "      <td>6.948847e+05</td>\n",
       "      <td>2.265168e+05</td>\n",
       "      <td>1.163355e+05</td>\n",
       "      <td>9.852422e+05</td>\n",
       "      <td>1.868023e+06</td>\n",
       "      <td>5.054020e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>2017-03-07</td>\n",
       "      <td>97</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-03-07</td>\n",
       "      <td>-0.181044</td>\n",
       "      <td>-0.231465</td>\n",
       "      <td>-0.156533</td>\n",
       "      <td>516.000000</td>\n",
       "      <td>4.551111e+00</td>\n",
       "      <td>8.853163e+04</td>\n",
       "      <td>3.582510e+05</td>\n",
       "      <td>6.696977e+05</td>\n",
       "      <td>6.474149e+05</td>\n",
       "      <td>9.404473e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>2017-04-01</td>\n",
       "      <td>180</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-04-01</td>\n",
       "      <td>-0.343154</td>\n",
       "      <td>-0.019590</td>\n",
       "      <td>-0.333070</td>\n",
       "      <td>1413.000000</td>\n",
       "      <td>2.330550e+05</td>\n",
       "      <td>2.541676e+05</td>\n",
       "      <td>5.468080e+05</td>\n",
       "      <td>3.447648e+06</td>\n",
       "      <td>1.692158e+06</td>\n",
       "      <td>2.723499e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>2017-08-27</td>\n",
       "      <td>86</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-08-27</td>\n",
       "      <td>-0.343154</td>\n",
       "      <td>0.616034</td>\n",
       "      <td>0.679076</td>\n",
       "      <td>962.000000</td>\n",
       "      <td>2.394908e+04</td>\n",
       "      <td>2.275556e+00</td>\n",
       "      <td>1.464536e+05</td>\n",
       "      <td>7.137678e+05</td>\n",
       "      <td>9.676447e+05</td>\n",
       "      <td>3.395261e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>2017-12-13</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-12-13</td>\n",
       "      <td>-0.343154</td>\n",
       "      <td>-2.197193</td>\n",
       "      <td>-2.145516</td>\n",
       "      <td>543.000000</td>\n",
       "      <td>1.084075e+04</td>\n",
       "      <td>5.324800e+02</td>\n",
       "      <td>1.268122e+05</td>\n",
       "      <td>1.341226e+06</td>\n",
       "      <td>3.379098e+05</td>\n",
       "      <td>7.845148e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>2017-10-19</td>\n",
       "      <td>148</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-10-19</td>\n",
       "      <td>-0.343154</td>\n",
       "      <td>0.298222</td>\n",
       "      <td>0.302463</td>\n",
       "      <td>1453.000000</td>\n",
       "      <td>1.986335e+06</td>\n",
       "      <td>4.123136e+04</td>\n",
       "      <td>7.132661e+05</td>\n",
       "      <td>4.105189e+06</td>\n",
       "      <td>4.300239e+06</td>\n",
       "      <td>1.440696e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>2017-12-08</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-12-08</td>\n",
       "      <td>-0.343154</td>\n",
       "      <td>-1.820527</td>\n",
       "      <td>-1.356984</td>\n",
       "      <td>930.000000</td>\n",
       "      <td>1.681766e+05</td>\n",
       "      <td>2.793586e+04</td>\n",
       "      <td>3.484314e+05</td>\n",
       "      <td>5.032750e+05</td>\n",
       "      <td>4.829195e+05</td>\n",
       "      <td>2.884198e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>2017-04-08</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-04-08</td>\n",
       "      <td>-0.343154</td>\n",
       "      <td>-0.066674</td>\n",
       "      <td>-0.862681</td>\n",
       "      <td>1408.000000</td>\n",
       "      <td>4.922596e+03</td>\n",
       "      <td>1.999076e+03</td>\n",
       "      <td>2.984038e+05</td>\n",
       "      <td>8.780766e+05</td>\n",
       "      <td>2.631342e+06</td>\n",
       "      <td>7.704650e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>2017-03-27</td>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-03-27</td>\n",
       "      <td>-0.106224</td>\n",
       "      <td>0.439472</td>\n",
       "      <td>-0.274224</td>\n",
       "      <td>892.000000</td>\n",
       "      <td>3.256889e+03</td>\n",
       "      <td>3.433813e+03</td>\n",
       "      <td>3.812477e+05</td>\n",
       "      <td>6.554789e+05</td>\n",
       "      <td>5.919767e+05</td>\n",
       "      <td>3.243230e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>2017-06-14</td>\n",
       "      <td>201</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-06-14</td>\n",
       "      <td>-0.343154</td>\n",
       "      <td>1.157492</td>\n",
       "      <td>1.267532</td>\n",
       "      <td>1212.000000</td>\n",
       "      <td>4.022613e+03</td>\n",
       "      <td>4.512996e+03</td>\n",
       "      <td>3.567832e+05</td>\n",
       "      <td>6.557486e+05</td>\n",
       "      <td>1.071341e+06</td>\n",
       "      <td>1.338284e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>2017-04-02</td>\n",
       "      <td>113</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-04-02</td>\n",
       "      <td>-0.343154</td>\n",
       "      <td>-0.337403</td>\n",
       "      <td>-0.662606</td>\n",
       "      <td>994.000000</td>\n",
       "      <td>1.254087e+05</td>\n",
       "      <td>1.769495e+05</td>\n",
       "      <td>1.360412e+05</td>\n",
       "      <td>1.899333e+06</td>\n",
       "      <td>2.116712e+06</td>\n",
       "      <td>4.118994e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>-0.343154</td>\n",
       "      <td>-0.855319</td>\n",
       "      <td>-0.886219</td>\n",
       "      <td>522.000000</td>\n",
       "      <td>6.725973e+03</td>\n",
       "      <td>3.350187e+03</td>\n",
       "      <td>2.805743e+05</td>\n",
       "      <td>3.028144e+05</td>\n",
       "      <td>1.638110e+05</td>\n",
       "      <td>9.189319e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2017-02-12</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-02-12</td>\n",
       "      <td>-0.343154</td>\n",
       "      <td>0.627805</td>\n",
       "      <td>-0.109456</td>\n",
       "      <td>1033.000000</td>\n",
       "      <td>4.556459e+04</td>\n",
       "      <td>1.860267e+02</td>\n",
       "      <td>2.937936e+05</td>\n",
       "      <td>6.129823e+05</td>\n",
       "      <td>6.310997e+05</td>\n",
       "      <td>3.510898e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>2017-06-07</td>\n",
       "      <td>219</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-06-07</td>\n",
       "      <td>-0.106224</td>\n",
       "      <td>0.062806</td>\n",
       "      <td>0.349540</td>\n",
       "      <td>1584.000000</td>\n",
       "      <td>1.658027e+04</td>\n",
       "      <td>2.508004e+04</td>\n",
       "      <td>2.640566e+05</td>\n",
       "      <td>2.444898e+06</td>\n",
       "      <td>2.111298e+06</td>\n",
       "      <td>1.019688e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2017-01-08</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-01-08</td>\n",
       "      <td>-0.318214</td>\n",
       "      <td>-2.997609</td>\n",
       "      <td>-2.828126</td>\n",
       "      <td>606.449219</td>\n",
       "      <td>1.137778e+03</td>\n",
       "      <td>1.476836e+03</td>\n",
       "      <td>1.591637e+04</td>\n",
       "      <td>9.978994e+04</td>\n",
       "      <td>1.923584e+04</td>\n",
       "      <td>1.647502e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>2017-08-12</td>\n",
       "      <td>86</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-08-12</td>\n",
       "      <td>0.118235</td>\n",
       "      <td>0.980930</td>\n",
       "      <td>1.196918</td>\n",
       "      <td>634.000000</td>\n",
       "      <td>1.148359e+04</td>\n",
       "      <td>2.228338e+03</td>\n",
       "      <td>3.053653e+05</td>\n",
       "      <td>9.346697e+05</td>\n",
       "      <td>7.835153e+05</td>\n",
       "      <td>7.517303e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>353 rows  20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Time  # Clients  weekday-0  weekday-1  weekday-2  weekday-3  \\\n",
       "254  2017-09-13         90          0          0          1          0   \n",
       "249  2017-09-08        127          0          0          0          0   \n",
       "231  2017-08-21        117          1          0          0          0   \n",
       "61   2017-03-03         96          0          0          0          0   \n",
       "50   2017-02-20        138          1          0          0          0   \n",
       "110  2017-04-21        264          0          0          0          0   \n",
       "87   2017-03-29        156          0          0          1          0   \n",
       "264  2017-09-23        174          0          0          0          0   \n",
       "183  2017-07-04         89          0          1          0          0   \n",
       "199  2017-07-20        134          0          0          0          1   \n",
       "55   2017-02-25        116          0          0          0          0   \n",
       "283  2017-10-12         81          0          0          0          1   \n",
       "346  2017-12-14         59          0          0          0          1   \n",
       "1    2017-01-02         73          1          0          0          0   \n",
       "271  2017-09-30        116          0          0          0          0   \n",
       "122  2017-05-03        170          0          0          1          0   \n",
       "219  2017-08-09        159          0          0          1          0   \n",
       "278  2017-10-07        134          0          0          0          0   \n",
       "201  2017-07-22        105          0          0          0          0   \n",
       "166  2017-06-16        156          0          0          0          0   \n",
       "250  2017-09-09        100          0          0          0          0   \n",
       "154  2017-06-04        130          0          0          0          0   \n",
       "267  2017-09-26        143          0          1          0          0   \n",
       "49   2017-02-19        147          0          0          0          0   \n",
       "274  2017-10-03        141          0          1          0          0   \n",
       "47   2017-02-17        140          0          0          0          0   \n",
       "117  2017-04-28        317          0          0          0          0   \n",
       "3    2017-01-04         97          0          0          1          0   \n",
       "327  2017-11-25         48          0          0          0          0   \n",
       "238  2017-08-28        104          1          0          0          0   \n",
       "..          ...        ...        ...        ...        ...        ...   \n",
       "30   2017-01-31        138          0          1          0          0   \n",
       "28   2017-01-29         61          0          0          0          0   \n",
       "120  2017-05-01        116          1          0          0          0   \n",
       "191  2017-07-12         88          0          0          1          0   \n",
       "119  2017-04-30        148          0          0          0          0   \n",
       "5    2017-01-06         88          0          0          0          0   \n",
       "173  2017-06-23        188          0          0          0          0   \n",
       "108  2017-04-19        103          0          0          1          0   \n",
       "138  2017-05-19        245          0          0          0          0   \n",
       "300  2017-10-29         51          0          0          0          0   \n",
       "106  2017-04-17        140          1          0          0          0   \n",
       "94   2017-04-05        157          0          0          1          0   \n",
       "319  2017-11-17         42          0          0          0          0   \n",
       "13   2017-01-14         94          0          0          0          0   \n",
       "230  2017-08-20         98          0          0          0          0   \n",
       "65   2017-03-07         97          0          1          0          0   \n",
       "90   2017-04-01        180          0          0          0          0   \n",
       "237  2017-08-27         86          0          0          0          0   \n",
       "345  2017-12-13         61          0          0          1          0   \n",
       "290  2017-10-19        148          0          0          0          1   \n",
       "340  2017-12-08         76          0          0          0          0   \n",
       "97   2017-04-08        172          0          0          0          0   \n",
       "85   2017-03-27        120          1          0          0          0   \n",
       "164  2017-06-14        201          0          0          1          0   \n",
       "91   2017-04-02        113          0          0          0          0   \n",
       "57   2017-02-27         87          1          0          0          0   \n",
       "42   2017-02-12        132          0          0          0          0   \n",
       "157  2017-06-07        219          0          0          1          0   \n",
       "7    2017-01-08         40          0          0          0          0   \n",
       "222  2017-08-12         86          0          0          0          0   \n",
       "\n",
       "     weekday-4  weekday-5  weekday-6        DATE      PRCP      TMAX  \\\n",
       "254          0          0          0  2017-09-13 -0.243394  0.616034   \n",
       "249          1          0          0  2017-09-08 -0.343154  0.345305   \n",
       "231          0          0          0  2017-08-21 -0.343154  1.345826   \n",
       "61           1          0          0  2017-03-03 -0.343154 -1.502714   \n",
       "50           0          0          0  2017-02-20 -0.343154  0.098118   \n",
       "110          1          0          0  2017-04-21 -0.280804  1.016242   \n",
       "87           0          0          0  2017-03-29 -0.343154 -0.007819   \n",
       "264          0          1          0  2017-09-23 -0.343154  0.980930   \n",
       "183          0          0          0  2017-07-04 -0.318214  1.110409   \n",
       "199          0          0          0  2017-07-20 -0.343154  1.675409   \n",
       "55           0          1          0  2017-02-25  0.305285  0.286451   \n",
       "283          0          0          0  2017-10-12  0.118235  0.227597   \n",
       "346          0          0          0  2017-12-14 -0.343154 -1.549798   \n",
       "1            0          0          0  2017-01-02  0.579624 -1.667506   \n",
       "271          0          1          0  2017-09-30 -0.343154  0.015722   \n",
       "122          0          0          0  2017-05-03 -0.343154  0.003951   \n",
       "219          0          0          0  2017-08-09 -0.343154  0.769055   \n",
       "278          0          1          0  2017-10-07  0.105765  0.992701   \n",
       "201          0          1          0  2017-07-22  0.567154  1.710721   \n",
       "166          1          0          0  2017-06-16  0.966193  0.721972   \n",
       "250          0          1          0  2017-09-09 -0.343154  0.109889   \n",
       "154          0          0          1  2017-06-04 -0.343154  0.980930   \n",
       "267          0          0          0  2017-09-26 -0.343154  0.980930   \n",
       "49           0          0          1  2017-02-19 -0.343154  0.333535   \n",
       "274          0          0          0  2017-10-03 -0.343154  0.309993   \n",
       "47           1          0          0  2017-02-17 -0.343154 -0.384486   \n",
       "117          1          0          0  2017-04-28 -0.343154  0.898534   \n",
       "3            0          0          0  2017-01-04 -0.343154 -0.725840   \n",
       "327          0          1          0  2017-11-25 -0.343154 -0.419798   \n",
       "238          0          0          0  2017-08-28 -0.343154  0.345305   \n",
       "..         ...        ...        ...         ...       ...       ...   \n",
       "30           0          0          0  2017-01-31 -0.343154 -0.502194   \n",
       "28           0          0          1  2017-01-29 -0.230924 -1.691048   \n",
       "120          0          0          0  2017-05-01  0.380104  0.474784   \n",
       "191          0          0          0  2017-07-12 -0.343154  1.569471   \n",
       "119          0          0          1  2017-04-30 -0.343154  0.874993   \n",
       "5            1          0          0  2017-01-06 -0.218454 -2.208964   \n",
       "173          1          0          0  2017-06-23  0.006005  0.992701   \n",
       "108          0          0          0  2017-04-19 -0.131164 -0.820006   \n",
       "138          1          0          0  2017-05-19 -0.343154  1.051555   \n",
       "300          0          0          1  2017-10-29  8.535465 -0.466882   \n",
       "106          0          0          0  2017-04-17 -0.343154  0.321764   \n",
       "94           0          0          0  2017-04-05 -0.343154  0.439472   \n",
       "319          1          0          0  2017-11-17 -0.343154 -0.902402   \n",
       "13           0          1          0  2017-01-14 -0.031405 -1.785214   \n",
       "230          0          0          1  2017-08-20 -0.343154  1.204576   \n",
       "65           0          0          0  2017-03-07 -0.181044 -0.231465   \n",
       "90           0          1          0  2017-04-01 -0.343154 -0.019590   \n",
       "237          0          0          1  2017-08-27 -0.343154  0.616034   \n",
       "345          0          0          0  2017-12-13 -0.343154 -2.197193   \n",
       "290          0          0          0  2017-10-19 -0.343154  0.298222   \n",
       "340          1          0          0  2017-12-08 -0.343154 -1.820527   \n",
       "97           0          1          0  2017-04-08 -0.343154 -0.066674   \n",
       "85           0          0          0  2017-03-27 -0.106224  0.439472   \n",
       "164          0          0          0  2017-06-14 -0.343154  1.157492   \n",
       "91           0          0          1  2017-04-02 -0.343154 -0.337403   \n",
       "57           0          0          0  2017-02-27 -0.343154 -0.855319   \n",
       "42           0          0          1  2017-02-12 -0.343154  0.627805   \n",
       "157          0          0          0  2017-06-07 -0.106224  0.062806   \n",
       "7            0          0          1  2017-01-08 -0.318214 -2.997609   \n",
       "222          0          1          0  2017-08-12  0.118235  0.980930   \n",
       "\n",
       "         TMIN   # Sessions       total-2       total-6      total-10  \\\n",
       "254  0.784998   869.000000  2.061653e+03  7.248270e+04  8.216997e+05   \n",
       "249  0.208310   987.000000  5.280364e+05  7.202702e+04  1.008288e+06   \n",
       "231  1.291071   883.000000  7.320212e+05  1.045060e+05  8.574180e+04   \n",
       "61  -1.498214   574.000000  3.339378e+03  6.826325e+04  3.796537e+04   \n",
       "50  -0.168302   926.000000  4.573867e+02  1.130951e+03  1.579048e+05   \n",
       "110  1.090995  2459.000000  5.088535e+05  4.257508e+04  1.735464e+05   \n",
       "87   0.114157  1316.000000  3.387904e+04  2.506866e+04  3.288530e+05   \n",
       "264  0.890920  2224.000000  4.472263e+04  5.658112e+04  8.956780e+05   \n",
       "183  1.326378   796.000000  5.145771e+04  1.008924e+04  6.868144e+05   \n",
       "199  1.526453  1014.000000  2.674284e+05  2.334987e+05  4.066998e+05   \n",
       "55  -0.850912   899.000000  4.869746e+04  1.918780e+05  3.350585e+05   \n",
       "283  0.361309   505.000000  4.455367e+04  1.641910e+05  3.726791e+05   \n",
       "346 -1.557060   600.000000  2.392804e+04  2.973241e+04  4.062686e+05   \n",
       "1   -0.792066   644.957031  4.494222e+01  7.420587e+03  7.166350e+04   \n",
       "271 -0.027072   858.000000  2.590458e+05  5.688889e+03  6.162478e+05   \n",
       "122  0.137696  1610.000000  2.347099e+05  4.573867e+03  4.199902e+05   \n",
       "219  0.667307  1247.000000  1.564217e+04  3.234702e+04  3.877962e+05   \n",
       "278  0.796767  1021.000000  1.483042e+05  5.206016e+04  1.142469e+06   \n",
       "201  1.373455  1162.000000  2.587961e+05  1.455924e+06  4.648914e+05   \n",
       "166  1.008611  1391.000000  1.114283e+04  2.061596e+04  7.369279e+05   \n",
       "250  0.208310   744.000000  1.979011e+05  1.353557e+04  1.317712e+05   \n",
       "154  1.043919  1377.000000  4.573184e+04  1.409536e+05  1.087124e+05   \n",
       "267  1.008611  1280.000000  7.151047e+04  5.756700e+04  5.321563e+05   \n",
       "49   0.090619  1095.000000  1.105977e+04  8.791609e+03  2.003524e+05   \n",
       "274  0.114157  1280.000000  1.818442e+05  3.645895e+04  7.627270e+05   \n",
       "47  -1.192217  1140.000000  1.820444e+01  1.601422e+03  2.518682e+05   \n",
       "117  0.749691  2868.000000  1.902780e+05  8.381713e+05  6.445039e+05   \n",
       "3   -1.262831   632.121094  1.755022e+03  7.644729e+03  3.313021e+05   \n",
       "327 -0.827373   382.000000  1.289728e+04  1.336661e+04  1.383595e+04   \n",
       "238  0.431924   832.000000  7.387711e+05  2.437404e+05  2.326795e+05   \n",
       "..        ...          ...           ...           ...           ...   \n",
       "30  -1.804212   458.835938  1.200128e+04  1.559324e+03  5.441872e+05   \n",
       "28  -1.286370   471.671875  2.650624e+04  6.554169e+03  1.403449e+04   \n",
       "120  0.937997   765.000000  1.365316e+05  2.044359e+04  3.866704e+05   \n",
       "191  1.797143   974.000000  6.666411e+04  5.997579e+05  6.138840e+05   \n",
       "119  1.102765  1241.000000  5.033415e+04  2.551689e+05  1.323785e+06   \n",
       "5   -1.757135   619.285156  8.192569e+03  1.422222e+02  1.798542e+04   \n",
       "173  1.090995  1719.000000  2.982707e+05  2.919708e+04  6.915715e+05   \n",
       "108 -0.203609   901.000000  6.081746e+05  2.078458e+05  7.140978e+04   \n",
       "138  1.255763  2194.000000  8.657351e+03  5.272064e+04  3.418112e+05   \n",
       "300 -0.733220   344.000000  1.129779e+05  1.354337e+05  4.474550e+05   \n",
       "106  0.490770  1149.000000  2.912785e+05  4.760804e+04  1.976420e+06   \n",
       "94   0.479000  1540.000000  1.358399e+05  1.746648e+05  1.229151e+06   \n",
       "319 -1.109833   302.000000  1.251556e+01  1.639538e+03  4.736512e+04   \n",
       "13  -1.239293   567.941406  1.228919e+05  6.468267e+02  1.738638e+04   \n",
       "230  1.196918   889.000000  6.948847e+05  2.265168e+05  1.163355e+05   \n",
       "65  -0.156533   516.000000  4.551111e+00  8.853163e+04  3.582510e+05   \n",
       "90  -0.333070  1413.000000  2.330550e+05  2.541676e+05  5.468080e+05   \n",
       "237  0.679076   962.000000  2.394908e+04  2.275556e+00  1.464536e+05   \n",
       "345 -2.145516   543.000000  1.084075e+04  5.324800e+02  1.268122e+05   \n",
       "290  0.302463  1453.000000  1.986335e+06  4.123136e+04  7.132661e+05   \n",
       "340 -1.356984   930.000000  1.681766e+05  2.793586e+04  3.484314e+05   \n",
       "97  -0.862681  1408.000000  4.922596e+03  1.999076e+03  2.984038e+05   \n",
       "85  -0.274224   892.000000  3.256889e+03  3.433813e+03  3.812477e+05   \n",
       "164  1.267532  1212.000000  4.022613e+03  4.512996e+03  3.567832e+05   \n",
       "91  -0.662606   994.000000  1.254087e+05  1.769495e+05  1.360412e+05   \n",
       "57  -0.886219   522.000000  6.725973e+03  3.350187e+03  2.805743e+05   \n",
       "42  -0.109456  1033.000000  4.556459e+04  1.860267e+02  2.937936e+05   \n",
       "157  0.349540  1584.000000  1.658027e+04  2.508004e+04  2.640566e+05   \n",
       "7   -2.828126   606.449219  1.137778e+03  1.476836e+03  1.591637e+04   \n",
       "222  1.196918   634.000000  1.148359e+04  2.228338e+03  3.053653e+05   \n",
       "\n",
       "         total-14      total-18      total-22  \n",
       "254  2.122316e+06  5.042313e+05  4.702453e+05  \n",
       "249  3.663622e+05  1.162125e+06  3.605407e+05  \n",
       "231  7.164888e+05  6.817377e+05  8.522286e+05  \n",
       "61   6.943698e+05  9.439255e+05  1.269333e+05  \n",
       "50   1.284398e+06  5.700033e+05  4.262172e+04  \n",
       "110  1.723384e+06  2.324479e+06  5.998552e+05  \n",
       "87   2.541641e+06  3.280004e+06  1.776759e+05  \n",
       "264  3.328403e+06  1.815322e+06  1.112104e+06  \n",
       "183  2.157694e+06  2.640716e+06  1.951693e+05  \n",
       "199  9.483463e+05  1.891738e+06  1.491976e+06  \n",
       "55   9.589180e+05  5.099315e+05  1.498573e+05  \n",
       "283  6.912848e+05  3.074765e+05  3.147031e+05  \n",
       "346  1.897109e+06  8.650359e+05  1.944161e+05  \n",
       "1    2.816171e+05  2.650556e+05  4.505202e+04  \n",
       "271  2.428831e+06  1.092194e+06  2.152516e+05  \n",
       "122  2.069174e+06  2.058882e+06  1.673921e+05  \n",
       "219  2.050097e+06  2.692654e+06  9.368957e+05  \n",
       "278  3.790628e+06  2.224056e+06  1.315133e+06  \n",
       "201  1.569263e+06  2.975002e+06  6.359603e+05  \n",
       "166  1.176380e+06  5.769330e+05  3.826130e+05  \n",
       "250  1.002985e+05  4.584329e+05  6.251275e+05  \n",
       "154  1.504274e+06  2.337648e+06  1.801657e+06  \n",
       "267  1.395713e+06  1.706582e+06  2.463875e+05  \n",
       "49   7.972346e+05  4.024519e+05  1.222650e+05  \n",
       "274  2.999414e+06  2.532742e+06  1.015587e+06  \n",
       "47   1.312908e+06  8.753283e+05  1.799179e+05  \n",
       "117  1.368909e+06  2.399233e+06  1.352837e+06  \n",
       "3    1.094120e+06  1.694071e+05  1.264754e+04  \n",
       "327  2.902539e+05  4.869473e+05  1.209822e+05  \n",
       "238  3.392243e+06  8.217145e+05  4.764723e+05  \n",
       "..            ...           ...           ...  \n",
       "30   1.554773e+06  2.769618e+05  8.624981e+04  \n",
       "28   9.518763e+05  2.153432e+05  8.008021e+04  \n",
       "120  1.302502e+06  1.833318e+05  1.794897e+06  \n",
       "191  7.740160e+05  7.628231e+05  3.302400e+05  \n",
       "119  1.379593e+06  2.139571e+06  7.459709e+05  \n",
       "5    1.443812e+05  3.356718e+05  2.263950e+04  \n",
       "173  1.474821e+06  1.621261e+06  4.887609e+05  \n",
       "108  1.118495e+06  1.016773e+06  7.712068e+05  \n",
       "138  1.891841e+06  1.404619e+06  2.026595e+06  \n",
       "300  3.186022e+05  5.274169e+03  1.466368e+05  \n",
       "106  1.549802e+06  1.538233e+06  1.940584e+06  \n",
       "94   1.387449e+06  2.449376e+06  9.449341e+05  \n",
       "319  4.355925e+04  5.668921e+04  1.337947e+05  \n",
       "13   3.042241e+05  2.020466e+05  8.550172e+04  \n",
       "230  9.852422e+05  1.868023e+06  5.054020e+05  \n",
       "65   6.696977e+05  6.474149e+05  9.404473e+04  \n",
       "90   3.447648e+06  1.692158e+06  2.723499e+05  \n",
       "237  7.137678e+05  9.676447e+05  3.395261e+06  \n",
       "345  1.341226e+06  3.379098e+05  7.845148e+04  \n",
       "290  4.105189e+06  4.300239e+06  1.440696e+06  \n",
       "340  5.032750e+05  4.829195e+05  2.884198e+05  \n",
       "97   8.780766e+05  2.631342e+06  7.704650e+05  \n",
       "85   6.554789e+05  5.919767e+05  3.243230e+05  \n",
       "164  6.557486e+05  1.071341e+06  1.338284e+06  \n",
       "91   1.899333e+06  2.116712e+06  4.118994e+05  \n",
       "57   3.028144e+05  1.638110e+05  9.189319e+04  \n",
       "42   6.129823e+05  6.310997e+05  3.510898e+04  \n",
       "157  2.444898e+06  2.111298e+06  1.019688e+05  \n",
       "7    9.978994e+04  1.923584e+04  1.647502e+03  \n",
       "222  9.346697e+05  7.835153e+05  7.517303e+05  \n",
       "\n",
       "[353 rows x 20 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add day of the week as a one-hot\n",
    "weekday_labels = []\n",
    "for d in range(7):\n",
    "    label = 'weekday-%i' % d\n",
    "    weekday_labels.append(label)\n",
    "    clients_df[label] = [int(dt.weekday()==d) for dt in clients_df['Time']]\n",
    "    \n",
    "# Put wifi date in the same format as it is in the weather data\n",
    "clients_df['Time'] = [dt.strftime('%Y-%m-%d') for dt in clients_df['Time']]\n",
    "sessions_df['Time'] = [dt.strftime('%Y-%m-%d') for dt in sessions_df['Time']]\n",
    "usage_df['Date'] = [dt.strftime('%Y-%m-%d') for dt in usage_df['Time']]\n",
    "usage_df['Hour'] = [dt.hour for dt in usage_df['Time']]\n",
    "\n",
    "\n",
    "all_data = clients_df.merge(weather_df, left_on='Time', right_on='DATE') \\\n",
    "    .merge(sessions_df, left_on='Time', right_on='Time')\n",
    "\n",
    "# Put 4-hour chunks together into rows by day\n",
    "usage_labels = set()\n",
    "for index, row in usage_df.iterrows():\n",
    "    total_label = 'total-%i' % row['Hour']\n",
    "    all_data.loc[all_data['DATE'] == row['Date'], total_label] \\\n",
    "        = row['Total (B)']\n",
    "    usage_labels.add(total_label)\n",
    "usage_labels = list(usage_labels)\n",
    "# Normalize some inputs using z-scores\n",
    "cols_to_norm = ['TMIN', 'TMAX', 'PRCP']\n",
    "for col in cols_to_norm:\n",
    "    all_data[col] = zscore(all_data[col])\n",
    "\n",
    "# Shuffle rows\n",
    "all_data = all_data.sample(frac=1)\n",
    "    \n",
    "# Separate inputs into categories for encoding\n",
    "day_x = all_data[weekday_labels].values\n",
    "weather_x = all_data[['PRCP', 'TMAX', 'TMIN']].values\n",
    "date_x = np.expand_dims(np.arange(0, 353/365, 1/365).astype('float32'), axis=1)\n",
    "\n",
    "y_labels = ['# Clients'] + usage_labels\n",
    "y = all_data[y_labels].values\n",
    "\n",
    "clients_y=all_data[['# Clients']].values\n",
    "#usage_y=np.log(all_data[usage_labels].values)\n",
    "usage_y=all_data[usage_labels].values\n",
    "#usage_y=usage_y/np.average(usage_y)\n",
    "sessions_y=all_data[['# Sessions']].values\n",
    "\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_79 (InputLayer)           (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_80 (InputLayer)           (None, 7)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_81 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_759 (Dense)               (None, 8)            32          input_79[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_763 (Dense)               (None, 10)           80          input_80[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_767 (Dense)               (None, 10)           20          input_81[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_605 (Dropout)           (None, 8)            0           dense_759[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_608 (Dropout)           (None, 10)           0           dense_763[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_611 (Dropout)           (None, 10)           0           dense_767[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_760 (Dense)               (None, 10)           90          dropout_605[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_764 (Dense)               (None, 10)           110         dropout_608[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_768 (Dense)               (None, 20)           220         dropout_611[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_606 (Dropout)           (None, 10)           0           dense_760[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_609 (Dropout)           (None, 10)           0           dense_764[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_612 (Dropout)           (None, 20)           0           dense_768[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_761 (Dense)               (None, 7)            77          dropout_606[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_765 (Dense)               (None, 10)           110         dropout_609[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_769 (Dense)               (None, 15)           315         dropout_612[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_607 (Dropout)           (None, 7)            0           dense_761[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_610 (Dropout)           (None, 10)           0           dense_765[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_613 (Dropout)           (None, 15)           0           dense_769[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_762 (Dense)               (None, 4)            32          dropout_607[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_766 (Dense)               (None, 4)            44          dropout_610[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_770 (Dense)               (None, 5)            80          dropout_613[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_27 (Concatenate)    (None, 13)           0           dense_762[0][0]                  \n",
      "                                                                 dense_766[0][0]                  \n",
      "                                                                 dense_770[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_771 (Dense)               (None, 50)           700         concatenate_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_614 (Dropout)           (None, 50)           0           dense_771[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_772 (Dense)               (None, 50)           2550        dropout_614[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_615 (Dropout)           (None, 50)           0           dense_772[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_773 (Dense)               (None, 50)           2550        dropout_615[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_616 (Dropout)           (None, 50)           0           dense_773[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_774 (Dense)               (None, 50)           2550        dropout_616[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_617 (Dropout)           (None, 50)           0           dense_774[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_775 (Dense)               (None, 50)           2550        dropout_617[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_618 (Dropout)           (None, 50)           0           dense_775[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_776 (Dense)               (None, 40)           2040        dropout_618[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_619 (Dropout)           (None, 40)           0           dense_776[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_777 (Dense)               (None, 20)           820         dropout_619[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_620 (Dropout)           (None, 20)           0           dense_777[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_781 (Dense)               (None, 25)           1275        dropout_618[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_785 (Dense)               (None, 25)           1275        dropout_618[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_778 (Dense)               (None, 12)           252         dropout_620[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_623 (Dropout)           (None, 25)           0           dense_781[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_626 (Dropout)           (None, 25)           0           dense_785[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_621 (Dropout)           (None, 12)           0           dense_778[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_782 (Dense)               (None, 10)           260         dropout_623[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_786 (Dense)               (None, 10)           260         dropout_626[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_779 (Dense)               (None, 8)            104         dropout_621[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_624 (Dropout)           (None, 10)           0           dense_782[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_627 (Dropout)           (None, 10)           0           dense_786[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_622 (Dropout)           (None, 8)            0           dense_779[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_783 (Dense)               (None, 5)            55          dropout_624[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_787 (Dense)               (None, 5)            55          dropout_627[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_780 (Dense)               (None, 6)            54          dropout_622[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_625 (Dropout)           (None, 5)            0           dense_783[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_628 (Dropout)           (None, 5)            0           dense_787[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 6)            0           dense_780[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_784 (Dense)               (None, 1)            6           dropout_625[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_788 (Dense)               (None, 1)            6           dropout_628[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 18,572\n",
      "Trainable params: 18,572\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "(353, 6)\n",
      "(353, 1)\n",
      "(353, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 317 samples, validate on 36 samples\n",
      "Epoch 1/1000\n",
      "317/317 [==============================] - 6s 18ms/step - loss: 299.8021 - activation_23_loss: 99.9656 - dense_784_loss: 99.8766 - dense_788_loss: 99.9599 - val_loss: 299.8474 - val_activation_23_loss: 99.9945 - val_dense_784_loss: 99.8782 - val_dense_788_loss: 99.9747\n",
      "Epoch 2/1000\n",
      "317/317 [==============================] - 0s 147us/step - loss: 302.3415 - activation_23_loss: 102.9080 - dense_784_loss: 99.5188 - dense_788_loss: 99.9147 - val_loss: 299.5869 - val_activation_23_loss: 99.9868 - val_dense_784_loss: 99.6642 - val_dense_788_loss: 99.9358\n",
      "Epoch 3/1000\n",
      "317/317 [==============================] - 0s 161us/step - loss: 896.7185 - activation_23_loss: 698.0443 - dense_784_loss: 98.8863 - dense_788_loss: 99.7879 - val_loss: 299.3116 - val_activation_23_loss: 99.9868 - val_dense_784_loss: 99.4267 - val_dense_788_loss: 99.8982\n",
      "Epoch 4/1000\n",
      "317/317 [==============================] - 0s 141us/step - loss: 869.2406 - activation_23_loss: 670.5680 - dense_784_loss: 99.0096 - dense_788_loss: 99.6630 - val_loss: 299.0921 - val_activation_23_loss: 99.9900 - val_dense_784_loss: 99.2344 - val_dense_788_loss: 99.8677\n",
      "Epoch 5/1000\n",
      "317/317 [==============================] - 0s 142us/step - loss: 298.0958 - activation_23_loss: 99.9370 - dense_784_loss: 98.4978 - dense_788_loss: 99.6610 - val_loss: 298.7940 - val_activation_23_loss: 99.9910 - val_dense_784_loss: 98.9796 - val_dense_788_loss: 99.8234\n",
      "Epoch 6/1000\n",
      "317/317 [==============================] - 0s 142us/step - loss: 341.8722 - activation_23_loss: 144.7484 - dense_784_loss: 97.7513 - dense_788_loss: 99.3724 - val_loss: 298.3385 - val_activation_23_loss: 99.9912 - val_dense_784_loss: 98.5903 - val_dense_788_loss: 99.7570\n",
      "Epoch 7/1000\n",
      "317/317 [==============================] - 0s 145us/step - loss: 377.1277 - activation_23_loss: 181.3406 - dense_784_loss: 96.7379 - dense_788_loss: 99.0492 - val_loss: 297.8068 - val_activation_23_loss: 99.9930 - val_dense_784_loss: 98.1150 - val_dense_788_loss: 99.6988\n",
      "Epoch 8/1000\n",
      "317/317 [==============================] - 0s 146us/step - loss: 17870.3744 - activation_23_loss: 17675.7232 - dense_784_loss: 95.3580 - dense_788_loss: 99.2924 - val_loss: 297.0137 - val_activation_23_loss: 99.9940 - val_dense_784_loss: 97.4153 - val_dense_788_loss: 99.6044\n",
      "Epoch 9/1000\n",
      "317/317 [==============================] - 0s 145us/step - loss: 293.7397 - activation_23_loss: 99.9645 - dense_784_loss: 94.5335 - dense_788_loss: 99.2417 - val_loss: 296.1676 - val_activation_23_loss: 99.9954 - val_dense_784_loss: 96.6711 - val_dense_788_loss: 99.5011\n",
      "Epoch 10/1000\n",
      "317/317 [==============================] - 0s 142us/step - loss: 292.3959 - activation_23_loss: 101.4829 - dense_784_loss: 92.4355 - dense_788_loss: 98.4775 - val_loss: 294.6499 - val_activation_23_loss: 99.9954 - val_dense_784_loss: 95.3385 - val_dense_788_loss: 99.3160\n",
      "Epoch 11/1000\n",
      "317/317 [==============================] - 0s 145us/step - loss: 290.4542 - activation_23_loss: 101.9919 - dense_784_loss: 90.5402 - dense_788_loss: 97.9222 - val_loss: 292.8160 - val_activation_23_loss: 99.9948 - val_dense_784_loss: 93.7538 - val_dense_788_loss: 99.0674\n",
      "Epoch 12/1000\n",
      "317/317 [==============================] - 0s 149us/step - loss: 285.1250 - activation_23_loss: 100.2012 - dense_784_loss: 87.0937 - dense_788_loss: 97.8301 - val_loss: 289.8036 - val_activation_23_loss: 99.9928 - val_dense_784_loss: 91.1579 - val_dense_788_loss: 98.6528\n",
      "Epoch 13/1000\n",
      "317/317 [==============================] - 0s 149us/step - loss: 454.2414 - activation_23_loss: 273.4714 - dense_784_loss: 84.4534 - dense_788_loss: 96.3166 - val_loss: 286.9891 - val_activation_23_loss: 99.9929 - val_dense_784_loss: 88.8087 - val_dense_788_loss: 98.1875\n",
      "Epoch 14/1000\n",
      "317/317 [==============================] - 0s 142us/step - loss: 1845522679278.3933 - activation_23_loss: 1845522679116.7954 - dense_784_loss: 81.9094 - dense_788_loss: 96.7337 - val_loss: 286.8867 - val_activation_23_loss: 99.9985 - val_dense_784_loss: 88.7717 - val_dense_788_loss: 98.1165\n",
      "Epoch 15/1000\n",
      "317/317 [==============================] - 0s 142us/step - loss: 278.0431 - activation_23_loss: 99.9175 - dense_784_loss: 83.0311 - dense_788_loss: 95.0946 - val_loss: 284.6495 - val_activation_23_loss: 99.9995 - val_dense_784_loss: 86.9799 - val_dense_788_loss: 97.6702\n",
      "Epoch 16/1000\n",
      "317/317 [==============================] - 0s 146us/step - loss: 271.5780 - activation_23_loss: 100.2242 - dense_784_loss: 76.9773 - dense_788_loss: 94.3764 - val_loss: 280.9966 - val_activation_23_loss: 99.9992 - val_dense_784_loss: 84.1412 - val_dense_788_loss: 96.8562\n",
      "Epoch 17/1000\n",
      "317/317 [==============================] - 0s 142us/step - loss: 273.2433 - activation_23_loss: 102.3210 - dense_784_loss: 78.0510 - dense_788_loss: 92.8713 - val_loss: 276.6522 - val_activation_23_loss: 99.9986 - val_dense_784_loss: 80.8678 - val_dense_788_loss: 95.7859\n",
      "Epoch 18/1000\n",
      "317/317 [==============================] - 0s 149us/step - loss: 1464.4852 - activation_23_loss: 1294.7337 - dense_784_loss: 77.2233 - dense_788_loss: 92.5281 - val_loss: 275.8159 - val_activation_23_loss: 99.9986 - val_dense_784_loss: 80.5436 - val_dense_788_loss: 95.2737\n",
      "Epoch 19/1000\n",
      "317/317 [==============================] - 0s 141us/step - loss: 4615.5999 - activation_23_loss: 4446.8209 - dense_784_loss: 79.2717 - dense_788_loss: 89.5070 - val_loss: 274.8828 - val_activation_23_loss: 99.9981 - val_dense_784_loss: 80.4048 - val_dense_788_loss: 94.4800\n",
      "Epoch 20/1000\n",
      "317/317 [==============================] - 0s 145us/step - loss: 270.3187 - activation_23_loss: 102.8151 - dense_784_loss: 76.5340 - dense_788_loss: 90.9696 - val_loss: 274.3242 - val_activation_23_loss: 99.9982 - val_dense_784_loss: 80.3521 - val_dense_788_loss: 93.9738\n",
      "Epoch 21/1000\n",
      "317/317 [==============================] - 0s 142us/step - loss: 265.2230 - activation_23_loss: 99.9745 - dense_784_loss: 77.9984 - dense_788_loss: 87.2501 - val_loss: 273.3712 - val_activation_23_loss: 99.9985 - val_dense_784_loss: 80.0042 - val_dense_788_loss: 93.3685\n",
      "Epoch 22/1000\n",
      "317/317 [==============================] - 0s 149us/step - loss: 272.1499 - activation_23_loss: 100.4668 - dense_784_loss: 84.0575 - dense_788_loss: 87.6255 - val_loss: 273.2692 - val_activation_23_loss: 99.9989 - val_dense_784_loss: 80.6353 - val_dense_788_loss: 92.6350\n",
      "Epoch 23/1000\n",
      "317/317 [==============================] - 0s 149us/step - loss: 42444.0696 - activation_23_loss: 42270.4101 - dense_784_loss: 84.6808 - dense_788_loss: 88.9794 - val_loss: 273.6790 - val_activation_23_loss: 99.9989 - val_dense_784_loss: 81.6475 - val_dense_788_loss: 92.0325\n",
      "Epoch 24/1000\n",
      "317/317 [==============================] - 0s 152us/step - loss: 260.2578 - activation_23_loss: 99.9375 - dense_784_loss: 76.1023 - dense_788_loss: 84.2179 - val_loss: 272.6298 - val_activation_23_loss: 99.9988 - val_dense_784_loss: 81.5485 - val_dense_788_loss: 91.0825\n",
      "Epoch 25/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 395.0954 - activation_23_loss: 235.9588 - dense_784_loss: 75.7891 - dense_788_loss: 83.3475 - val_loss: 271.0680 - val_activation_23_loss: 99.9988 - val_dense_784_loss: 80.9553 - val_dense_788_loss: 90.1140\n",
      "Epoch 26/1000\n",
      "317/317 [==============================] - 0s 142us/step - loss: 279.6296 - activation_23_loss: 106.0826 - dense_784_loss: 87.6678 - dense_788_loss: 85.8792 - val_loss: 271.5866 - val_activation_23_loss: 99.9990 - val_dense_784_loss: 81.8249 - val_dense_788_loss: 89.7628\n",
      "Epoch 27/1000\n",
      "317/317 [==============================] - 0s 146us/step - loss: 260.3982 - activation_23_loss: 99.9621 - dense_784_loss: 73.7582 - dense_788_loss: 86.6779 - val_loss: 270.4097 - val_activation_23_loss: 99.9990 - val_dense_784_loss: 81.3721 - val_dense_788_loss: 89.0387\n",
      "Epoch 28/1000\n",
      "317/317 [==============================] - 0s 142us/step - loss: 258.9611 - activation_23_loss: 99.8979 - dense_784_loss: 72.1422 - dense_788_loss: 86.9210 - val_loss: 268.7907 - val_activation_23_loss: 99.9989 - val_dense_784_loss: 80.5199 - val_dense_788_loss: 88.2719\n",
      "Epoch 29/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "317/317 [==============================] - 0s 142us/step - loss: 263.0383 - activation_23_loss: 100.3770 - dense_784_loss: 80.5351 - dense_788_loss: 82.1263 - val_loss: 268.4995 - val_activation_23_loss: 99.9989 - val_dense_784_loss: 80.4269 - val_dense_788_loss: 88.0737\n",
      "Epoch 30/1000\n",
      "317/317 [==============================] - 0s 145us/step - loss: 253.6129 - activation_23_loss: 99.9522 - dense_784_loss: 74.5628 - dense_788_loss: 79.0979 - val_loss: 267.5163 - val_activation_23_loss: 99.9989 - val_dense_784_loss: 80.0432 - val_dense_788_loss: 87.4742\n",
      "Epoch 31/1000\n",
      "317/317 [==============================] - 0s 142us/step - loss: 268.7203 - activation_23_loss: 99.9008 - dense_784_loss: 83.6092 - dense_788_loss: 85.2103 - val_loss: 266.4422 - val_activation_23_loss: 99.9989 - val_dense_784_loss: 79.6672 - val_dense_788_loss: 86.7761\n",
      "Epoch 32/1000\n",
      "317/317 [==============================] - 0s 146us/step - loss: 255.6690 - activation_23_loss: 99.8900 - dense_784_loss: 74.9870 - dense_788_loss: 80.7919 - val_loss: 265.6914 - val_activation_23_loss: 99.9989 - val_dense_784_loss: 79.3753 - val_dense_788_loss: 86.3171\n",
      "Epoch 33/1000\n",
      "317/317 [==============================] - 0s 144us/step - loss: 253.8161 - activation_23_loss: 99.9248 - dense_784_loss: 74.9115 - dense_788_loss: 78.9797 - val_loss: 264.0918 - val_activation_23_loss: 99.9989 - val_dense_784_loss: 78.6246 - val_dense_788_loss: 85.4683\n",
      "Epoch 34/1000\n",
      "317/317 [==============================] - 0s 146us/step - loss: 407.6065 - activation_23_loss: 249.4519 - dense_784_loss: 78.1207 - dense_788_loss: 80.0339 - val_loss: 263.5430 - val_activation_23_loss: 99.9989 - val_dense_784_loss: 78.4250 - val_dense_788_loss: 85.1190\n",
      "Epoch 35/1000\n",
      "317/317 [==============================] - 0s 145us/step - loss: 261.9223 - activation_23_loss: 99.9234 - dense_784_loss: 80.2681 - dense_788_loss: 81.7308 - val_loss: 263.4845 - val_activation_23_loss: 99.9990 - val_dense_784_loss: 78.6567 - val_dense_788_loss: 84.8288\n",
      "Epoch 36/1000\n",
      "317/317 [==============================] - 0s 142us/step - loss: 253.0386 - activation_23_loss: 99.9500 - dense_784_loss: 75.8847 - dense_788_loss: 77.2039 - val_loss: 262.7176 - val_activation_23_loss: 99.9990 - val_dense_784_loss: 78.3750 - val_dense_788_loss: 84.3436\n",
      "Epoch 37/1000\n",
      "317/317 [==============================] - 0s 145us/step - loss: 254.8108 - activation_23_loss: 103.7552 - dense_784_loss: 74.4400 - dense_788_loss: 76.6155 - val_loss: 261.4877 - val_activation_23_loss: 99.9990 - val_dense_784_loss: 77.7504 - val_dense_788_loss: 83.7383\n",
      "Epoch 38/1000\n",
      "317/317 [==============================] - 0s 146us/step - loss: 271.7906 - activation_23_loss: 100.2561 - dense_784_loss: 76.9715 - dense_788_loss: 94.5629 - val_loss: 260.4928 - val_activation_23_loss: 99.9990 - val_dense_784_loss: 77.1691 - val_dense_788_loss: 83.3248\n",
      "Epoch 39/1000\n",
      "317/317 [==============================] - 0s 145us/step - loss: 258.6223 - activation_23_loss: 99.8776 - dense_784_loss: 77.5921 - dense_788_loss: 81.1525 - val_loss: 260.0931 - val_activation_23_loss: 99.9989 - val_dense_784_loss: 76.9905 - val_dense_788_loss: 83.1037\n",
      "Epoch 40/1000\n",
      "317/317 [==============================] - 0s 146us/step - loss: 248.4412 - activation_23_loss: 99.9880 - dense_784_loss: 73.2074 - dense_788_loss: 75.2459 - val_loss: 259.5371 - val_activation_23_loss: 99.9989 - val_dense_784_loss: 76.7977 - val_dense_788_loss: 82.7406\n",
      "Epoch 41/1000\n",
      "317/317 [==============================] - 0s 145us/step - loss: 253.6430 - activation_23_loss: 99.9198 - dense_784_loss: 74.9515 - dense_788_loss: 78.7718 - val_loss: 258.6271 - val_activation_23_loss: 99.9989 - val_dense_784_loss: 76.5480 - val_dense_788_loss: 82.0803\n",
      "Epoch 42/1000\n",
      "317/317 [==============================] - 0s 142us/step - loss: 299.7871 - activation_23_loss: 140.6476 - dense_784_loss: 76.5112 - dense_788_loss: 82.6283 - val_loss: 258.5962 - val_activation_23_loss: 99.9989 - val_dense_784_loss: 76.7551 - val_dense_788_loss: 81.8423\n",
      "Epoch 43/1000\n",
      "317/317 [==============================] - 0s 142us/step - loss: 259.1480 - activation_23_loss: 100.1102 - dense_784_loss: 80.3220 - dense_788_loss: 78.7158 - val_loss: 258.6801 - val_activation_23_loss: 99.9990 - val_dense_784_loss: 76.9455 - val_dense_788_loss: 81.7356\n",
      "Epoch 44/1000\n",
      "317/317 [==============================] - 0s 147us/step - loss: 251.5314 - activation_23_loss: 99.9776 - dense_784_loss: 73.3463 - dense_788_loss: 78.2076 - val_loss: 258.0459 - val_activation_23_loss: 99.9990 - val_dense_784_loss: 76.7377 - val_dense_788_loss: 81.3093\n",
      "Epoch 45/1000\n",
      "317/317 [==============================] - 0s 149us/step - loss: 254299337.3814 - activation_23_loss: 254299203.3327 - dense_784_loss: 73.8775 - dense_788_loss: 74.8757 - val_loss: 257.4653 - val_activation_23_loss: 99.9990 - val_dense_784_loss: 76.4796 - val_dense_788_loss: 80.9867\n",
      "Epoch 46/1000\n",
      "317/317 [==============================] - 0s 152us/step - loss: 248.2673 - activation_23_loss: 102.9724 - dense_784_loss: 71.8770 - dense_788_loss: 73.4180 - val_loss: 257.5860 - val_activation_23_loss: 99.9991 - val_dense_784_loss: 76.6604 - val_dense_788_loss: 80.9264\n",
      "Epoch 47/1000\n",
      "317/317 [==============================] - 0s 160us/step - loss: 250.4639 - activation_23_loss: 102.7622 - dense_784_loss: 71.3698 - dense_788_loss: 76.3319 - val_loss: 256.8796 - val_activation_23_loss: 99.9992 - val_dense_784_loss: 76.2622 - val_dense_788_loss: 80.6182\n",
      "Epoch 48/1000\n",
      "317/317 [==============================] - 0s 146us/step - loss: 247.7607 - activation_23_loss: 99.9731 - dense_784_loss: 72.7094 - dense_788_loss: 75.0782 - val_loss: 255.9483 - val_activation_23_loss: 99.9992 - val_dense_784_loss: 75.7642 - val_dense_788_loss: 80.1849\n",
      "Epoch 49/1000\n",
      "317/317 [==============================] - 0s 142us/step - loss: 251.7378 - activation_23_loss: 99.9652 - dense_784_loss: 74.0102 - dense_788_loss: 77.7624 - val_loss: 254.6840 - val_activation_23_loss: 99.9992 - val_dense_784_loss: 75.0812 - val_dense_788_loss: 79.6036\n",
      "Epoch 50/1000\n",
      "317/317 [==============================] - 0s 146us/step - loss: 245.3864 - activation_23_loss: 99.9574 - dense_784_loss: 69.0031 - dense_788_loss: 76.4258 - val_loss: 253.1494 - val_activation_23_loss: 99.9992 - val_dense_784_loss: 73.8016 - val_dense_788_loss: 79.3487\n",
      "Epoch 51/1000\n",
      "317/317 [==============================] - 0s 144us/step - loss: 275.5891 - activation_23_loss: 99.9099 - dense_784_loss: 78.0476 - dense_788_loss: 97.6316 - val_loss: 252.7690 - val_activation_23_loss: 99.9992 - val_dense_784_loss: 73.2698 - val_dense_788_loss: 79.5001\n",
      "Epoch 52/1000\n",
      "317/317 [==============================] - 0s 145us/step - loss: 268.9422 - activation_23_loss: 102.5750 - dense_784_loss: 79.5371 - dense_788_loss: 86.8300 - val_loss: 254.1004 - val_activation_23_loss: 99.9992 - val_dense_784_loss: 73.6877 - val_dense_788_loss: 80.4134\n",
      "Epoch 53/1000\n",
      "317/317 [==============================] - 0s 142us/step - loss: 247.9289 - activation_23_loss: 99.9780 - dense_784_loss: 67.0983 - dense_788_loss: 80.8525 - val_loss: 255.7189 - val_activation_23_loss: 99.9993 - val_dense_784_loss: 74.4380 - val_dense_788_loss: 81.2815\n",
      "Epoch 54/1000\n",
      "317/317 [==============================] - 0s 142us/step - loss: 291.4112 - activation_23_loss: 142.4520 - dense_784_loss: 73.2910 - dense_788_loss: 75.6682 - val_loss: 255.5817 - val_activation_23_loss: 99.9994 - val_dense_784_loss: 74.2709 - val_dense_788_loss: 81.3115\n",
      "Epoch 55/1000\n",
      "317/317 [==============================] - 0s 142us/step - loss: 258.7335 - activation_23_loss: 100.2801 - dense_784_loss: 79.9604 - dense_788_loss: 78.4930 - val_loss: 255.8828 - val_activation_23_loss: 99.9993 - val_dense_784_loss: 74.6582 - val_dense_788_loss: 81.2253\n",
      "Epoch 56/1000\n",
      "317/317 [==============================] - 0s 149us/step - loss: 258.1172 - activation_23_loss: 99.9576 - dense_784_loss: 74.6165 - dense_788_loss: 83.5431 - val_loss: 255.4808 - val_activation_23_loss: 99.9993 - val_dense_784_loss: 74.4721 - val_dense_788_loss: 81.0095\n",
      "Epoch 57/1000\n",
      "317/317 [==============================] - 0s 146us/step - loss: 5762.0240 - activation_23_loss: 5611.2756 - dense_784_loss: 74.0757 - dense_788_loss: 76.6727 - val_loss: 256.9605 - val_activation_23_loss: 99.9992 - val_dense_784_loss: 75.1262 - val_dense_788_loss: 81.8351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/1000\n",
      "317/317 [==============================] - 0s 145us/step - loss: 266.3561 - activation_23_loss: 99.9509 - dense_784_loss: 79.0103 - dense_788_loss: 87.3950 - val_loss: 257.0806 - val_activation_23_loss: 99.9991 - val_dense_784_loss: 75.2305 - val_dense_788_loss: 81.8511\n",
      "Epoch 59/1000\n",
      "317/317 [==============================] - 0s 142us/step - loss: 244.8642 - activation_23_loss: 99.9629 - dense_784_loss: 71.0440 - dense_788_loss: 73.8573 - val_loss: 256.3259 - val_activation_23_loss: 99.9989 - val_dense_784_loss: 74.6729 - val_dense_788_loss: 81.6540\n",
      "Epoch 60/1000\n",
      "317/317 [==============================] - 0s 142us/step - loss: 254.1792 - activation_23_loss: 99.9216 - dense_784_loss: 79.1431 - dense_788_loss: 75.1145 - val_loss: 255.5503 - val_activation_23_loss: 99.9987 - val_dense_784_loss: 74.3235 - val_dense_788_loss: 81.2280\n",
      "Epoch 61/1000\n",
      "317/317 [==============================] - 0s 147us/step - loss: 246.8235 - activation_23_loss: 99.9182 - dense_784_loss: 70.4158 - dense_788_loss: 76.4895 - val_loss: 253.8502 - val_activation_23_loss: 99.9985 - val_dense_784_loss: 73.3020 - val_dense_788_loss: 80.5497\n",
      "Epoch 62/1000\n",
      "317/317 [==============================] - 0s 146us/step - loss: 254.9425 - activation_23_loss: 99.9266 - dense_784_loss: 76.6126 - dense_788_loss: 78.4032 - val_loss: 252.6890 - val_activation_23_loss: 99.9984 - val_dense_784_loss: 72.5917 - val_dense_788_loss: 80.0989\n",
      "Epoch 63/1000\n",
      "317/317 [==============================] - 0s 144us/step - loss: 253.9813 - activation_23_loss: 99.9327 - dense_784_loss: 72.1309 - dense_788_loss: 81.9177 - val_loss: 251.8987 - val_activation_23_loss: 99.9983 - val_dense_784_loss: 72.0603 - val_dense_788_loss: 79.8401\n",
      "Epoch 64/1000\n",
      "317/317 [==============================] - 0s 141us/step - loss: 247.4299 - activation_23_loss: 99.9778 - dense_784_loss: 71.1812 - dense_788_loss: 76.2709 - val_loss: 251.7103 - val_activation_23_loss: 99.9983 - val_dense_784_loss: 71.9711 - val_dense_788_loss: 79.7408\n",
      "Epoch 65/1000\n",
      "317/317 [==============================] - 0s 142us/step - loss: 251.9060 - activation_23_loss: 99.9756 - dense_784_loss: 75.6257 - dense_788_loss: 76.3046 - val_loss: 251.0409 - val_activation_23_loss: 99.9983 - val_dense_784_loss: 71.6166 - val_dense_788_loss: 79.4260\n",
      "Epoch 66/1000\n",
      "317/317 [==============================] - 0s 146us/step - loss: 450.3750 - activation_23_loss: 303.6885 - dense_784_loss: 74.2574 - dense_788_loss: 72.4291 - val_loss: 250.4178 - val_activation_23_loss: 99.9986 - val_dense_784_loss: 71.2086 - val_dense_788_loss: 79.2105\n",
      "Epoch 67/1000\n",
      "317/317 [==============================] - 0s 149us/step - loss: 244.3953 - activation_23_loss: 99.8866 - dense_784_loss: 66.4344 - dense_788_loss: 78.0743 - val_loss: 249.4706 - val_activation_23_loss: 99.9986 - val_dense_784_loss: 70.4323 - val_dense_788_loss: 79.0396\n",
      "Epoch 68/1000\n",
      "317/317 [==============================] - 0s 149us/step - loss: 244.0937 - activation_23_loss: 99.9450 - dense_784_loss: 71.0382 - dense_788_loss: 73.1105 - val_loss: 248.2121 - val_activation_23_loss: 99.9986 - val_dense_784_loss: 69.4876 - val_dense_788_loss: 78.7259\n",
      "Epoch 69/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 243.6824 - activation_23_loss: 99.9435 - dense_784_loss: 76.3024 - dense_788_loss: 67.4365 - val_loss: 247.8136 - val_activation_23_loss: 99.9986 - val_dense_784_loss: 69.4854 - val_dense_788_loss: 78.3296\n",
      "Epoch 70/1000\n",
      "317/317 [==============================] - 0s 142us/step - loss: 248.1852 - activation_23_loss: 99.9171 - dense_784_loss: 72.3239 - dense_788_loss: 75.9442 - val_loss: 246.7546 - val_activation_23_loss: 99.9985 - val_dense_784_loss: 69.1071 - val_dense_788_loss: 77.6490\n",
      "Epoch 71/1000\n",
      "317/317 [==============================] - 0s 139us/step - loss: 267.0468 - activation_23_loss: 99.9286 - dense_784_loss: 82.0949 - dense_788_loss: 85.0233 - val_loss: 247.2687 - val_activation_23_loss: 99.9985 - val_dense_784_loss: 69.5736 - val_dense_788_loss: 77.6966\n",
      "Epoch 72/1000\n",
      "317/317 [==============================] - 0s 142us/step - loss: 244.8650 - activation_23_loss: 99.9650 - dense_784_loss: 68.5875 - dense_788_loss: 76.3126 - val_loss: 246.8516 - val_activation_23_loss: 99.9985 - val_dense_784_loss: 69.2576 - val_dense_788_loss: 77.5955\n",
      "Epoch 73/1000\n",
      "317/317 [==============================] - 0s 145us/step - loss: 249.6750 - activation_23_loss: 99.9289 - dense_784_loss: 72.5771 - dense_788_loss: 77.1690 - val_loss: 246.8473 - val_activation_23_loss: 99.9986 - val_dense_784_loss: 69.2652 - val_dense_788_loss: 77.5835\n",
      "Epoch 74/1000\n",
      "317/317 [==============================] - 0s 142us/step - loss: 259.8051 - activation_23_loss: 99.9615 - dense_784_loss: 74.0881 - dense_788_loss: 85.7556 - val_loss: 246.9597 - val_activation_23_loss: 99.9986 - val_dense_784_loss: 69.2393 - val_dense_788_loss: 77.7218\n",
      "Epoch 75/1000\n",
      "317/317 [==============================] - 0s 142us/step - loss: 249.2057 - activation_23_loss: 99.9612 - dense_784_loss: 79.3905 - dense_788_loss: 69.8540 - val_loss: 247.4277 - val_activation_23_loss: 99.9986 - val_dense_784_loss: 69.9375 - val_dense_788_loss: 77.4916\n",
      "Epoch 76/1000\n",
      "317/317 [==============================] - 0s 147us/step - loss: 248.7497 - activation_23_loss: 100.6680 - dense_784_loss: 69.7947 - dense_788_loss: 78.2871 - val_loss: 247.2644 - val_activation_23_loss: 99.9986 - val_dense_784_loss: 69.9846 - val_dense_788_loss: 77.2812\n",
      "Epoch 77/1000\n",
      "317/317 [==============================] - 0s 149us/step - loss: 265.5934 - activation_23_loss: 99.9307 - dense_784_loss: 78.4711 - dense_788_loss: 87.1915 - val_loss: 247.8840 - val_activation_23_loss: 99.9987 - val_dense_784_loss: 70.3898 - val_dense_788_loss: 77.4955\n",
      "Epoch 78/1000\n",
      "317/317 [==============================] - 0s 142us/step - loss: 252.1848 - activation_23_loss: 99.9256 - dense_784_loss: 72.3537 - dense_788_loss: 79.9056 - val_loss: 249.7743 - val_activation_23_loss: 99.9989 - val_dense_784_loss: 71.3270 - val_dense_788_loss: 78.4484\n",
      "Epoch 79/1000\n",
      "317/317 [==============================] - 0s 146us/step - loss: 251.9130 - activation_23_loss: 99.9849 - dense_784_loss: 68.8524 - dense_788_loss: 83.0757 - val_loss: 250.0565 - val_activation_23_loss: 99.9990 - val_dense_784_loss: 71.2216 - val_dense_788_loss: 78.8359\n",
      "Epoch 80/1000\n",
      "317/317 [==============================] - 0s 145us/step - loss: 258.0466 - activation_23_loss: 99.9532 - dense_784_loss: 78.7067 - dense_788_loss: 79.3867 - val_loss: 249.8125 - val_activation_23_loss: 99.9990 - val_dense_784_loss: 71.1297 - val_dense_788_loss: 78.6838\n",
      "Epoch 81/1000\n",
      "317/317 [==============================] - 0s 146us/step - loss: 246.0394 - activation_23_loss: 99.9538 - dense_784_loss: 72.9758 - dense_788_loss: 73.1098 - val_loss: 249.3220 - val_activation_23_loss: 99.9989 - val_dense_784_loss: 70.9363 - val_dense_788_loss: 78.3867\n",
      "Epoch 82/1000\n",
      "317/317 [==============================] - 0s 145us/step - loss: 255.0464 - activation_23_loss: 101.1639 - dense_784_loss: 75.7490 - dense_788_loss: 78.1335 - val_loss: 248.8754 - val_activation_23_loss: 99.9989 - val_dense_784_loss: 70.6978 - val_dense_788_loss: 78.1787\n",
      "Epoch 83/1000\n",
      "317/317 [==============================] - 0s 142us/step - loss: 248.4697 - activation_23_loss: 99.9748 - dense_784_loss: 69.7611 - dense_788_loss: 78.7339 - val_loss: 248.2352 - val_activation_23_loss: 99.9988 - val_dense_784_loss: 70.2292 - val_dense_788_loss: 78.0072\n",
      "Epoch 84/1000\n",
      "317/317 [==============================] - 0s 142us/step - loss: 250.0678 - activation_23_loss: 102.1430 - dense_784_loss: 67.5127 - dense_788_loss: 80.4120 - val_loss: 247.1696 - val_activation_23_loss: 99.9988 - val_dense_784_loss: 69.3624 - val_dense_788_loss: 77.8084\n",
      "Epoch 85/1000\n",
      "317/317 [==============================] - 0s 142us/step - loss: 250.9361 - activation_23_loss: 99.9666 - dense_784_loss: 73.9976 - dense_788_loss: 76.9719 - val_loss: 247.1583 - val_activation_23_loss: 99.9988 - val_dense_784_loss: 69.1011 - val_dense_788_loss: 78.0584\n",
      "Epoch 86/1000\n",
      "317/317 [==============================] - 0s 149us/step - loss: 244.3025 - activation_23_loss: 99.8889 - dense_784_loss: 71.7048 - dense_788_loss: 72.7088 - val_loss: 246.7116 - val_activation_23_loss: 99.9988 - val_dense_784_loss: 68.7931 - val_dense_788_loss: 77.9197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/1000\n",
      "317/317 [==============================] - 0s 149us/step - loss: 245.0183 - activation_23_loss: 99.9461 - dense_784_loss: 67.3091 - dense_788_loss: 77.7631 - val_loss: 245.6553 - val_activation_23_loss: 99.9988 - val_dense_784_loss: 68.1267 - val_dense_788_loss: 77.5299\n",
      "Epoch 88/1000\n",
      "317/317 [==============================] - 0s 142us/step - loss: 328.9463 - activation_23_loss: 186.5074 - dense_784_loss: 66.9694 - dense_788_loss: 75.4695 - val_loss: 245.7733 - val_activation_23_loss: 99.9990 - val_dense_784_loss: 67.9959 - val_dense_788_loss: 77.7783\n",
      "Epoch 89/1000\n",
      "317/317 [==============================] - 0s 149us/step - loss: 254.8845 - activation_23_loss: 99.8924 - dense_784_loss: 75.5060 - dense_788_loss: 79.4860 - val_loss: 245.9132 - val_activation_23_loss: 99.9992 - val_dense_784_loss: 67.9787 - val_dense_788_loss: 77.9353\n",
      "Epoch 90/1000\n",
      "317/317 [==============================] - 0s 149us/step - loss: 254.0499 - activation_23_loss: 99.9731 - dense_784_loss: 77.1620 - dense_788_loss: 76.9147 - val_loss: 246.4585 - val_activation_23_loss: 99.9992 - val_dense_784_loss: 68.5413 - val_dense_788_loss: 77.9181\n",
      "Epoch 91/1000\n",
      "317/317 [==============================] - 0s 161us/step - loss: 246.6720 - activation_23_loss: 99.9690 - dense_784_loss: 73.3174 - dense_788_loss: 73.3855 - val_loss: 246.1539 - val_activation_23_loss: 99.9992 - val_dense_784_loss: 68.7361 - val_dense_788_loss: 77.4187\n",
      "Epoch 92/1000\n",
      "317/317 [==============================] - 0s 142us/step - loss: 269.5651 - activation_23_loss: 99.9601 - dense_784_loss: 74.2475 - dense_788_loss: 95.3575 - val_loss: 245.7611 - val_activation_23_loss: 99.9991 - val_dense_784_loss: 68.7774 - val_dense_788_loss: 76.9846\n",
      "Epoch 93/1000\n",
      "317/317 [==============================] - 0s 144us/step - loss: 251.4494 - activation_23_loss: 99.9858 - dense_784_loss: 74.1098 - dense_788_loss: 77.3538 - val_loss: 246.2985 - val_activation_23_loss: 99.9991 - val_dense_784_loss: 68.9428 - val_dense_788_loss: 77.3565\n",
      "Epoch 94/1000\n",
      "317/317 [==============================] - 0s 144us/step - loss: 247.4884 - activation_23_loss: 99.9502 - dense_784_loss: 72.5681 - dense_788_loss: 74.9702 - val_loss: 246.7333 - val_activation_23_loss: 99.9991 - val_dense_784_loss: 69.2844 - val_dense_788_loss: 77.4498\n",
      "Epoch 95/1000\n",
      "317/317 [==============================] - 0s 144us/step - loss: 257.9712 - activation_23_loss: 99.9776 - dense_784_loss: 79.4609 - dense_788_loss: 78.5328 - val_loss: 246.0675 - val_activation_23_loss: 99.9991 - val_dense_784_loss: 69.1250 - val_dense_788_loss: 76.9435\n",
      "Epoch 96/1000\n",
      "317/317 [==============================] - 0s 147us/step - loss: 255.4935 - activation_23_loss: 99.9809 - dense_784_loss: 71.8049 - dense_788_loss: 83.7077 - val_loss: 245.8029 - val_activation_23_loss: 99.9990 - val_dense_784_loss: 69.0993 - val_dense_788_loss: 76.7045\n",
      "Epoch 97/1000\n",
      "317/317 [==============================] - 0s 142us/step - loss: 255.6463 - activation_23_loss: 99.9727 - dense_784_loss: 77.0105 - dense_788_loss: 78.6630 - val_loss: 245.9687 - val_activation_23_loss: 99.9990 - val_dense_784_loss: 69.2988 - val_dense_788_loss: 76.6709\n",
      "Epoch 98/1000\n",
      "317/317 [==============================] - 0s 142us/step - loss: 254.9992 - activation_23_loss: 99.9899 - dense_784_loss: 77.8059 - dense_788_loss: 77.2034 - val_loss: 245.5537 - val_activation_23_loss: 99.9990 - val_dense_784_loss: 69.2071 - val_dense_788_loss: 76.3476\n",
      "Epoch 99/1000\n",
      "317/317 [==============================] - 0s 145us/step - loss: 245.1364 - activation_23_loss: 104.6106 - dense_784_loss: 69.1557 - dense_788_loss: 71.3701 - val_loss: 244.9362 - val_activation_23_loss: 99.9989 - val_dense_784_loss: 68.9856 - val_dense_788_loss: 75.9517\n",
      "Epoch 100/1000\n",
      "317/317 [==============================] - 0s 142us/step - loss: 252.2835 - activation_23_loss: 99.9799 - dense_784_loss: 72.6390 - dense_788_loss: 79.6646 - val_loss: 244.4263 - val_activation_23_loss: 99.9989 - val_dense_784_loss: 68.6881 - val_dense_788_loss: 75.7393\n",
      "Epoch 101/1000\n",
      "317/317 [==============================] - 0s 142us/step - loss: 253.2563 - activation_23_loss: 99.9801 - dense_784_loss: 68.0871 - dense_788_loss: 85.1891 - val_loss: 245.0086 - val_activation_23_loss: 99.9990 - val_dense_784_loss: 68.4576 - val_dense_788_loss: 76.5519\n",
      "Epoch 102/1000\n",
      "317/317 [==============================] - 0s 142us/step - loss: 248.6806 - activation_23_loss: 99.9915 - dense_784_loss: 72.1592 - dense_788_loss: 76.5299 - val_loss: 245.0411 - val_activation_23_loss: 99.9990 - val_dense_784_loss: 68.3274 - val_dense_788_loss: 76.7146\n",
      "Epoch 103/1000\n",
      "317/317 [==============================] - 0s 145us/step - loss: 260.2802 - activation_23_loss: 99.9757 - dense_784_loss: 73.8160 - dense_788_loss: 86.4884 - val_loss: 245.9194 - val_activation_23_loss: 99.9991 - val_dense_784_loss: 68.6796 - val_dense_788_loss: 77.2408\n",
      "Epoch 104/1000\n",
      "317/317 [==============================] - 0s 149us/step - loss: 970.1810 - activation_23_loss: 820.8336 - dense_784_loss: 70.2064 - dense_788_loss: 79.1409 - val_loss: 247.1739 - val_activation_23_loss: 99.9993 - val_dense_784_loss: 68.9750 - val_dense_788_loss: 78.1996\n",
      "Epoch 105/1000\n",
      "317/317 [==============================] - 0s 145us/step - loss: 252.7837 - activation_23_loss: 99.9843 - dense_784_loss: 70.3693 - dense_788_loss: 82.4301 - val_loss: 247.3597 - val_activation_23_loss: 99.9994 - val_dense_784_loss: 68.5818 - val_dense_788_loss: 78.7786\n",
      "Epoch 106/1000\n",
      "317/317 [==============================] - 0s 146us/step - loss: 249.5974 - activation_23_loss: 99.9905 - dense_784_loss: 71.7326 - dense_788_loss: 77.8743 - val_loss: 246.8048 - val_activation_23_loss: 99.9994 - val_dense_784_loss: 68.0356 - val_dense_788_loss: 78.7698\n",
      "Epoch 107/1000\n",
      "317/317 [==============================] - 0s 142us/step - loss: 247.6950 - activation_23_loss: 99.9554 - dense_784_loss: 68.0173 - dense_788_loss: 79.7223 - val_loss: 245.6352 - val_activation_23_loss: 99.9994 - val_dense_784_loss: 67.1964 - val_dense_788_loss: 78.4394\n",
      "Epoch 108/1000\n",
      "317/317 [==============================] - 0s 147us/step - loss: 241.1771 - activation_23_loss: 99.9793 - dense_784_loss: 70.6091 - dense_788_loss: 70.5887 - val_loss: 245.1363 - val_activation_23_loss: 99.9994 - val_dense_784_loss: 66.8732 - val_dense_788_loss: 78.2637\n",
      "Epoch 109/1000\n",
      "317/317 [==============================] - 0s 147us/step - loss: 259.1656 - activation_23_loss: 100.0553 - dense_784_loss: 76.6029 - dense_788_loss: 82.5074 - val_loss: 244.4888 - val_activation_23_loss: 99.9994 - val_dense_784_loss: 66.5363 - val_dense_788_loss: 77.9532\n",
      "Epoch 110/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 250.8309 - activation_23_loss: 99.9536 - dense_784_loss: 72.5410 - dense_788_loss: 78.3364 - val_loss: 244.6990 - val_activation_23_loss: 99.9994 - val_dense_784_loss: 66.7648 - val_dense_788_loss: 77.9348\n",
      "Epoch 111/1000\n",
      "317/317 [==============================] - 0s 144us/step - loss: 275.0159 - activation_23_loss: 99.9824 - dense_784_loss: 73.8250 - dense_788_loss: 101.2085 - val_loss: 244.0894 - val_activation_23_loss: 99.9994 - val_dense_784_loss: 66.2884 - val_dense_788_loss: 77.8015\n",
      "Epoch 112/1000\n",
      "317/317 [==============================] - 0s 145us/step - loss: 243.1540 - activation_23_loss: 99.9873 - dense_784_loss: 67.5009 - dense_788_loss: 75.6657 - val_loss: 243.8395 - val_activation_23_loss: 99.9994 - val_dense_784_loss: 65.8964 - val_dense_788_loss: 77.9437\n",
      "Epoch 113/1000\n",
      "317/317 [==============================] - 0s 161us/step - loss: 241.1308 - activation_23_loss: 99.9762 - dense_784_loss: 68.0583 - dense_788_loss: 73.0962 - val_loss: 242.9052 - val_activation_23_loss: 99.9993 - val_dense_784_loss: 65.2912 - val_dense_788_loss: 77.6147\n",
      "Epoch 114/1000\n",
      "317/317 [==============================] - 0s 145us/step - loss: 241.3028 - activation_23_loss: 99.9947 - dense_784_loss: 69.7729 - dense_788_loss: 71.5352 - val_loss: 242.2530 - val_activation_23_loss: 99.9993 - val_dense_784_loss: 65.0463 - val_dense_788_loss: 77.2074\n",
      "Epoch 115/1000\n",
      "317/317 [==============================] - 0s 139us/step - loss: 240.1786 - activation_23_loss: 99.9910 - dense_784_loss: 64.6031 - dense_788_loss: 75.5844 - val_loss: 240.7675 - val_activation_23_loss: 99.9993 - val_dense_784_loss: 64.0797 - val_dense_788_loss: 76.6884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116/1000\n",
      "317/317 [==============================] - 0s 149us/step - loss: 243.7336 - activation_23_loss: 100.0417 - dense_784_loss: 68.7598 - dense_788_loss: 74.9320 - val_loss: 239.8531 - val_activation_23_loss: 99.9993 - val_dense_784_loss: 63.4815 - val_dense_788_loss: 76.3722\n",
      "Epoch 117/1000\n",
      "317/317 [==============================] - 0s 142us/step - loss: 238.4076 - activation_23_loss: 99.9868 - dense_784_loss: 65.3480 - dense_788_loss: 73.0728 - val_loss: 238.6958 - val_activation_23_loss: 99.9993 - val_dense_784_loss: 62.7337 - val_dense_788_loss: 75.9627\n",
      "Epoch 118/1000\n",
      "317/317 [==============================] - 0s 142us/step - loss: 237.7539 - activation_23_loss: 99.9919 - dense_784_loss: 64.5855 - dense_788_loss: 73.1765 - val_loss: 238.1328 - val_activation_23_loss: 99.9993 - val_dense_784_loss: 62.4113 - val_dense_788_loss: 75.7221\n",
      "Epoch 119/1000\n",
      "317/317 [==============================] - 0s 142us/step - loss: 243.2046 - activation_23_loss: 99.9582 - dense_784_loss: 69.9978 - dense_788_loss: 73.2486 - val_loss: 237.8174 - val_activation_23_loss: 99.9993 - val_dense_784_loss: 62.3014 - val_dense_788_loss: 75.5167\n",
      "Epoch 120/1000\n",
      "317/317 [==============================] - 0s 149us/step - loss: 244.1932 - activation_23_loss: 99.9834 - dense_784_loss: 66.9696 - dense_788_loss: 77.2402 - val_loss: 237.7083 - val_activation_23_loss: 99.9993 - val_dense_784_loss: 62.3786 - val_dense_788_loss: 75.3304\n",
      "Epoch 121/1000\n",
      "317/317 [==============================] - 0s 146us/step - loss: 251.5888 - activation_23_loss: 99.9651 - dense_784_loss: 70.0454 - dense_788_loss: 81.5783 - val_loss: 237.2400 - val_activation_23_loss: 99.9993 - val_dense_784_loss: 62.1095 - val_dense_788_loss: 75.1312\n",
      "Epoch 122/1000\n",
      "317/317 [==============================] - 0s 145us/step - loss: 239.9390 - activation_23_loss: 99.9850 - dense_784_loss: 66.9058 - dense_788_loss: 73.0482 - val_loss: 236.3829 - val_activation_23_loss: 99.9993 - val_dense_784_loss: 61.4936 - val_dense_788_loss: 74.8901\n",
      "Epoch 123/1000\n",
      "317/317 [==============================] - 0s 144us/step - loss: 238.7605 - activation_23_loss: 99.9536 - dense_784_loss: 64.9516 - dense_788_loss: 73.8552 - val_loss: 235.4715 - val_activation_23_loss: 99.9993 - val_dense_784_loss: 60.8650 - val_dense_788_loss: 74.6072\n",
      "Epoch 124/1000\n",
      "317/317 [==============================] - 0s 141us/step - loss: 251.1107 - activation_23_loss: 99.9810 - dense_784_loss: 76.0546 - dense_788_loss: 75.0751 - val_loss: 236.1173 - val_activation_23_loss: 99.9993 - val_dense_784_loss: 61.1462 - val_dense_788_loss: 74.9719\n",
      "Epoch 125/1000\n",
      "317/317 [==============================] - 0s 147us/step - loss: 242.7854 - activation_23_loss: 99.9575 - dense_784_loss: 67.4506 - dense_788_loss: 75.3773 - val_loss: 235.7511 - val_activation_23_loss: 99.9993 - val_dense_784_loss: 60.7624 - val_dense_788_loss: 74.9894\n",
      "Epoch 126/1000\n",
      "317/317 [==============================] - 0s 142us/step - loss: 244.5758 - activation_23_loss: 99.9659 - dense_784_loss: 69.9681 - dense_788_loss: 74.6417 - val_loss: 236.8323 - val_activation_23_loss: 99.9994 - val_dense_784_loss: 61.4750 - val_dense_788_loss: 75.3580\n",
      "Epoch 127/1000\n",
      "317/317 [==============================] - 0s 142us/step - loss: 238.6384 - activation_23_loss: 99.9254 - dense_784_loss: 65.8430 - dense_788_loss: 72.8700 - val_loss: 237.0140 - val_activation_23_loss: 99.9994 - val_dense_784_loss: 61.5682 - val_dense_788_loss: 75.4464\n",
      "Epoch 128/1000\n",
      "317/317 [==============================] - 0s 146us/step - loss: 259.4468 - activation_23_loss: 99.9277 - dense_784_loss: 73.3815 - dense_788_loss: 86.1376 - val_loss: 237.6984 - val_activation_23_loss: 99.9994 - val_dense_784_loss: 61.9167 - val_dense_788_loss: 75.7823\n",
      "Epoch 129/1000\n",
      "317/317 [==============================] - 0s 149us/step - loss: 247.7327 - activation_23_loss: 99.9588 - dense_784_loss: 70.7172 - dense_788_loss: 77.0568 - val_loss: 238.0712 - val_activation_23_loss: 99.9994 - val_dense_784_loss: 62.3339 - val_dense_788_loss: 75.7378\n",
      "Epoch 130/1000\n",
      "317/317 [==============================] - 0s 146us/step - loss: 246.6027 - activation_23_loss: 99.9897 - dense_784_loss: 65.2688 - dense_788_loss: 81.3442 - val_loss: 238.3508 - val_activation_23_loss: 99.9994 - val_dense_784_loss: 62.4792 - val_dense_788_loss: 75.8722\n",
      "Epoch 131/1000\n",
      "317/317 [==============================] - 0s 145us/step - loss: 241.6014 - activation_23_loss: 99.9402 - dense_784_loss: 66.5384 - dense_788_loss: 75.1228 - val_loss: 237.5087 - val_activation_23_loss: 99.9994 - val_dense_784_loss: 62.0917 - val_dense_788_loss: 75.4176\n",
      "Epoch 132/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 245.0352 - activation_23_loss: 99.9889 - dense_784_loss: 73.8117 - dense_788_loss: 71.2346 - val_loss: 237.1221 - val_activation_23_loss: 99.9994 - val_dense_784_loss: 62.0970 - val_dense_788_loss: 75.0257\n",
      "Epoch 133/1000\n",
      "317/317 [==============================] - 0s 145us/step - loss: 250.5902 - activation_23_loss: 99.9659 - dense_784_loss: 68.5538 - dense_788_loss: 82.0706 - val_loss: 236.7559 - val_activation_23_loss: 99.9994 - val_dense_784_loss: 62.0504 - val_dense_788_loss: 74.7061\n",
      "Epoch 134/1000\n",
      "317/317 [==============================] - 0s 146us/step - loss: 244.8178 - activation_23_loss: 99.9846 - dense_784_loss: 67.8978 - dense_788_loss: 76.9353 - val_loss: 236.8157 - val_activation_23_loss: 99.9994 - val_dense_784_loss: 61.8814 - val_dense_788_loss: 74.9348\n",
      "Epoch 135/1000\n",
      "317/317 [==============================] - 0s 161us/step - loss: 244.6485 - activation_23_loss: 99.9578 - dense_784_loss: 65.8646 - dense_788_loss: 78.8260 - val_loss: 236.8748 - val_activation_23_loss: 99.9994 - val_dense_784_loss: 61.6822 - val_dense_788_loss: 75.1931\n",
      "Epoch 136/1000\n",
      "317/317 [==============================] - 0s 142us/step - loss: 243.8856 - activation_23_loss: 99.9870 - dense_784_loss: 71.6906 - dense_788_loss: 72.2080 - val_loss: 237.3884 - val_activation_23_loss: 99.9995 - val_dense_784_loss: 61.9963 - val_dense_788_loss: 75.3927\n",
      "Epoch 137/1000\n",
      "317/317 [==============================] - 0s 142us/step - loss: 257.5617 - activation_23_loss: 99.9753 - dense_784_loss: 74.3509 - dense_788_loss: 83.2355 - val_loss: 237.4976 - val_activation_23_loss: 99.9995 - val_dense_784_loss: 61.9701 - val_dense_788_loss: 75.5280\n",
      "Epoch 138/1000\n",
      "317/317 [==============================] - 0s 146us/step - loss: 253.4892 - activation_23_loss: 99.9583 - dense_784_loss: 71.3771 - dense_788_loss: 82.1538 - val_loss: 238.2979 - val_activation_23_loss: 99.9995 - val_dense_784_loss: 62.4545 - val_dense_788_loss: 75.8438\n",
      "Epoch 139/1000\n",
      "317/317 [==============================] - 0s 142us/step - loss: 240.4195 - activation_23_loss: 100.1139 - dense_784_loss: 68.4938 - dense_788_loss: 71.8118 - val_loss: 238.5300 - val_activation_23_loss: 99.9995 - val_dense_784_loss: 62.8463 - val_dense_788_loss: 75.6842\n",
      "Epoch 140/1000\n",
      "317/317 [==============================] - 0s 146us/step - loss: 251.3472 - activation_23_loss: 99.9828 - dense_784_loss: 69.8345 - dense_788_loss: 81.5298 - val_loss: 238.1662 - val_activation_23_loss: 99.9995 - val_dense_784_loss: 62.7499 - val_dense_788_loss: 75.4168\n",
      "Epoch 141/1000\n",
      "317/317 [==============================] - 0s 144us/step - loss: 238.4865 - activation_23_loss: 99.9814 - dense_784_loss: 63.4037 - dense_788_loss: 75.1014 - val_loss: 237.7355 - val_activation_23_loss: 99.9995 - val_dense_784_loss: 62.3575 - val_dense_788_loss: 75.3785\n",
      "Epoch 142/1000\n",
      "317/317 [==============================] - 0s 146us/step - loss: 249.3728 - activation_23_loss: 99.9690 - dense_784_loss: 72.2656 - dense_788_loss: 77.1383 - val_loss: 237.8038 - val_activation_23_loss: 99.9995 - val_dense_784_loss: 62.4178 - val_dense_788_loss: 75.3865\n",
      "Epoch 143/1000\n",
      "317/317 [==============================] - 0s 141us/step - loss: 245.3100 - activation_23_loss: 99.9661 - dense_784_loss: 70.4734 - dense_788_loss: 74.8705 - val_loss: 237.9005 - val_activation_23_loss: 99.9995 - val_dense_784_loss: 62.5037 - val_dense_788_loss: 75.3973\n",
      "Epoch 144/1000\n",
      "317/317 [==============================] - 0s 142us/step - loss: 243.6962 - activation_23_loss: 99.9910 - dense_784_loss: 67.6518 - dense_788_loss: 76.0535 - val_loss: 237.1048 - val_activation_23_loss: 99.9995 - val_dense_784_loss: 61.9956 - val_dense_788_loss: 75.1098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145/1000\n",
      "317/317 [==============================] - 0s 149us/step - loss: 245.4074 - activation_23_loss: 99.9857 - dense_784_loss: 72.4335 - dense_788_loss: 72.9882 - val_loss: 236.6096 - val_activation_23_loss: 99.9995 - val_dense_784_loss: 61.8366 - val_dense_788_loss: 74.7735\n",
      "Epoch 146/1000\n",
      "317/317 [==============================] - 0s 142us/step - loss: 238.5353 - activation_23_loss: 99.9315 - dense_784_loss: 67.3301 - dense_788_loss: 71.2737 - val_loss: 235.4709 - val_activation_23_loss: 99.9995 - val_dense_784_loss: 61.2651 - val_dense_788_loss: 74.2063\n",
      "Epoch 147/1000\n",
      "317/317 [==============================] - 0s 146us/step - loss: 239.3825 - activation_23_loss: 99.9713 - dense_784_loss: 66.6843 - dense_788_loss: 72.7268 - val_loss: 234.2733 - val_activation_23_loss: 99.9995 - val_dense_784_loss: 60.6033 - val_dense_788_loss: 73.6705\n",
      "Epoch 148/1000\n",
      "317/317 [==============================] - 0s 142us/step - loss: 240.4494 - activation_23_loss: 99.9708 - dense_784_loss: 64.1384 - dense_788_loss: 76.3402 - val_loss: 234.0250 - val_activation_23_loss: 99.9995 - val_dense_784_loss: 60.2624 - val_dense_788_loss: 73.7632\n",
      "Epoch 149/1000\n",
      "317/317 [==============================] - 0s 139us/step - loss: 253.7087 - activation_23_loss: 99.9735 - dense_784_loss: 67.8827 - dense_788_loss: 85.8524 - val_loss: 234.0585 - val_activation_23_loss: 99.9995 - val_dense_784_loss: 59.8125 - val_dense_788_loss: 74.2465\n",
      "Epoch 150/1000\n",
      "317/317 [==============================] - 0s 142us/step - loss: 246.4411 - activation_23_loss: 99.9481 - dense_784_loss: 72.9331 - dense_788_loss: 73.5599 - val_loss: 234.1617 - val_activation_23_loss: 99.9995 - val_dense_784_loss: 59.7615 - val_dense_788_loss: 74.4007\n",
      "Epoch 151/1000\n",
      "317/317 [==============================] - 0s 146us/step - loss: 250.8542 - activation_23_loss: 99.9629 - dense_784_loss: 72.5199 - dense_788_loss: 78.3713 - val_loss: 234.7354 - val_activation_23_loss: 99.9995 - val_dense_784_loss: 60.2377 - val_dense_788_loss: 74.4982\n",
      "Epoch 152/1000\n",
      "317/317 [==============================] - 0s 145us/step - loss: 245.6309 - activation_23_loss: 100.1960 - dense_784_loss: 72.0907 - dense_788_loss: 73.3441 - val_loss: 235.4560 - val_activation_23_loss: 99.9995 - val_dense_784_loss: 60.9720 - val_dense_788_loss: 74.4845\n",
      "Epoch 153/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 244.9137 - activation_23_loss: 99.9877 - dense_784_loss: 68.7634 - dense_788_loss: 76.1626 - val_loss: 236.1280 - val_activation_23_loss: 99.9995 - val_dense_784_loss: 61.7331 - val_dense_788_loss: 74.3954\n",
      "Epoch 154/1000\n",
      "317/317 [==============================] - 0s 152us/step - loss: 236.9392 - activation_23_loss: 99.9843 - dense_784_loss: 68.8646 - dense_788_loss: 68.0903 - val_loss: 235.6024 - val_activation_23_loss: 99.9995 - val_dense_784_loss: 61.6196 - val_dense_788_loss: 73.9833\n",
      "Epoch 155/1000\n",
      "317/317 [==============================] - 0s 160us/step - loss: 240.7229 - activation_23_loss: 99.9299 - dense_784_loss: 68.4823 - dense_788_loss: 72.3108 - val_loss: 234.6364 - val_activation_23_loss: 99.9994 - val_dense_784_loss: 61.3301 - val_dense_788_loss: 73.3068\n",
      "Epoch 156/1000\n",
      "317/317 [==============================] - 0s 152us/step - loss: 257.8502 - activation_23_loss: 99.9782 - dense_784_loss: 72.9209 - dense_788_loss: 84.9511 - val_loss: 234.7535 - val_activation_23_loss: 99.9994 - val_dense_784_loss: 61.3502 - val_dense_788_loss: 73.4039\n",
      "Epoch 157/1000\n",
      "317/317 [==============================] - 0s 168us/step - loss: 246.9455 - activation_23_loss: 99.9796 - dense_784_loss: 66.1471 - dense_788_loss: 80.8187 - val_loss: 233.8922 - val_activation_23_loss: 99.9994 - val_dense_784_loss: 60.6924 - val_dense_788_loss: 73.2004\n",
      "Epoch 158/1000\n",
      "317/317 [==============================] - 0s 161us/step - loss: 251.4256 - activation_23_loss: 99.9810 - dense_784_loss: 74.1150 - dense_788_loss: 77.3296 - val_loss: 234.6316 - val_activation_23_loss: 99.9995 - val_dense_784_loss: 60.8105 - val_dense_788_loss: 73.8216\n",
      "Epoch 159/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 246.4695 - activation_23_loss: 99.9632 - dense_784_loss: 71.7015 - dense_788_loss: 74.8048 - val_loss: 235.1184 - val_activation_23_loss: 99.9995 - val_dense_784_loss: 60.9604 - val_dense_788_loss: 74.1586\n",
      "Epoch 160/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 272.4257 - activation_23_loss: 124.3088 - dense_784_loss: 66.9063 - dense_788_loss: 81.2106 - val_loss: 235.7935 - val_activation_23_loss: 99.9996 - val_dense_784_loss: 61.0315 - val_dense_788_loss: 74.7624\n",
      "Epoch 161/1000\n",
      "317/317 [==============================] - 0s 152us/step - loss: 249.9427 - activation_23_loss: 99.9895 - dense_784_loss: 70.3712 - dense_788_loss: 79.5820 - val_loss: 235.8322 - val_activation_23_loss: 99.9996 - val_dense_784_loss: 60.7273 - val_dense_788_loss: 75.1053\n",
      "Epoch 162/1000\n",
      "317/317 [==============================] - 0s 161us/step - loss: 274.5587 - activation_23_loss: 99.9877 - dense_784_loss: 72.0469 - dense_788_loss: 102.5241 - val_loss: 236.2041 - val_activation_23_loss: 99.9997 - val_dense_784_loss: 60.4632 - val_dense_788_loss: 75.7413\n",
      "Epoch 163/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 243.1304 - activation_23_loss: 99.9810 - dense_784_loss: 64.3652 - dense_788_loss: 78.7842 - val_loss: 236.1985 - val_activation_23_loss: 99.9997 - val_dense_784_loss: 59.9769 - val_dense_788_loss: 76.2219\n",
      "Epoch 164/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 247.5769 - activation_23_loss: 99.9634 - dense_784_loss: 73.4760 - dense_788_loss: 74.1376 - val_loss: 236.0024 - val_activation_23_loss: 99.9997 - val_dense_784_loss: 59.7818 - val_dense_788_loss: 76.2210\n",
      "Epoch 165/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 235.5916 - activation_23_loss: 99.9500 - dense_784_loss: 63.6246 - dense_788_loss: 72.0170 - val_loss: 235.9327 - val_activation_23_loss: 99.9996 - val_dense_784_loss: 60.0846 - val_dense_788_loss: 75.8484\n",
      "Epoch 166/1000\n",
      "317/317 [==============================] - 0s 149us/step - loss: 244.4122 - activation_23_loss: 99.9882 - dense_784_loss: 64.8887 - dense_788_loss: 79.5354 - val_loss: 235.3021 - val_activation_23_loss: 99.9996 - val_dense_784_loss: 59.7463 - val_dense_788_loss: 75.5562\n",
      "Epoch 167/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 260.5539 - activation_23_loss: 99.9599 - dense_784_loss: 71.3181 - dense_788_loss: 89.2759 - val_loss: 235.0737 - val_activation_23_loss: 99.9996 - val_dense_784_loss: 59.3046 - val_dense_788_loss: 75.7694\n",
      "Epoch 168/1000\n",
      "317/317 [==============================] - 0s 152us/step - loss: 241.9120 - activation_23_loss: 99.9763 - dense_784_loss: 72.9827 - dense_788_loss: 68.9530 - val_loss: 235.2426 - val_activation_23_loss: 99.9996 - val_dense_784_loss: 59.6637 - val_dense_788_loss: 75.5792\n",
      "Epoch 169/1000\n",
      "317/317 [==============================] - 0s 152us/step - loss: 242.4052 - activation_23_loss: 99.9929 - dense_784_loss: 67.8994 - dense_788_loss: 74.5129 - val_loss: 234.6027 - val_activation_23_loss: 99.9996 - val_dense_784_loss: 59.6303 - val_dense_788_loss: 74.9728\n",
      "Epoch 170/1000\n",
      "317/317 [==============================] - 0s 149us/step - loss: 250.4610 - activation_23_loss: 99.9496 - dense_784_loss: 73.7235 - dense_788_loss: 76.7878 - val_loss: 234.4271 - val_activation_23_loss: 99.9996 - val_dense_784_loss: 59.6758 - val_dense_788_loss: 74.7517\n",
      "Epoch 171/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 251.9058 - activation_23_loss: 99.9767 - dense_784_loss: 66.6256 - dense_788_loss: 85.3035 - val_loss: 233.3389 - val_activation_23_loss: 99.9996 - val_dense_784_loss: 59.0634 - val_dense_788_loss: 74.2760\n",
      "Epoch 172/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 253.8147 - activation_23_loss: 99.9909 - dense_784_loss: 69.8719 - dense_788_loss: 83.9519 - val_loss: 233.4805 - val_activation_23_loss: 99.9996 - val_dense_784_loss: 58.9943 - val_dense_788_loss: 74.4867\n",
      "Epoch 173/1000\n",
      "317/317 [==============================] - 0s 164us/step - loss: 246.0583 - activation_23_loss: 99.9597 - dense_784_loss: 65.6565 - dense_788_loss: 80.4421 - val_loss: 233.3550 - val_activation_23_loss: 99.9996 - val_dense_784_loss: 58.7784 - val_dense_788_loss: 74.5771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 174/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 244.4647 - activation_23_loss: 100.5550 - dense_784_loss: 66.9252 - dense_788_loss: 76.9845 - val_loss: 232.3337 - val_activation_23_loss: 99.9996 - val_dense_784_loss: 58.0135 - val_dense_788_loss: 74.3207\n",
      "Epoch 175/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 242.1493 - activation_23_loss: 101.3023 - dense_784_loss: 69.0908 - dense_788_loss: 71.7561 - val_loss: 232.6778 - val_activation_23_loss: 99.9996 - val_dense_784_loss: 58.2983 - val_dense_788_loss: 74.3799\n",
      "Epoch 176/1000\n",
      "317/317 [==============================] - 0s 152us/step - loss: 241.2558 - activation_23_loss: 99.9708 - dense_784_loss: 70.5372 - dense_788_loss: 70.7477 - val_loss: 232.4999 - val_activation_23_loss: 99.9996 - val_dense_784_loss: 58.3528 - val_dense_788_loss: 74.1475\n",
      "Epoch 177/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 251.1763 - activation_23_loss: 99.9793 - dense_784_loss: 70.0547 - dense_788_loss: 81.1423 - val_loss: 232.1491 - val_activation_23_loss: 99.9996 - val_dense_784_loss: 58.3374 - val_dense_788_loss: 73.8121\n",
      "Epoch 178/1000\n",
      "317/317 [==============================] - 0s 161us/step - loss: 244.4318 - activation_23_loss: 99.9848 - dense_784_loss: 70.4134 - dense_788_loss: 74.0337 - val_loss: 232.7163 - val_activation_23_loss: 99.9996 - val_dense_784_loss: 58.8035 - val_dense_788_loss: 73.9132\n",
      "Epoch 179/1000\n",
      "317/317 [==============================] - 0s 149us/step - loss: 240.3272 - activation_23_loss: 99.9811 - dense_784_loss: 69.8932 - dense_788_loss: 70.4528 - val_loss: 233.0226 - val_activation_23_loss: 99.9997 - val_dense_784_loss: 58.9829 - val_dense_788_loss: 74.0401\n",
      "Epoch 180/1000\n",
      "317/317 [==============================] - 0s 152us/step - loss: 240.6565 - activation_23_loss: 99.9746 - dense_784_loss: 66.0387 - dense_788_loss: 74.6432 - val_loss: 233.2558 - val_activation_23_loss: 99.9997 - val_dense_784_loss: 59.0824 - val_dense_788_loss: 74.1738\n",
      "Epoch 181/1000\n",
      "317/317 [==============================] - 0s 142us/step - loss: 239.9359 - activation_23_loss: 99.9817 - dense_784_loss: 68.7186 - dense_788_loss: 71.2355 - val_loss: 233.1270 - val_activation_23_loss: 99.9997 - val_dense_784_loss: 59.2158 - val_dense_788_loss: 73.9115\n",
      "Epoch 182/1000\n",
      "317/317 [==============================] - 0s 145us/step - loss: 236.4972 - activation_23_loss: 99.9900 - dense_784_loss: 64.7488 - dense_788_loss: 71.7584 - val_loss: 232.4495 - val_activation_23_loss: 99.9996 - val_dense_784_loss: 59.0070 - val_dense_788_loss: 73.4428\n",
      "Epoch 183/1000\n",
      "317/317 [==============================] - 0s 149us/step - loss: 252.4087 - activation_23_loss: 99.9713 - dense_784_loss: 69.3167 - dense_788_loss: 83.1207 - val_loss: 231.5907 - val_activation_23_loss: 99.9996 - val_dense_784_loss: 58.4344 - val_dense_788_loss: 73.1566\n",
      "Epoch 184/1000\n",
      "317/317 [==============================] - 0s 142us/step - loss: 253.8675 - activation_23_loss: 99.9690 - dense_784_loss: 71.0907 - dense_788_loss: 82.8078 - val_loss: 231.4491 - val_activation_23_loss: 99.9996 - val_dense_784_loss: 58.4421 - val_dense_788_loss: 73.0074\n",
      "Epoch 185/1000\n",
      "317/317 [==============================] - 0s 144us/step - loss: 239.7699 - activation_23_loss: 99.9779 - dense_784_loss: 65.4987 - dense_788_loss: 74.2933 - val_loss: 231.2133 - val_activation_23_loss: 99.9996 - val_dense_784_loss: 58.4111 - val_dense_788_loss: 72.8025\n",
      "Epoch 186/1000\n",
      "317/317 [==============================] - 0s 142us/step - loss: 245.2785 - activation_23_loss: 99.9719 - dense_784_loss: 67.4093 - dense_788_loss: 77.8973 - val_loss: 231.4903 - val_activation_23_loss: 99.9996 - val_dense_784_loss: 58.6815 - val_dense_788_loss: 72.8092\n",
      "Epoch 187/1000\n",
      "317/317 [==============================] - 0s 149us/step - loss: 256.0499 - activation_23_loss: 99.9282 - dense_784_loss: 74.1183 - dense_788_loss: 82.0034 - val_loss: 231.8046 - val_activation_23_loss: 99.9996 - val_dense_784_loss: 58.9995 - val_dense_788_loss: 72.8055\n",
      "Epoch 188/1000\n",
      "317/317 [==============================] - 0s 149us/step - loss: 240.5887 - activation_23_loss: 99.9715 - dense_784_loss: 64.9137 - dense_788_loss: 75.7036 - val_loss: 232.0653 - val_activation_23_loss: 99.9997 - val_dense_784_loss: 58.9636 - val_dense_788_loss: 73.1020\n",
      "Epoch 189/1000\n",
      "317/317 [==============================] - 0s 142us/step - loss: 245.4364 - activation_23_loss: 99.9619 - dense_784_loss: 69.6264 - dense_788_loss: 75.8481 - val_loss: 231.8807 - val_activation_23_loss: 99.9997 - val_dense_784_loss: 58.7168 - val_dense_788_loss: 73.1642\n",
      "Epoch 190/1000\n",
      "317/317 [==============================] - 0s 139us/step - loss: 256.7450 - activation_23_loss: 99.9764 - dense_784_loss: 72.8253 - dense_788_loss: 83.9432 - val_loss: 232.4657 - val_activation_23_loss: 99.9997 - val_dense_784_loss: 58.9614 - val_dense_788_loss: 73.5045\n",
      "Epoch 191/1000\n",
      "317/317 [==============================] - 0s 145us/step - loss: 242.9734 - activation_23_loss: 99.9783 - dense_784_loss: 66.2391 - dense_788_loss: 76.7560 - val_loss: 232.9648 - val_activation_23_loss: 99.9998 - val_dense_784_loss: 59.0431 - val_dense_788_loss: 73.9220\n",
      "Epoch 192/1000\n",
      "317/317 [==============================] - 0s 146us/step - loss: 238.9490 - activation_23_loss: 99.9880 - dense_784_loss: 66.2826 - dense_788_loss: 72.6784 - val_loss: 232.5747 - val_activation_23_loss: 99.9997 - val_dense_784_loss: 58.7974 - val_dense_788_loss: 73.7776\n",
      "Epoch 193/1000\n",
      "317/317 [==============================] - 0s 149us/step - loss: 244.0285 - activation_23_loss: 99.9568 - dense_784_loss: 66.4418 - dense_788_loss: 77.6299 - val_loss: 231.4487 - val_activation_23_loss: 99.9997 - val_dense_784_loss: 58.1117 - val_dense_788_loss: 73.3374\n",
      "Epoch 194/1000\n",
      "317/317 [==============================] - 0s 146us/step - loss: 242.6032 - activation_23_loss: 99.9882 - dense_784_loss: 64.1239 - dense_788_loss: 78.4912 - val_loss: 230.3169 - val_activation_23_loss: 99.9997 - val_dense_784_loss: 57.2374 - val_dense_788_loss: 73.0798\n",
      "Epoch 195/1000\n",
      "317/317 [==============================] - 0s 145us/step - loss: 233.7724 - activation_23_loss: 99.9758 - dense_784_loss: 64.0606 - dense_788_loss: 69.7360 - val_loss: 229.1800 - val_activation_23_loss: 99.9997 - val_dense_784_loss: 56.5101 - val_dense_788_loss: 72.6703\n",
      "Epoch 196/1000\n",
      "317/317 [==============================] - 0s 146us/step - loss: 250.4931 - activation_23_loss: 99.9740 - dense_784_loss: 77.0698 - dense_788_loss: 73.4494 - val_loss: 228.9398 - val_activation_23_loss: 99.9997 - val_dense_784_loss: 56.5110 - val_dense_788_loss: 72.4291\n",
      "Epoch 197/1000\n",
      "317/317 [==============================] - 0s 142us/step - loss: 241.4567 - activation_23_loss: 99.9361 - dense_784_loss: 67.9783 - dense_788_loss: 73.5423 - val_loss: 229.6303 - val_activation_23_loss: 99.9997 - val_dense_784_loss: 57.1462 - val_dense_788_loss: 72.4845\n",
      "Epoch 198/1000\n",
      "317/317 [==============================] - 0s 146us/step - loss: 250.3628 - activation_23_loss: 99.9928 - dense_784_loss: 64.1808 - dense_788_loss: 86.1892 - val_loss: 230.0660 - val_activation_23_loss: 99.9997 - val_dense_784_loss: 57.3513 - val_dense_788_loss: 72.7150\n",
      "Epoch 199/1000\n",
      "317/317 [==============================] - 0s 153us/step - loss: 258.2801 - activation_23_loss: 99.9833 - dense_784_loss: 72.7730 - dense_788_loss: 85.5238 - val_loss: 230.7444 - val_activation_23_loss: 99.9997 - val_dense_784_loss: 57.7533 - val_dense_788_loss: 72.9913\n",
      "Epoch 200/1000\n",
      "317/317 [==============================] - 0s 153us/step - loss: 254.3211 - activation_23_loss: 99.9726 - dense_784_loss: 73.0266 - dense_788_loss: 81.3219 - val_loss: 230.5212 - val_activation_23_loss: 99.9997 - val_dense_784_loss: 57.4958 - val_dense_788_loss: 73.0257\n",
      "Epoch 201/1000\n",
      "317/317 [==============================] - 0s 146us/step - loss: 243.9772 - activation_23_loss: 99.9719 - dense_784_loss: 72.1356 - dense_788_loss: 71.8697 - val_loss: 231.7858 - val_activation_23_loss: 99.9997 - val_dense_784_loss: 58.1263 - val_dense_788_loss: 73.6598\n",
      "Epoch 202/1000\n",
      "317/317 [==============================] - 0s 144us/step - loss: 237.1565 - activation_23_loss: 99.9736 - dense_784_loss: 66.8356 - dense_788_loss: 70.3473 - val_loss: 231.9543 - val_activation_23_loss: 99.9997 - val_dense_784_loss: 58.4776 - val_dense_788_loss: 73.4770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 203/1000\n",
      "317/317 [==============================] - 0s 146us/step - loss: 236.1942 - activation_23_loss: 99.9892 - dense_784_loss: 62.4909 - dense_788_loss: 73.7141 - val_loss: 231.9523 - val_activation_23_loss: 99.9997 - val_dense_784_loss: 58.3959 - val_dense_788_loss: 73.5567\n",
      "Epoch 204/1000\n",
      "317/317 [==============================] - 0s 142us/step - loss: 242.8772 - activation_23_loss: 99.9755 - dense_784_loss: 69.2748 - dense_788_loss: 73.6269 - val_loss: 231.4989 - val_activation_23_loss: 99.9997 - val_dense_784_loss: 58.1599 - val_dense_788_loss: 73.3393\n",
      "Epoch 205/1000\n",
      "317/317 [==============================] - 0s 146us/step - loss: 255.5984 - activation_23_loss: 99.9513 - dense_784_loss: 64.3920 - dense_788_loss: 91.2551 - val_loss: 231.3458 - val_activation_23_loss: 99.9998 - val_dense_784_loss: 57.9319 - val_dense_788_loss: 73.4141\n",
      "Epoch 206/1000\n",
      "317/317 [==============================] - 0s 145us/step - loss: 244.2036 - activation_23_loss: 99.9821 - dense_784_loss: 71.7934 - dense_788_loss: 72.4281 - val_loss: 230.5739 - val_activation_23_loss: 99.9997 - val_dense_784_loss: 57.4227 - val_dense_788_loss: 73.1515\n",
      "Epoch 207/1000\n",
      "317/317 [==============================] - 0s 142us/step - loss: 247.0182 - activation_23_loss: 99.9706 - dense_784_loss: 64.1005 - dense_788_loss: 82.9471 - val_loss: 230.4443 - val_activation_23_loss: 99.9998 - val_dense_784_loss: 57.2423 - val_dense_788_loss: 73.2023\n",
      "Epoch 208/1000\n",
      "317/317 [==============================] - 0s 142us/step - loss: 246.3859 - activation_23_loss: 99.9731 - dense_784_loss: 71.0466 - dense_788_loss: 75.3663 - val_loss: 230.7057 - val_activation_23_loss: 99.9998 - val_dense_784_loss: 57.3093 - val_dense_788_loss: 73.3966\n",
      "Epoch 209/1000\n",
      "317/317 [==============================] - 0s 146us/step - loss: 242.6330 - activation_23_loss: 99.9306 - dense_784_loss: 72.0164 - dense_788_loss: 70.6860 - val_loss: 230.9501 - val_activation_23_loss: 99.9998 - val_dense_784_loss: 57.6705 - val_dense_788_loss: 73.2799\n",
      "Epoch 210/1000\n",
      "317/317 [==============================] - 0s 145us/step - loss: 251.0392 - activation_23_loss: 99.9803 - dense_784_loss: 66.5845 - dense_788_loss: 84.4744 - val_loss: 231.4958 - val_activation_23_loss: 99.9998 - val_dense_784_loss: 57.8023 - val_dense_788_loss: 73.6936\n",
      "Epoch 211/1000\n",
      "317/317 [==============================] - 0s 149us/step - loss: 247.0011 - activation_23_loss: 99.9593 - dense_784_loss: 71.6852 - dense_788_loss: 75.3566 - val_loss: 231.6892 - val_activation_23_loss: 99.9998 - val_dense_784_loss: 57.9717 - val_dense_788_loss: 73.7177\n",
      "Epoch 212/1000\n",
      "317/317 [==============================] - 0s 145us/step - loss: 249.9463 - activation_23_loss: 99.9772 - dense_784_loss: 72.4629 - dense_788_loss: 77.5062 - val_loss: 231.7438 - val_activation_23_loss: 99.9998 - val_dense_784_loss: 58.1734 - val_dense_788_loss: 73.5705\n",
      "Epoch 213/1000\n",
      "317/317 [==============================] - 0s 146us/step - loss: 248.9388 - activation_23_loss: 99.9896 - dense_784_loss: 66.2648 - dense_788_loss: 82.6843 - val_loss: 231.5360 - val_activation_23_loss: 99.9998 - val_dense_784_loss: 57.8814 - val_dense_788_loss: 73.6548\n",
      "Epoch 214/1000\n",
      "317/317 [==============================] - 0s 144us/step - loss: 242.4707 - activation_23_loss: 99.9806 - dense_784_loss: 66.2064 - dense_788_loss: 76.2837 - val_loss: 230.7326 - val_activation_23_loss: 99.9998 - val_dense_784_loss: 57.2976 - val_dense_788_loss: 73.4352\n",
      "Epoch 215/1000\n",
      "317/317 [==============================] - 0s 149us/step - loss: 243.6641 - activation_23_loss: 99.9713 - dense_784_loss: 67.5918 - dense_788_loss: 76.1010 - val_loss: 230.1383 - val_activation_23_loss: 99.9998 - val_dense_784_loss: 56.8154 - val_dense_788_loss: 73.3231\n",
      "Epoch 216/1000\n",
      "317/317 [==============================] - 0s 153us/step - loss: 240.2368 - activation_23_loss: 99.9787 - dense_784_loss: 69.3095 - dense_788_loss: 70.9486 - val_loss: 229.6204 - val_activation_23_loss: 99.9998 - val_dense_784_loss: 56.4313 - val_dense_788_loss: 73.1893\n",
      "Epoch 217/1000\n",
      "317/317 [==============================] - 0s 147us/step - loss: 239.8341 - activation_23_loss: 99.9463 - dense_784_loss: 63.9430 - dense_788_loss: 75.9447 - val_loss: 229.1337 - val_activation_23_loss: 99.9998 - val_dense_784_loss: 56.2674 - val_dense_788_loss: 72.8665\n",
      "Epoch 218/1000\n",
      "317/317 [==============================] - 0s 144us/step - loss: 253.0911 - activation_23_loss: 99.9864 - dense_784_loss: 69.8083 - dense_788_loss: 83.2964 - val_loss: 228.5649 - val_activation_23_loss: 99.9998 - val_dense_784_loss: 55.8958 - val_dense_788_loss: 72.6693\n",
      "Epoch 219/1000\n",
      "317/317 [==============================] - 0s 149us/step - loss: 235.8430 - activation_23_loss: 99.9859 - dense_784_loss: 64.8193 - dense_788_loss: 71.0378 - val_loss: 228.6689 - val_activation_23_loss: 99.9998 - val_dense_784_loss: 55.8828 - val_dense_788_loss: 72.7863\n",
      "Epoch 220/1000\n",
      "317/317 [==============================] - 0s 146us/step - loss: 239.7679 - activation_23_loss: 99.9697 - dense_784_loss: 68.5378 - dense_788_loss: 71.2604 - val_loss: 228.4437 - val_activation_23_loss: 99.9997 - val_dense_784_loss: 55.8926 - val_dense_788_loss: 72.5514\n",
      "Epoch 221/1000\n",
      "317/317 [==============================] - 0s 164us/step - loss: 238.9748 - activation_23_loss: 99.9704 - dense_784_loss: 64.5035 - dense_788_loss: 74.5009 - val_loss: 228.4999 - val_activation_23_loss: 99.9997 - val_dense_784_loss: 56.0406 - val_dense_788_loss: 72.4595\n",
      "Epoch 222/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 242.5310 - activation_23_loss: 99.9910 - dense_784_loss: 66.6819 - dense_788_loss: 75.8581 - val_loss: 228.7859 - val_activation_23_loss: 99.9997 - val_dense_784_loss: 56.3523 - val_dense_788_loss: 72.4339\n",
      "Epoch 223/1000\n",
      "317/317 [==============================] - 0s 142us/step - loss: 252.7451 - activation_23_loss: 99.9828 - dense_784_loss: 65.9446 - dense_788_loss: 86.8177 - val_loss: 228.2972 - val_activation_23_loss: 99.9997 - val_dense_784_loss: 56.1861 - val_dense_788_loss: 72.1113\n",
      "Epoch 224/1000\n",
      "317/317 [==============================] - 0s 152us/step - loss: 248.9033 - activation_23_loss: 99.9897 - dense_784_loss: 64.9302 - dense_788_loss: 83.9834 - val_loss: 228.6006 - val_activation_23_loss: 99.9997 - val_dense_784_loss: 56.2517 - val_dense_788_loss: 72.3492\n",
      "Epoch 225/1000\n",
      "317/317 [==============================] - 0s 161us/step - loss: 238.4838 - activation_23_loss: 99.9754 - dense_784_loss: 63.6698 - dense_788_loss: 74.8386 - val_loss: 228.9888 - val_activation_23_loss: 99.9997 - val_dense_784_loss: 56.1300 - val_dense_788_loss: 72.8590\n",
      "Epoch 226/1000\n",
      "317/317 [==============================] - 0s 161us/step - loss: 238.3191 - activation_23_loss: 99.9910 - dense_784_loss: 65.4392 - dense_788_loss: 72.8890 - val_loss: 228.2440 - val_activation_23_loss: 99.9997 - val_dense_784_loss: 55.4762 - val_dense_788_loss: 72.7681\n",
      "Epoch 227/1000\n",
      "317/317 [==============================] - 0s 168us/step - loss: 240.5901 - activation_23_loss: 99.9775 - dense_784_loss: 71.5381 - dense_788_loss: 69.0745 - val_loss: 227.4147 - val_activation_23_loss: 99.9996 - val_dense_784_loss: 55.1431 - val_dense_788_loss: 72.2719\n",
      "Epoch 228/1000\n",
      "317/317 [==============================] - 0s 169us/step - loss: 241.7147 - activation_23_loss: 99.9891 - dense_784_loss: 64.9775 - dense_788_loss: 76.7481 - val_loss: 227.1769 - val_activation_23_loss: 99.9996 - val_dense_784_loss: 55.2616 - val_dense_788_loss: 71.9157\n",
      "Epoch 229/1000\n",
      "317/317 [==============================] - 0s 166us/step - loss: 243.8514 - activation_23_loss: 99.9615 - dense_784_loss: 66.4082 - dense_788_loss: 77.4817 - val_loss: 226.9215 - val_activation_23_loss: 99.9996 - val_dense_784_loss: 55.1584 - val_dense_788_loss: 71.7635\n",
      "Epoch 230/1000\n",
      "317/317 [==============================] - 0s 160us/step - loss: 257.0596 - activation_23_loss: 99.9426 - dense_784_loss: 61.5099 - dense_788_loss: 95.6072 - val_loss: 227.1635 - val_activation_23_loss: 99.9997 - val_dense_784_loss: 54.7513 - val_dense_788_loss: 72.4126\n",
      "Epoch 231/1000\n",
      "317/317 [==============================] - 0s 163us/step - loss: 239.3478 - activation_23_loss: 99.9727 - dense_784_loss: 69.1049 - dense_788_loss: 70.2702 - val_loss: 226.9233 - val_activation_23_loss: 99.9997 - val_dense_784_loss: 54.5540 - val_dense_788_loss: 72.3696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 232/1000\n",
      "317/317 [==============================] - 0s 150us/step - loss: 254.6543 - activation_23_loss: 99.9670 - dense_784_loss: 70.8035 - dense_788_loss: 83.8838 - val_loss: 226.1949 - val_activation_23_loss: 99.9996 - val_dense_784_loss: 54.0462 - val_dense_788_loss: 72.1490\n",
      "Epoch 233/1000\n",
      "317/317 [==============================] - 0s 152us/step - loss: 234.7558 - activation_23_loss: 99.9758 - dense_784_loss: 61.8111 - dense_788_loss: 72.9689 - val_loss: 225.8437 - val_activation_23_loss: 99.9996 - val_dense_784_loss: 53.7947 - val_dense_788_loss: 72.0493\n",
      "Epoch 234/1000\n",
      "317/317 [==============================] - 0s 163us/step - loss: 233.8277 - activation_23_loss: 99.9866 - dense_784_loss: 63.0489 - dense_788_loss: 70.7922 - val_loss: 224.8237 - val_activation_23_loss: 99.9997 - val_dense_784_loss: 53.0427 - val_dense_788_loss: 71.7814\n",
      "Epoch 235/1000\n",
      "317/317 [==============================] - 0s 164us/step - loss: 237.1582 - activation_23_loss: 99.9841 - dense_784_loss: 65.5736 - dense_788_loss: 71.6005 - val_loss: 224.1354 - val_activation_23_loss: 99.9996 - val_dense_784_loss: 52.8611 - val_dense_788_loss: 71.2747\n",
      "Epoch 236/1000\n",
      "317/317 [==============================] - 0s 169us/step - loss: 246.0384 - activation_23_loss: 99.9841 - dense_784_loss: 72.5845 - dense_788_loss: 73.4698 - val_loss: 224.5277 - val_activation_23_loss: 99.9996 - val_dense_784_loss: 53.6274 - val_dense_788_loss: 70.9007\n",
      "Epoch 237/1000\n",
      "317/317 [==============================] - 0s 161us/step - loss: 241.3256 - activation_23_loss: 100.5966 - dense_784_loss: 67.7595 - dense_788_loss: 72.9696 - val_loss: 225.0266 - val_activation_23_loss: 99.9997 - val_dense_784_loss: 54.4286 - val_dense_788_loss: 70.5983\n",
      "Epoch 238/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 235.6428 - activation_23_loss: 99.9910 - dense_784_loss: 64.1236 - dense_788_loss: 71.5282 - val_loss: 225.2695 - val_activation_23_loss: 99.9998 - val_dense_784_loss: 54.7514 - val_dense_788_loss: 70.5183\n",
      "Epoch 239/1000\n",
      "317/317 [==============================] - 0s 161us/step - loss: 238.2097 - activation_23_loss: 99.9853 - dense_784_loss: 68.0160 - dense_788_loss: 70.2085 - val_loss: 225.1607 - val_activation_23_loss: 99.9998 - val_dense_784_loss: 55.0809 - val_dense_788_loss: 70.0800\n",
      "Epoch 240/1000\n",
      "317/317 [==============================] - 0s 152us/step - loss: 242.6941 - activation_23_loss: 99.9861 - dense_784_loss: 67.8907 - dense_788_loss: 74.8173 - val_loss: 225.3767 - val_activation_23_loss: 99.9998 - val_dense_784_loss: 55.2583 - val_dense_788_loss: 70.1185\n",
      "Epoch 241/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 241.0383 - activation_23_loss: 99.9921 - dense_784_loss: 66.0441 - dense_788_loss: 75.0021 - val_loss: 224.6659 - val_activation_23_loss: 99.9998 - val_dense_784_loss: 54.8679 - val_dense_788_loss: 69.7981\n",
      "Epoch 242/1000\n",
      "317/317 [==============================] - 0s 171us/step - loss: 236.1771 - activation_23_loss: 99.9864 - dense_784_loss: 63.2707 - dense_788_loss: 72.9199 - val_loss: 223.7108 - val_activation_23_loss: 99.9998 - val_dense_784_loss: 54.2736 - val_dense_788_loss: 69.4374\n",
      "Epoch 243/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 237.0269 - activation_23_loss: 99.9787 - dense_784_loss: 65.9228 - dense_788_loss: 71.1253 - val_loss: 223.4262 - val_activation_23_loss: 99.9998 - val_dense_784_loss: 54.1020 - val_dense_788_loss: 69.3243\n",
      "Epoch 244/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 241.4635 - activation_23_loss: 99.9312 - dense_784_loss: 70.0386 - dense_788_loss: 71.4937 - val_loss: 223.6081 - val_activation_23_loss: 99.9998 - val_dense_784_loss: 54.3698 - val_dense_788_loss: 69.2385\n",
      "Epoch 245/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 236.7207 - activation_23_loss: 99.9884 - dense_784_loss: 68.8524 - dense_788_loss: 67.8798 - val_loss: 223.9568 - val_activation_23_loss: 99.9998 - val_dense_784_loss: 54.8769 - val_dense_788_loss: 69.0801\n",
      "Epoch 246/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 241.0315 - activation_23_loss: 99.9777 - dense_784_loss: 71.0753 - dense_788_loss: 69.9786 - val_loss: 224.3771 - val_activation_23_loss: 99.9998 - val_dense_784_loss: 55.1298 - val_dense_788_loss: 69.2474\n",
      "Epoch 247/1000\n",
      "317/317 [==============================] - 0s 149us/step - loss: 231.9908 - activation_23_loss: 99.9676 - dense_784_loss: 62.7652 - dense_788_loss: 69.2581 - val_loss: 224.4105 - val_activation_23_loss: 99.9998 - val_dense_784_loss: 55.0730 - val_dense_788_loss: 69.3376\n",
      "Epoch 248/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 240.4834 - activation_23_loss: 99.9319 - dense_784_loss: 65.8693 - dense_788_loss: 74.6822 - val_loss: 224.8937 - val_activation_23_loss: 99.9998 - val_dense_784_loss: 55.1956 - val_dense_788_loss: 69.6983\n",
      "Epoch 249/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 230.8637 - activation_23_loss: 99.9910 - dense_784_loss: 63.4562 - dense_788_loss: 67.4165 - val_loss: 223.8771 - val_activation_23_loss: 99.9998 - val_dense_784_loss: 54.5674 - val_dense_788_loss: 69.3098\n",
      "Epoch 250/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 243.7596 - activation_23_loss: 99.9700 - dense_784_loss: 67.5658 - dense_788_loss: 76.2239 - val_loss: 224.1439 - val_activation_23_loss: 99.9999 - val_dense_784_loss: 54.7316 - val_dense_788_loss: 69.4125\n",
      "Epoch 251/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 248.6883 - activation_23_loss: 99.9864 - dense_784_loss: 63.6549 - dense_788_loss: 85.0470 - val_loss: 224.4506 - val_activation_23_loss: 99.9999 - val_dense_784_loss: 54.4871 - val_dense_788_loss: 69.9636\n",
      "Epoch 252/1000\n",
      "317/317 [==============================] - 0s 152us/step - loss: 232.5357 - activation_23_loss: 99.9638 - dense_784_loss: 63.9643 - dense_788_loss: 68.6076 - val_loss: 224.0947 - val_activation_23_loss: 99.9999 - val_dense_784_loss: 54.0993 - val_dense_788_loss: 69.9955\n",
      "Epoch 253/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 244.0577 - activation_23_loss: 99.9919 - dense_784_loss: 64.4729 - dense_788_loss: 79.5930 - val_loss: 223.6657 - val_activation_23_loss: 99.9999 - val_dense_784_loss: 53.8079 - val_dense_788_loss: 69.8579\n",
      "Epoch 254/1000\n",
      "317/317 [==============================] - 0s 152us/step - loss: 246.8497 - activation_23_loss: 99.9479 - dense_784_loss: 64.1246 - dense_788_loss: 82.7772 - val_loss: 224.1580 - val_activation_23_loss: 99.9999 - val_dense_784_loss: 53.6598 - val_dense_788_loss: 70.4984\n",
      "Epoch 255/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 241.7608 - activation_23_loss: 99.9835 - dense_784_loss: 63.6964 - dense_788_loss: 78.0809 - val_loss: 224.5601 - val_activation_23_loss: 99.9999 - val_dense_784_loss: 53.4702 - val_dense_788_loss: 71.0900\n",
      "Epoch 256/1000\n",
      "317/317 [==============================] - 0s 171us/step - loss: 242.2321 - activation_23_loss: 99.9800 - dense_784_loss: 68.9457 - dense_788_loss: 73.3065 - val_loss: 224.9845 - val_activation_23_loss: 99.9999 - val_dense_784_loss: 53.5276 - val_dense_788_loss: 71.4570\n",
      "Epoch 257/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 239.5028 - activation_23_loss: 99.9325 - dense_784_loss: 64.0208 - dense_788_loss: 75.5495 - val_loss: 225.1638 - val_activation_23_loss: 99.9999 - val_dense_784_loss: 53.5698 - val_dense_788_loss: 71.5941\n",
      "Epoch 258/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 229.4289 - activation_23_loss: 99.9685 - dense_784_loss: 62.1487 - dense_788_loss: 67.3117 - val_loss: 224.9707 - val_activation_23_loss: 99.9999 - val_dense_784_loss: 53.7024 - val_dense_788_loss: 71.2684\n",
      "Epoch 259/1000\n",
      "317/317 [==============================] - 0s 149us/step - loss: 240.6793 - activation_23_loss: 99.9569 - dense_784_loss: 65.5166 - dense_788_loss: 75.2058 - val_loss: 224.9609 - val_activation_23_loss: 99.9999 - val_dense_784_loss: 53.7815 - val_dense_788_loss: 71.1796\n",
      "Epoch 260/1000\n",
      "317/317 [==============================] - 0s 149us/step - loss: 246.5534 - activation_23_loss: 99.9881 - dense_784_loss: 66.8123 - dense_788_loss: 79.7530 - val_loss: 224.8074 - val_activation_23_loss: 99.9999 - val_dense_784_loss: 53.6298 - val_dense_788_loss: 71.1777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 261/1000\n",
      "317/317 [==============================] - 0s 152us/step - loss: 242.2444 - activation_23_loss: 99.9749 - dense_784_loss: 64.8227 - dense_788_loss: 77.4468 - val_loss: 224.6643 - val_activation_23_loss: 99.9998 - val_dense_784_loss: 53.6442 - val_dense_788_loss: 71.0203\n",
      "Epoch 262/1000\n",
      "317/317 [==============================] - 0s 161us/step - loss: 237.1746 - activation_23_loss: 99.9912 - dense_784_loss: 65.7636 - dense_788_loss: 71.4198 - val_loss: 224.3478 - val_activation_23_loss: 99.9998 - val_dense_784_loss: 53.5389 - val_dense_788_loss: 70.8090\n",
      "Epoch 263/1000\n",
      "317/317 [==============================] - 0s 149us/step - loss: 247.4249 - activation_23_loss: 99.9902 - dense_784_loss: 75.3820 - dense_788_loss: 72.0527 - val_loss: 223.9538 - val_activation_23_loss: 99.9998 - val_dense_784_loss: 53.6295 - val_dense_788_loss: 70.3245\n",
      "Epoch 264/1000\n",
      "317/317 [==============================] - 0s 152us/step - loss: 234.1287 - activation_23_loss: 100.0088 - dense_784_loss: 63.3137 - dense_788_loss: 70.8063 - val_loss: 223.5299 - val_activation_23_loss: 99.9998 - val_dense_784_loss: 53.5930 - val_dense_788_loss: 69.9371\n",
      "Epoch 265/1000\n",
      "317/317 [==============================] - 0s 146us/step - loss: 248.7820 - activation_23_loss: 99.9875 - dense_784_loss: 70.2722 - dense_788_loss: 78.5223 - val_loss: 223.5766 - val_activation_23_loss: 99.9998 - val_dense_784_loss: 53.6638 - val_dense_788_loss: 69.9130\n",
      "Epoch 266/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 236.1329 - activation_23_loss: 99.9912 - dense_784_loss: 62.6310 - dense_788_loss: 73.5108 - val_loss: 223.1444 - val_activation_23_loss: 99.9998 - val_dense_784_loss: 53.4617 - val_dense_788_loss: 69.6829\n",
      "Epoch 267/1000\n",
      "317/317 [==============================] - 0s 149us/step - loss: 234.8069 - activation_23_loss: 99.9801 - dense_784_loss: 61.4884 - dense_788_loss: 73.3385 - val_loss: 222.9911 - val_activation_23_loss: 99.9998 - val_dense_784_loss: 53.6588 - val_dense_788_loss: 69.3325\n",
      "Epoch 268/1000\n",
      "317/317 [==============================] - 0s 152us/step - loss: 244.0535 - activation_23_loss: 99.9871 - dense_784_loss: 62.6432 - dense_788_loss: 81.4232 - val_loss: 222.7572 - val_activation_23_loss: 99.9998 - val_dense_784_loss: 53.3729 - val_dense_788_loss: 69.3844\n",
      "Epoch 269/1000\n",
      "317/317 [==============================] - 0s 147us/step - loss: 230.4872 - activation_23_loss: 99.9887 - dense_784_loss: 62.4344 - dense_788_loss: 68.0642 - val_loss: 223.3626 - val_activation_23_loss: 99.9999 - val_dense_784_loss: 53.3129 - val_dense_788_loss: 70.0498\n",
      "Epoch 270/1000\n",
      "317/317 [==============================] - 0s 150us/step - loss: 233.4145 - activation_23_loss: 99.9645 - dense_784_loss: 63.4640 - dense_788_loss: 69.9860 - val_loss: 223.1951 - val_activation_23_loss: 99.9999 - val_dense_784_loss: 53.2010 - val_dense_788_loss: 69.9943\n",
      "Epoch 271/1000\n",
      "317/317 [==============================] - 0s 152us/step - loss: 243.6699 - activation_23_loss: 99.9908 - dense_784_loss: 67.6301 - dense_788_loss: 76.0489 - val_loss: 223.0684 - val_activation_23_loss: 99.9999 - val_dense_784_loss: 53.2630 - val_dense_788_loss: 69.8055\n",
      "Epoch 272/1000\n",
      "317/317 [==============================] - 0s 152us/step - loss: 235.9782 - activation_23_loss: 99.9783 - dense_784_loss: 65.4135 - dense_788_loss: 70.5863 - val_loss: 223.2841 - val_activation_23_loss: 99.9999 - val_dense_784_loss: 53.6687 - val_dense_788_loss: 69.6156\n",
      "Epoch 273/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 235.4492 - activation_23_loss: 99.9910 - dense_784_loss: 64.8963 - dense_788_loss: 70.5619 - val_loss: 222.6611 - val_activation_23_loss: 99.9998 - val_dense_784_loss: 53.6530 - val_dense_788_loss: 69.0083\n",
      "Epoch 274/1000\n",
      "317/317 [==============================] - 0s 153us/step - loss: 239.5238 - activation_23_loss: 99.9685 - dense_784_loss: 72.0435 - dense_788_loss: 67.5118 - val_loss: 222.2636 - val_activation_23_loss: 99.9998 - val_dense_784_loss: 53.5780 - val_dense_788_loss: 68.6858\n",
      "Epoch 275/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 235.9210 - activation_23_loss: 99.9849 - dense_784_loss: 65.3075 - dense_788_loss: 70.6286 - val_loss: 221.6463 - val_activation_23_loss: 99.9998 - val_dense_784_loss: 53.5452 - val_dense_788_loss: 68.1014\n",
      "Epoch 276/1000\n",
      "317/317 [==============================] - 0s 152us/step - loss: 242.8715 - activation_23_loss: 99.9363 - dense_784_loss: 68.3582 - dense_788_loss: 74.5770 - val_loss: 221.5318 - val_activation_23_loss: 99.9998 - val_dense_784_loss: 53.5085 - val_dense_788_loss: 68.0235\n",
      "Epoch 277/1000\n",
      "317/317 [==============================] - 0s 152us/step - loss: 239.2774 - activation_23_loss: 99.9772 - dense_784_loss: 66.4337 - dense_788_loss: 72.8665 - val_loss: 222.3830 - val_activation_23_loss: 99.9998 - val_dense_784_loss: 53.9008 - val_dense_788_loss: 68.4824\n",
      "Epoch 278/1000\n",
      "317/317 [==============================] - 0s 147us/step - loss: 237.1797 - activation_23_loss: 99.9753 - dense_784_loss: 67.0872 - dense_788_loss: 70.1173 - val_loss: 222.0130 - val_activation_23_loss: 99.9998 - val_dense_784_loss: 53.6927 - val_dense_788_loss: 68.3205\n",
      "Epoch 279/1000\n",
      "317/317 [==============================] - 0s 152us/step - loss: 250.2521 - activation_23_loss: 99.9598 - dense_784_loss: 65.6710 - dense_788_loss: 84.6213 - val_loss: 222.8649 - val_activation_23_loss: 99.9999 - val_dense_784_loss: 53.8264 - val_dense_788_loss: 69.0386\n",
      "Epoch 280/1000\n",
      "317/317 [==============================] - 0s 150us/step - loss: 240.3388 - activation_23_loss: 99.9772 - dense_784_loss: 68.0322 - dense_788_loss: 72.3294 - val_loss: 223.5686 - val_activation_23_loss: 99.9999 - val_dense_784_loss: 53.8385 - val_dense_788_loss: 69.7302\n",
      "Epoch 281/1000\n",
      "317/317 [==============================] - 0s 152us/step - loss: 234.5751 - activation_23_loss: 99.9876 - dense_784_loss: 64.4176 - dense_788_loss: 70.1698 - val_loss: 223.6280 - val_activation_23_loss: 99.9999 - val_dense_784_loss: 53.6501 - val_dense_788_loss: 69.9779\n",
      "Epoch 282/1000\n",
      "317/317 [==============================] - 0s 149us/step - loss: 227.6043 - activation_23_loss: 99.9882 - dense_784_loss: 61.5117 - dense_788_loss: 66.1044 - val_loss: 223.1647 - val_activation_23_loss: 99.9999 - val_dense_784_loss: 53.5038 - val_dense_788_loss: 69.6610\n",
      "Epoch 283/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 239.0923 - activation_23_loss: 99.9919 - dense_784_loss: 64.6622 - dense_788_loss: 74.4382 - val_loss: 222.6920 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 53.3099 - val_dense_788_loss: 69.3822\n",
      "Epoch 284/1000\n",
      "317/317 [==============================] - 0s 149us/step - loss: 238.2189 - activation_23_loss: 99.9708 - dense_784_loss: 68.6895 - dense_788_loss: 69.5586 - val_loss: 222.4612 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 53.1374 - val_dense_788_loss: 69.3238\n",
      "Epoch 285/1000\n",
      "317/317 [==============================] - 0s 149us/step - loss: 255.3321 - activation_23_loss: 99.9887 - dense_784_loss: 63.5598 - dense_788_loss: 91.7835 - val_loss: 222.7169 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 53.0871 - val_dense_788_loss: 69.6299\n",
      "Epoch 286/1000\n",
      "317/317 [==============================] - 0s 146us/step - loss: 254.7127 - activation_23_loss: 99.9738 - dense_784_loss: 60.0957 - dense_788_loss: 94.6431 - val_loss: 223.2099 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.8547 - val_dense_788_loss: 70.3553\n",
      "Epoch 287/1000\n",
      "317/317 [==============================] - 0s 145us/step - loss: 246.0967 - activation_23_loss: 99.9843 - dense_784_loss: 59.7257 - dense_788_loss: 86.3867 - val_loss: 224.0502 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.5776 - val_dense_788_loss: 71.4727\n",
      "Epoch 288/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 239.5293 - activation_23_loss: 99.9828 - dense_784_loss: 69.2081 - dense_788_loss: 70.3384 - val_loss: 223.9626 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.2279 - val_dense_788_loss: 71.7347\n",
      "Epoch 289/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 242.3182 - activation_23_loss: 99.9500 - dense_784_loss: 68.1235 - dense_788_loss: 74.2447 - val_loss: 224.3670 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.5113 - val_dense_788_loss: 71.8558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 290/1000\n",
      "317/317 [==============================] - 0s 161us/step - loss: 246.7194 - activation_23_loss: 99.9789 - dense_784_loss: 70.9202 - dense_788_loss: 75.8203 - val_loss: 225.1790 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 53.0332 - val_dense_788_loss: 72.1458\n",
      "Epoch 291/1000\n",
      "317/317 [==============================] - 0s 164us/step - loss: 244.0970 - activation_23_loss: 99.9857 - dense_784_loss: 65.4558 - dense_788_loss: 78.6555 - val_loss: 225.7124 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 53.4061 - val_dense_788_loss: 72.3063\n",
      "Epoch 292/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 237.9053 - activation_23_loss: 99.9898 - dense_784_loss: 65.3861 - dense_788_loss: 72.5294 - val_loss: 225.8739 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 53.6602 - val_dense_788_loss: 72.2137\n",
      "Epoch 293/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 234.9120 - activation_23_loss: 99.9321 - dense_784_loss: 63.9921 - dense_788_loss: 70.9878 - val_loss: 225.5496 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 53.5295 - val_dense_788_loss: 72.0201\n",
      "Epoch 294/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 238.9309 - activation_23_loss: 99.9488 - dense_784_loss: 64.5175 - dense_788_loss: 74.4647 - val_loss: 224.8170 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 53.1114 - val_dense_788_loss: 71.7056\n",
      "Epoch 295/1000\n",
      "317/317 [==============================] - 0s 161us/step - loss: 233.8568 - activation_23_loss: 99.9771 - dense_784_loss: 62.5015 - dense_788_loss: 71.3782 - val_loss: 223.8247 - val_activation_23_loss: 99.9999 - val_dense_784_loss: 52.6403 - val_dense_788_loss: 71.1845\n",
      "Epoch 296/1000\n",
      "317/317 [==============================] - 0s 164us/step - loss: 241.3289 - activation_23_loss: 99.9807 - dense_784_loss: 63.5830 - dense_788_loss: 77.7652 - val_loss: 223.3329 - val_activation_23_loss: 99.9999 - val_dense_784_loss: 52.5114 - val_dense_788_loss: 70.8215\n",
      "Epoch 297/1000\n",
      "317/317 [==============================] - 0s 163us/step - loss: 231.7815 - activation_23_loss: 99.9673 - dense_784_loss: 62.9978 - dense_788_loss: 68.8164 - val_loss: 222.8220 - val_activation_23_loss: 99.9999 - val_dense_784_loss: 52.3570 - val_dense_788_loss: 70.4651\n",
      "Epoch 298/1000\n",
      "317/317 [==============================] - 0s 157us/step - loss: 246.2866 - activation_23_loss: 109.7201 - dense_784_loss: 66.9127 - dense_788_loss: 69.6538 - val_loss: 222.9294 - val_activation_23_loss: 99.9999 - val_dense_784_loss: 52.4186 - val_dense_788_loss: 70.5109\n",
      "Epoch 299/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 238.0416 - activation_23_loss: 99.9884 - dense_784_loss: 66.4228 - dense_788_loss: 71.6304 - val_loss: 222.6621 - val_activation_23_loss: 99.9998 - val_dense_784_loss: 52.2374 - val_dense_788_loss: 70.4248\n",
      "Epoch 300/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 240.9591 - activation_23_loss: 99.9776 - dense_784_loss: 66.0238 - dense_788_loss: 74.9576 - val_loss: 222.5738 - val_activation_23_loss: 99.9998 - val_dense_784_loss: 52.2647 - val_dense_788_loss: 70.3092\n",
      "Epoch 301/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 238.3442 - activation_23_loss: 99.9916 - dense_784_loss: 64.7963 - dense_788_loss: 73.5563 - val_loss: 223.0957 - val_activation_23_loss: 99.9999 - val_dense_784_loss: 52.5404 - val_dense_788_loss: 70.5554\n",
      "Epoch 302/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 230.2132 - activation_23_loss: 99.9895 - dense_784_loss: 63.0842 - dense_788_loss: 67.1395 - val_loss: 222.3627 - val_activation_23_loss: 99.9998 - val_dense_784_loss: 52.2367 - val_dense_788_loss: 70.1262\n",
      "Epoch 303/1000\n",
      "317/317 [==============================] - 0s 160us/step - loss: 235.9165 - activation_23_loss: 99.9887 - dense_784_loss: 65.7875 - dense_788_loss: 70.1402 - val_loss: 221.8771 - val_activation_23_loss: 99.9998 - val_dense_784_loss: 52.4513 - val_dense_788_loss: 69.4260\n",
      "Epoch 304/1000\n",
      "317/317 [==============================] - 0s 172us/step - loss: 229.9092 - activation_23_loss: 99.9663 - dense_784_loss: 64.3785 - dense_788_loss: 65.5644 - val_loss: 221.4204 - val_activation_23_loss: 99.9998 - val_dense_784_loss: 52.7053 - val_dense_788_loss: 68.7153\n",
      "Epoch 305/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 236.1298 - activation_23_loss: 99.9783 - dense_784_loss: 64.6990 - dense_788_loss: 71.4525 - val_loss: 220.3670 - val_activation_23_loss: 99.9998 - val_dense_784_loss: 52.2836 - val_dense_788_loss: 68.0836\n",
      "Epoch 306/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 252.9976 - activation_23_loss: 99.9811 - dense_784_loss: 71.2768 - dense_788_loss: 81.7398 - val_loss: 220.4516 - val_activation_23_loss: 99.9998 - val_dense_784_loss: 52.1569 - val_dense_788_loss: 68.2949\n",
      "Epoch 307/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 244.7059 - activation_23_loss: 99.9836 - dense_784_loss: 74.1445 - dense_788_loss: 70.5777 - val_loss: 221.9087 - val_activation_23_loss: 99.9998 - val_dense_784_loss: 53.1217 - val_dense_788_loss: 68.7871\n",
      "Epoch 308/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 245.9268 - activation_23_loss: 99.9833 - dense_784_loss: 72.6561 - dense_788_loss: 73.2874 - val_loss: 222.7505 - val_activation_23_loss: 99.9999 - val_dense_784_loss: 53.7288 - val_dense_788_loss: 69.0219\n",
      "Epoch 309/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 244.7022 - activation_23_loss: 99.9417 - dense_784_loss: 67.3668 - dense_788_loss: 77.3937 - val_loss: 223.2618 - val_activation_23_loss: 99.9999 - val_dense_784_loss: 54.0349 - val_dense_788_loss: 69.2270\n",
      "Epoch 310/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 242.4086 - activation_23_loss: 99.9852 - dense_784_loss: 66.2089 - dense_788_loss: 76.2146 - val_loss: 222.8349 - val_activation_23_loss: 99.9999 - val_dense_784_loss: 53.7141 - val_dense_788_loss: 69.1210\n",
      "Epoch 311/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 239.0064 - activation_23_loss: 99.9777 - dense_784_loss: 66.1024 - dense_788_loss: 72.9262 - val_loss: 222.4070 - val_activation_23_loss: 99.9998 - val_dense_784_loss: 53.3091 - val_dense_788_loss: 69.0981\n",
      "Epoch 312/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 236.8287 - activation_23_loss: 99.9805 - dense_784_loss: 63.0831 - dense_788_loss: 73.7651 - val_loss: 222.1420 - val_activation_23_loss: 99.9998 - val_dense_784_loss: 52.9718 - val_dense_788_loss: 69.1703\n",
      "Epoch 313/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 233.7351 - activation_23_loss: 99.9910 - dense_784_loss: 61.4783 - dense_788_loss: 72.2657 - val_loss: 221.6600 - val_activation_23_loss: 99.9998 - val_dense_784_loss: 52.5783 - val_dense_788_loss: 69.0819\n",
      "Epoch 314/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 238.3294 - activation_23_loss: 99.9821 - dense_784_loss: 69.3350 - dense_788_loss: 69.0123 - val_loss: 221.2979 - val_activation_23_loss: 99.9998 - val_dense_784_loss: 52.6934 - val_dense_788_loss: 68.6046\n",
      "Epoch 315/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 242.0974 - activation_23_loss: 99.9835 - dense_784_loss: 61.2088 - dense_788_loss: 80.9051 - val_loss: 220.9621 - val_activation_23_loss: 99.9999 - val_dense_784_loss: 52.0839 - val_dense_788_loss: 68.8783\n",
      "Epoch 316/1000\n",
      "317/317 [==============================] - 0s 161us/step - loss: 228.9426 - activation_23_loss: 99.9912 - dense_784_loss: 62.9714 - dense_788_loss: 65.9800 - val_loss: 220.6640 - val_activation_23_loss: 99.9999 - val_dense_784_loss: 51.5525 - val_dense_788_loss: 69.1116\n",
      "Epoch 317/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 237.7364 - activation_23_loss: 99.9908 - dense_784_loss: 65.3202 - dense_788_loss: 72.4254 - val_loss: 220.2841 - val_activation_23_loss: 99.9999 - val_dense_784_loss: 51.1728 - val_dense_788_loss: 69.1114\n",
      "Epoch 318/1000\n",
      "317/317 [==============================] - 0s 149us/step - loss: 250.2354 - activation_23_loss: 99.9325 - dense_784_loss: 69.2080 - dense_788_loss: 81.0949 - val_loss: 220.9214 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 51.3353 - val_dense_788_loss: 69.5861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 319/1000\n",
      "317/317 [==============================] - 0s 149us/step - loss: 233.8391 - activation_23_loss: 99.9953 - dense_784_loss: 59.2778 - dense_788_loss: 74.5660 - val_loss: 221.4409 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 51.3204 - val_dense_788_loss: 70.1206\n",
      "Epoch 320/1000\n",
      "317/317 [==============================] - 0s 149us/step - loss: 249.7752 - activation_23_loss: 99.9873 - dense_784_loss: 71.4512 - dense_788_loss: 78.3366 - val_loss: 222.3712 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 51.8059 - val_dense_788_loss: 70.5653\n",
      "Epoch 321/1000\n",
      "317/317 [==============================] - 0s 149us/step - loss: 242.1453 - activation_23_loss: 99.9794 - dense_784_loss: 68.4449 - dense_788_loss: 73.7209 - val_loss: 222.9803 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.4654 - val_dense_788_loss: 70.5149\n",
      "Epoch 322/1000\n",
      "317/317 [==============================] - 0s 146us/step - loss: 231.6831 - activation_23_loss: 99.9925 - dense_784_loss: 64.6369 - dense_788_loss: 67.0537 - val_loss: 223.1727 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.8762 - val_dense_788_loss: 70.2965\n",
      "Epoch 323/1000\n",
      "317/317 [==============================] - 0s 150us/step - loss: 243.8167 - activation_23_loss: 99.9700 - dense_784_loss: 64.5848 - dense_788_loss: 79.2620 - val_loss: 223.4979 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 53.1173 - val_dense_788_loss: 70.3806\n",
      "Epoch 324/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 239.7748 - activation_23_loss: 99.9805 - dense_784_loss: 61.6369 - dense_788_loss: 78.1574 - val_loss: 223.5987 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 53.1729 - val_dense_788_loss: 70.4258\n",
      "Epoch 325/1000\n",
      "317/317 [==============================] - 0s 157us/step - loss: 236.8324 - activation_23_loss: 99.9789 - dense_784_loss: 67.2396 - dense_788_loss: 69.6139 - val_loss: 223.3740 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 53.0405 - val_dense_788_loss: 70.3334\n",
      "Epoch 326/1000\n",
      "317/317 [==============================] - 0s 146us/step - loss: 234.2002 - activation_23_loss: 99.8829 - dense_784_loss: 65.5276 - dense_788_loss: 68.7898 - val_loss: 223.2529 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 53.2235 - val_dense_788_loss: 70.0295\n",
      "Epoch 327/1000\n",
      "317/317 [==============================] - 0s 146us/step - loss: 242.9373 - activation_23_loss: 99.9564 - dense_784_loss: 64.5980 - dense_788_loss: 78.3830 - val_loss: 222.7412 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.9509 - val_dense_788_loss: 69.7903\n",
      "Epoch 328/1000\n",
      "317/317 [==============================] - 0s 141us/step - loss: 250.5245 - activation_23_loss: 99.9561 - dense_784_loss: 67.8701 - dense_788_loss: 82.6984 - val_loss: 222.7636 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 53.0546 - val_dense_788_loss: 69.7091\n",
      "Epoch 329/1000\n",
      "317/317 [==============================] - 0s 149us/step - loss: 232.9229 - activation_23_loss: 99.9925 - dense_784_loss: 66.9709 - dense_788_loss: 65.9595 - val_loss: 222.9246 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 53.4163 - val_dense_788_loss: 69.5084\n",
      "Epoch 330/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 234.3827 - activation_23_loss: 99.9771 - dense_784_loss: 67.2839 - dense_788_loss: 67.1217 - val_loss: 222.8796 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 53.8389 - val_dense_788_loss: 69.0408\n",
      "Epoch 331/1000\n",
      "317/317 [==============================] - 0s 161us/step - loss: 229.7951 - activation_23_loss: 99.9826 - dense_784_loss: 60.8660 - dense_788_loss: 68.9464 - val_loss: 222.0847 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 53.5428 - val_dense_788_loss: 68.5418\n",
      "Epoch 332/1000\n",
      "317/317 [==============================] - 0s 161us/step - loss: 230.0645 - activation_23_loss: 99.9731 - dense_784_loss: 62.7398 - dense_788_loss: 67.3517 - val_loss: 221.4456 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 53.1248 - val_dense_788_loss: 68.3208\n",
      "Epoch 333/1000\n",
      "317/317 [==============================] - 0s 152us/step - loss: 234.0759 - activation_23_loss: 99.9774 - dense_784_loss: 63.1356 - dense_788_loss: 70.9629 - val_loss: 221.2826 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 53.0092 - val_dense_788_loss: 68.2734\n",
      "Epoch 334/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 232.2390 - activation_23_loss: 99.9898 - dense_784_loss: 62.2504 - dense_788_loss: 69.9987 - val_loss: 220.8033 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.6241 - val_dense_788_loss: 68.1792\n",
      "Epoch 335/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 233.2959 - activation_23_loss: 99.9870 - dense_784_loss: 65.0365 - dense_788_loss: 68.2724 - val_loss: 220.3025 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.2714 - val_dense_788_loss: 68.0311\n",
      "Epoch 336/1000\n",
      "317/317 [==============================] - 0s 164us/step - loss: 243.1934 - activation_23_loss: 99.9853 - dense_784_loss: 66.8658 - dense_788_loss: 76.3423 - val_loss: 220.0343 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.2587 - val_dense_788_loss: 67.7756\n",
      "Epoch 337/1000\n",
      "317/317 [==============================] - 0s 161us/step - loss: 228.8018 - activation_23_loss: 99.9790 - dense_784_loss: 64.5898 - dense_788_loss: 64.2329 - val_loss: 219.3557 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.1027 - val_dense_788_loss: 67.2530\n",
      "Epoch 338/1000\n",
      "317/317 [==============================] - 0s 152us/step - loss: 244.9922 - activation_23_loss: 99.9370 - dense_784_loss: 67.6622 - dense_788_loss: 77.3930 - val_loss: 219.0488 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 51.8290 - val_dense_788_loss: 67.2198\n",
      "Epoch 339/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 237.2762 - activation_23_loss: 99.9564 - dense_784_loss: 68.7912 - dense_788_loss: 68.5286 - val_loss: 219.3235 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.1385 - val_dense_788_loss: 67.1851\n",
      "Epoch 340/1000\n",
      "317/317 [==============================] - 0s 153us/step - loss: 231.9789 - activation_23_loss: 99.9798 - dense_784_loss: 61.1022 - dense_788_loss: 70.8969 - val_loss: 218.9503 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.1187 - val_dense_788_loss: 66.8317\n",
      "Epoch 341/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 243.1232 - activation_23_loss: 99.9925 - dense_784_loss: 62.2279 - dense_788_loss: 80.9027 - val_loss: 218.6773 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 51.5621 - val_dense_788_loss: 67.1152\n",
      "Epoch 342/1000\n",
      "317/317 [==============================] - 0s 161us/step - loss: 232.1918 - activation_23_loss: 99.9770 - dense_784_loss: 64.5804 - dense_788_loss: 67.6343 - val_loss: 218.2183 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 50.9635 - val_dense_788_loss: 67.2548\n",
      "Epoch 343/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 248.8221 - activation_23_loss: 99.9685 - dense_784_loss: 67.4221 - dense_788_loss: 81.4314 - val_loss: 219.3391 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 51.3367 - val_dense_788_loss: 68.0024\n",
      "Epoch 344/1000\n",
      "317/317 [==============================] - 0s 150us/step - loss: 231.0239 - activation_23_loss: 99.9825 - dense_784_loss: 65.0375 - dense_788_loss: 66.0039 - val_loss: 219.4998 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 51.5484 - val_dense_788_loss: 67.9515\n",
      "Epoch 345/1000\n",
      "317/317 [==============================] - 0s 166us/step - loss: 238.2230 - activation_23_loss: 99.9484 - dense_784_loss: 62.7227 - dense_788_loss: 75.5519 - val_loss: 219.7427 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 51.7969 - val_dense_788_loss: 67.9458\n",
      "Epoch 346/1000\n",
      "317/317 [==============================] - 0s 150us/step - loss: 246.2372 - activation_23_loss: 99.9129 - dense_784_loss: 69.8947 - dense_788_loss: 76.4296 - val_loss: 220.3012 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.2995 - val_dense_788_loss: 68.0017\n",
      "Epoch 347/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "317/317 [==============================] - 0s 150us/step - loss: 240.7222 - activation_23_loss: 99.9853 - dense_784_loss: 70.2146 - dense_788_loss: 70.5223 - val_loss: 220.7844 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.8594 - val_dense_788_loss: 67.9250\n",
      "Epoch 348/1000\n",
      "317/317 [==============================] - 0s 150us/step - loss: 247.3173 - activation_23_loss: 99.9909 - dense_784_loss: 64.2755 - dense_788_loss: 83.0509 - val_loss: 220.9006 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 53.0550 - val_dense_788_loss: 67.8455\n",
      "Epoch 349/1000\n",
      "317/317 [==============================] - 0s 149us/step - loss: 234.5938 - activation_23_loss: 99.9621 - dense_784_loss: 63.3304 - dense_788_loss: 71.3013 - val_loss: 221.4941 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.9900 - val_dense_788_loss: 68.5041\n",
      "Epoch 350/1000\n",
      "317/317 [==============================] - 0s 144us/step - loss: 248.4740 - activation_23_loss: 99.9669 - dense_784_loss: 67.7308 - dense_788_loss: 80.7762 - val_loss: 221.3253 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.6117 - val_dense_788_loss: 68.7136\n",
      "Epoch 351/1000\n",
      "317/317 [==============================] - 0s 149us/step - loss: 238.6448 - activation_23_loss: 99.9705 - dense_784_loss: 65.1276 - dense_788_loss: 73.5467 - val_loss: 221.3536 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.4234 - val_dense_788_loss: 68.9302\n",
      "Epoch 352/1000\n",
      "317/317 [==============================] - 0s 149us/step - loss: 234.5765 - activation_23_loss: 100.3126 - dense_784_loss: 61.4409 - dense_788_loss: 72.8231 - val_loss: 221.1261 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.1364 - val_dense_788_loss: 68.9898\n",
      "Epoch 353/1000\n",
      "317/317 [==============================] - 0s 145us/step - loss: 243.3581 - activation_23_loss: 100.0882 - dense_784_loss: 66.4250 - dense_788_loss: 76.8449 - val_loss: 221.1948 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 51.9958 - val_dense_788_loss: 69.1991\n",
      "Epoch 354/1000\n",
      "317/317 [==============================] - 0s 146us/step - loss: 239.2467 - activation_23_loss: 99.9857 - dense_784_loss: 63.2466 - dense_788_loss: 76.0144 - val_loss: 221.2785 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 51.7904 - val_dense_788_loss: 69.4882\n",
      "Epoch 355/1000\n",
      "317/317 [==============================] - 0s 142us/step - loss: 229.8676 - activation_23_loss: 99.9800 - dense_784_loss: 59.9021 - dense_788_loss: 69.9855 - val_loss: 220.9806 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 51.6609 - val_dense_788_loss: 69.3196\n",
      "Epoch 356/1000\n",
      "317/317 [==============================] - 0s 149us/step - loss: 240.9745 - activation_23_loss: 99.9835 - dense_784_loss: 64.7602 - dense_788_loss: 76.2308 - val_loss: 220.5452 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 51.5634 - val_dense_788_loss: 68.9818\n",
      "Epoch 357/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 231.2107 - activation_23_loss: 99.9550 - dense_784_loss: 64.4420 - dense_788_loss: 66.8137 - val_loss: 220.5021 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 51.5894 - val_dense_788_loss: 68.9127\n",
      "Epoch 358/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 229.7949 - activation_23_loss: 99.9458 - dense_784_loss: 63.2481 - dense_788_loss: 66.6010 - val_loss: 220.3013 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 51.7223 - val_dense_788_loss: 68.5790\n",
      "Epoch 359/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 238.7686 - activation_23_loss: 99.9840 - dense_784_loss: 63.8186 - dense_788_loss: 74.9660 - val_loss: 219.7956 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 51.5320 - val_dense_788_loss: 68.2636\n",
      "Epoch 360/1000\n",
      "317/317 [==============================] - 0s 161us/step - loss: 233.2545 - activation_23_loss: 99.9739 - dense_784_loss: 62.2318 - dense_788_loss: 71.0488 - val_loss: 219.3673 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 51.1605 - val_dense_788_loss: 68.2068\n",
      "Epoch 361/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 240.2983 - activation_23_loss: 99.9807 - dense_784_loss: 70.0670 - dense_788_loss: 70.2506 - val_loss: 219.4156 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 51.2846 - val_dense_788_loss: 68.1311\n",
      "Epoch 362/1000\n",
      "317/317 [==============================] - 0s 161us/step - loss: 234.5212 - activation_23_loss: 99.9620 - dense_784_loss: 61.4573 - dense_788_loss: 73.1019 - val_loss: 219.2429 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 51.3415 - val_dense_788_loss: 67.9015\n",
      "Epoch 363/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 235.7111 - activation_23_loss: 99.9952 - dense_784_loss: 67.8959 - dense_788_loss: 67.8200 - val_loss: 219.1509 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 51.3490 - val_dense_788_loss: 67.8019\n",
      "Epoch 364/1000\n",
      "317/317 [==============================] - 0s 161us/step - loss: 236.5911 - activation_23_loss: 99.9866 - dense_784_loss: 63.0470 - dense_788_loss: 73.5574 - val_loss: 218.8056 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 51.2960 - val_dense_788_loss: 67.5096\n",
      "Epoch 365/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 232.3992 - activation_23_loss: 99.9858 - dense_784_loss: 60.3873 - dense_788_loss: 72.0261 - val_loss: 218.8404 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 51.2032 - val_dense_788_loss: 67.6372\n",
      "Epoch 366/1000\n",
      "317/317 [==============================] - 0s 168us/step - loss: 231.1825 - activation_23_loss: 99.9763 - dense_784_loss: 66.5146 - dense_788_loss: 64.6917 - val_loss: 218.9258 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 51.4096 - val_dense_788_loss: 67.5163\n",
      "Epoch 367/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 243.3104 - activation_23_loss: 99.9607 - dense_784_loss: 69.1171 - dense_788_loss: 74.2325 - val_loss: 219.2399 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 51.5808 - val_dense_788_loss: 67.6591\n",
      "Epoch 368/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 235.6778 - activation_23_loss: 99.9920 - dense_784_loss: 62.8492 - dense_788_loss: 72.8366 - val_loss: 219.7763 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 51.9311 - val_dense_788_loss: 67.8452\n",
      "Epoch 369/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 238.4269 - activation_23_loss: 99.9834 - dense_784_loss: 63.9083 - dense_788_loss: 74.5353 - val_loss: 220.2007 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.3373 - val_dense_788_loss: 67.8634\n",
      "Epoch 370/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 236.0458 - activation_23_loss: 99.9574 - dense_784_loss: 67.3986 - dense_788_loss: 68.6898 - val_loss: 220.5624 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.7970 - val_dense_788_loss: 67.7654\n",
      "Epoch 371/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 233.4255 - activation_23_loss: 99.9872 - dense_784_loss: 56.2373 - dense_788_loss: 77.2010 - val_loss: 220.2947 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.5411 - val_dense_788_loss: 67.7536\n",
      "Epoch 372/1000\n",
      "317/317 [==============================] - 0s 152us/step - loss: 230.5786 - activation_23_loss: 99.9877 - dense_784_loss: 65.3991 - dense_788_loss: 65.1919 - val_loss: 220.0071 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.3048 - val_dense_788_loss: 67.7024\n",
      "Epoch 373/1000\n",
      "317/317 [==============================] - 0s 149us/step - loss: 229.6002 - activation_23_loss: 99.9786 - dense_784_loss: 58.4005 - dense_788_loss: 71.2211 - val_loss: 219.4063 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.1215 - val_dense_788_loss: 67.2848\n",
      "Epoch 374/1000\n",
      "317/317 [==============================] - 0s 149us/step - loss: 237.5360 - activation_23_loss: 99.9850 - dense_784_loss: 68.0910 - dense_788_loss: 69.4600 - val_loss: 219.4241 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.2714 - val_dense_788_loss: 67.1527\n",
      "Epoch 375/1000\n",
      "317/317 [==============================] - 0s 149us/step - loss: 227.8684 - activation_23_loss: 99.9714 - dense_784_loss: 63.8406 - dense_788_loss: 64.0564 - val_loss: 219.0265 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.0736 - val_dense_788_loss: 66.9530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 376/1000\n",
      "317/317 [==============================] - 0s 149us/step - loss: 243.2168 - activation_23_loss: 99.9923 - dense_784_loss: 67.1121 - dense_788_loss: 76.1125 - val_loss: 218.7864 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.1572 - val_dense_788_loss: 66.6292\n",
      "Epoch 377/1000\n",
      "317/317 [==============================] - 0s 152us/step - loss: 235.3613 - activation_23_loss: 99.9822 - dense_784_loss: 68.0786 - dense_788_loss: 67.3005 - val_loss: 219.0566 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.5797 - val_dense_788_loss: 66.4769\n",
      "Epoch 378/1000\n",
      "317/317 [==============================] - 0s 152us/step - loss: 239.9905 - activation_23_loss: 99.9721 - dense_784_loss: 72.2900 - dense_788_loss: 67.7284 - val_loss: 219.2167 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.9775 - val_dense_788_loss: 66.2392\n",
      "Epoch 379/1000\n",
      "317/317 [==============================] - 0s 149us/step - loss: 244.1791 - activation_23_loss: 99.9672 - dense_784_loss: 74.1515 - dense_788_loss: 70.0604 - val_loss: 219.8252 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 53.7037 - val_dense_788_loss: 66.1215\n",
      "Epoch 380/1000\n",
      "317/317 [==============================] - 0s 146us/step - loss: 233.1811 - activation_23_loss: 99.9832 - dense_784_loss: 64.1947 - dense_788_loss: 69.0033 - val_loss: 219.4327 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 53.6673 - val_dense_788_loss: 65.7654\n",
      "Epoch 381/1000\n",
      "317/317 [==============================] - 0s 149us/step - loss: 233.1906 - activation_23_loss: 99.9286 - dense_784_loss: 66.0860 - dense_788_loss: 67.1760 - val_loss: 218.9577 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 53.4220 - val_dense_788_loss: 65.5357\n",
      "Epoch 382/1000\n",
      "317/317 [==============================] - 0s 149us/step - loss: 245.7942 - activation_23_loss: 99.9815 - dense_784_loss: 76.3511 - dense_788_loss: 69.4617 - val_loss: 218.1949 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 53.1436 - val_dense_788_loss: 65.0513\n",
      "Epoch 383/1000\n",
      "317/317 [==============================] - 0s 144us/step - loss: 228.8616 - activation_23_loss: 99.9439 - dense_784_loss: 59.3766 - dense_788_loss: 69.5411 - val_loss: 218.5802 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 53.3733 - val_dense_788_loss: 65.2069\n",
      "Epoch 384/1000\n",
      "317/317 [==============================] - 0s 150us/step - loss: 238.8803 - activation_23_loss: 99.9856 - dense_784_loss: 68.5744 - dense_788_loss: 70.3203 - val_loss: 218.0833 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 53.0144 - val_dense_788_loss: 65.0690\n",
      "Epoch 385/1000\n",
      "317/317 [==============================] - 0s 147us/step - loss: 242.1456 - activation_23_loss: 99.9835 - dense_784_loss: 72.9305 - dense_788_loss: 69.2316 - val_loss: 219.1479 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 53.5686 - val_dense_788_loss: 65.5793\n",
      "Epoch 386/1000\n",
      "317/317 [==============================] - 0s 152us/step - loss: 223.9388 - activation_23_loss: 99.9845 - dense_784_loss: 60.6926 - dense_788_loss: 63.2618 - val_loss: 219.1522 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 53.4937 - val_dense_788_loss: 65.6584\n",
      "Epoch 387/1000\n",
      "317/317 [==============================] - 0s 153us/step - loss: 237.0985 - activation_23_loss: 99.9399 - dense_784_loss: 60.5985 - dense_788_loss: 76.5600 - val_loss: 219.5121 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 53.2890 - val_dense_788_loss: 66.2231\n",
      "Epoch 388/1000\n",
      "317/317 [==============================] - 0s 164us/step - loss: 235.3217 - activation_23_loss: 99.9890 - dense_784_loss: 62.7271 - dense_788_loss: 72.6056 - val_loss: 219.1999 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.7938 - val_dense_788_loss: 66.4061\n",
      "Epoch 389/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 250.7343 - activation_23_loss: 99.9895 - dense_784_loss: 67.0768 - dense_788_loss: 83.6680 - val_loss: 220.6484 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 53.0515 - val_dense_788_loss: 67.5969\n",
      "Epoch 390/1000\n",
      "317/317 [==============================] - 0s 161us/step - loss: 240.6294 - activation_23_loss: 99.9850 - dense_784_loss: 70.1116 - dense_788_loss: 70.5328 - val_loss: 221.2298 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 53.3185 - val_dense_788_loss: 67.9113\n",
      "Epoch 391/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 236.2314 - activation_23_loss: 99.9873 - dense_784_loss: 65.7793 - dense_788_loss: 70.4649 - val_loss: 221.3059 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 53.3813 - val_dense_788_loss: 67.9246\n",
      "Epoch 392/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 237.0698 - activation_23_loss: 99.9924 - dense_784_loss: 69.1517 - dense_788_loss: 67.9256 - val_loss: 220.8527 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 53.1028 - val_dense_788_loss: 67.7498\n",
      "Epoch 393/1000\n",
      "317/317 [==============================] - 0s 152us/step - loss: 233.8235 - activation_23_loss: 99.9955 - dense_784_loss: 68.2783 - dense_788_loss: 65.5497 - val_loss: 220.0832 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.9676 - val_dense_788_loss: 67.1156\n",
      "Epoch 394/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 233.1023 - activation_23_loss: 99.9394 - dense_784_loss: 66.1524 - dense_788_loss: 67.0105 - val_loss: 219.9657 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 53.2857 - val_dense_788_loss: 66.6800\n",
      "Epoch 395/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 245.5794 - activation_23_loss: 99.9198 - dense_784_loss: 65.6844 - dense_788_loss: 79.9753 - val_loss: 220.4621 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 53.6955 - val_dense_788_loss: 66.7667\n",
      "Epoch 396/1000\n",
      "317/317 [==============================] - 0s 153us/step - loss: 243.5571 - activation_23_loss: 99.9769 - dense_784_loss: 69.3319 - dense_788_loss: 74.2483 - val_loss: 220.7677 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 53.8318 - val_dense_788_loss: 66.9359\n",
      "Epoch 397/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 229.5488 - activation_23_loss: 99.9736 - dense_784_loss: 64.1753 - dense_788_loss: 65.3999 - val_loss: 220.5094 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 53.7744 - val_dense_788_loss: 66.7351\n",
      "Epoch 398/1000\n",
      "317/317 [==============================] - 0s 161us/step - loss: 245.3136 - activation_23_loss: 99.9810 - dense_784_loss: 74.2342 - dense_788_loss: 71.0985 - val_loss: 221.0739 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 54.1734 - val_dense_788_loss: 66.9005\n",
      "Epoch 399/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 235.8174 - activation_23_loss: 99.9845 - dense_784_loss: 67.9892 - dense_788_loss: 67.8437 - val_loss: 220.6360 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 53.8967 - val_dense_788_loss: 66.7394\n",
      "Epoch 400/1000\n",
      "317/317 [==============================] - 0s 157us/step - loss: 237.5586 - activation_23_loss: 99.9778 - dense_784_loss: 67.2241 - dense_788_loss: 70.3568 - val_loss: 220.8742 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 54.0643 - val_dense_788_loss: 66.8099\n",
      "Epoch 401/1000\n",
      "317/317 [==============================] - 0s 160us/step - loss: 227.6597 - activation_23_loss: 99.9919 - dense_784_loss: 63.8416 - dense_788_loss: 63.8262 - val_loss: 220.3282 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 54.0246 - val_dense_788_loss: 66.3036\n",
      "Epoch 402/1000\n",
      "317/317 [==============================] - 0s 153us/step - loss: 226.5900 - activation_23_loss: 99.9795 - dense_784_loss: 61.5013 - dense_788_loss: 65.1092 - val_loss: 219.2243 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 53.4781 - val_dense_788_loss: 65.7462\n",
      "Epoch 403/1000\n",
      "317/317 [==============================] - 0s 160us/step - loss: 232.7466 - activation_23_loss: 99.9822 - dense_784_loss: 63.1128 - dense_788_loss: 69.6516 - val_loss: 218.3637 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 53.0785 - val_dense_788_loss: 65.2852\n",
      "Epoch 404/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "317/317 [==============================] - 0s 157us/step - loss: 241.5837 - activation_23_loss: 99.9436 - dense_784_loss: 65.9736 - dense_788_loss: 75.6665 - val_loss: 217.7936 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.8361 - val_dense_788_loss: 64.9575\n",
      "Epoch 405/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 236.6264 - activation_23_loss: 99.9530 - dense_784_loss: 71.1545 - dense_788_loss: 65.5189 - val_loss: 218.1407 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 53.1943 - val_dense_788_loss: 64.9464\n",
      "Epoch 406/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 235.2035 - activation_23_loss: 99.9605 - dense_784_loss: 64.1752 - dense_788_loss: 71.0678 - val_loss: 218.8110 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 53.4739 - val_dense_788_loss: 65.3371\n",
      "Epoch 407/1000\n",
      "317/317 [==============================] - 0s 152us/step - loss: 241.0950 - activation_23_loss: 100.0229 - dense_784_loss: 67.8687 - dense_788_loss: 73.2034 - val_loss: 218.4791 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 53.2898 - val_dense_788_loss: 65.1893\n",
      "Epoch 408/1000\n",
      "317/317 [==============================] - 0s 174us/step - loss: 237.4936 - activation_23_loss: 99.9857 - dense_784_loss: 64.6295 - dense_788_loss: 72.8783 - val_loss: 218.4296 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 53.3230 - val_dense_788_loss: 65.1066\n",
      "Epoch 409/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 235.3297 - activation_23_loss: 99.9872 - dense_784_loss: 64.7731 - dense_788_loss: 70.5694 - val_loss: 219.1550 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 53.5247 - val_dense_788_loss: 65.6303\n",
      "Epoch 410/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 231.2591 - activation_23_loss: 99.9843 - dense_784_loss: 65.8472 - dense_788_loss: 65.4276 - val_loss: 219.7733 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 53.8255 - val_dense_788_loss: 65.9479\n",
      "Epoch 411/1000\n",
      "317/317 [==============================] - 0s 152us/step - loss: 240.1192 - activation_23_loss: 99.9814 - dense_784_loss: 67.0049 - dense_788_loss: 73.1329 - val_loss: 219.4086 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 53.5423 - val_dense_788_loss: 65.8663\n",
      "Epoch 412/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 235.2740 - activation_23_loss: 99.9893 - dense_784_loss: 68.0068 - dense_788_loss: 67.2779 - val_loss: 219.5810 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 53.6233 - val_dense_788_loss: 65.9577\n",
      "Epoch 413/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 239.0220 - activation_23_loss: 99.9952 - dense_784_loss: 65.0840 - dense_788_loss: 73.9429 - val_loss: 220.0017 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 53.8423 - val_dense_788_loss: 66.1594\n",
      "Epoch 414/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 240.7235 - activation_23_loss: 100.0189 - dense_784_loss: 68.1218 - dense_788_loss: 72.5828 - val_loss: 220.6059 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 54.0638 - val_dense_788_loss: 66.5421\n",
      "Epoch 415/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 229.2714 - activation_23_loss: 99.9762 - dense_784_loss: 60.8149 - dense_788_loss: 68.4804 - val_loss: 220.3720 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 53.8278 - val_dense_788_loss: 66.5442\n",
      "Epoch 416/1000\n",
      "317/317 [==============================] - 0s 152us/step - loss: 241.8482 - activation_23_loss: 99.9772 - dense_784_loss: 61.8273 - dense_788_loss: 80.0437 - val_loss: 220.2628 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 53.4046 - val_dense_788_loss: 66.8583\n",
      "Epoch 417/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 232.7295 - activation_23_loss: 99.9872 - dense_784_loss: 61.8706 - dense_788_loss: 70.8717 - val_loss: 219.9983 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.9826 - val_dense_788_loss: 67.0157\n",
      "Epoch 418/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 27100.1825 - activation_23_loss: 26968.6834 - dense_784_loss: 62.6131 - dense_788_loss: 68.8879 - val_loss: 220.4767 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.8437 - val_dense_788_loss: 67.6330\n",
      "Epoch 419/1000\n",
      "317/317 [==============================] - 0s 152us/step - loss: 239.9511 - activation_23_loss: 99.9943 - dense_784_loss: 67.0912 - dense_788_loss: 72.8655 - val_loss: 220.2821 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.8436 - val_dense_788_loss: 67.4385\n",
      "Epoch 420/1000\n",
      "317/317 [==============================] - 0s 152us/step - loss: 232.4003 - activation_23_loss: 99.9857 - dense_784_loss: 63.1462 - dense_788_loss: 69.2684 - val_loss: 219.4997 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.5332 - val_dense_788_loss: 66.9665\n",
      "Epoch 421/1000\n",
      "317/317 [==============================] - 0s 152us/step - loss: 229.7282 - activation_23_loss: 99.9928 - dense_784_loss: 65.6066 - dense_788_loss: 64.1289 - val_loss: 218.2963 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.2424 - val_dense_788_loss: 66.0540\n",
      "Epoch 422/1000\n",
      "317/317 [==============================] - 0s 150us/step - loss: 240.9172 - activation_23_loss: 99.9913 - dense_784_loss: 72.0329 - dense_788_loss: 68.8930 - val_loss: 217.7711 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.3624 - val_dense_788_loss: 65.4088\n",
      "Epoch 423/1000\n",
      "317/317 [==============================] - 0s 152us/step - loss: 233.9394 - activation_23_loss: 99.9901 - dense_784_loss: 61.2005 - dense_788_loss: 72.7489 - val_loss: 217.6958 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.3600 - val_dense_788_loss: 65.3358\n",
      "Epoch 424/1000\n",
      "317/317 [==============================] - 0s 146us/step - loss: 230.6379 - activation_23_loss: 99.9925 - dense_784_loss: 66.5579 - dense_788_loss: 64.0875 - val_loss: 216.9446 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.1912 - val_dense_788_loss: 64.7534\n",
      "Epoch 425/1000\n",
      "317/317 [==============================] - 0s 147us/step - loss: 234.0102 - activation_23_loss: 99.9940 - dense_784_loss: 63.7696 - dense_788_loss: 70.2466 - val_loss: 216.0251 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 51.7897 - val_dense_788_loss: 64.2354\n",
      "Epoch 426/1000\n",
      "317/317 [==============================] - 0s 144us/step - loss: 224.9166 - activation_23_loss: 99.9925 - dense_784_loss: 57.9073 - dense_788_loss: 67.0169 - val_loss: 215.6673 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 51.7063 - val_dense_788_loss: 63.9610\n",
      "Epoch 427/1000\n",
      "317/317 [==============================] - 0s 144us/step - loss: 242.6784 - activation_23_loss: 99.9932 - dense_784_loss: 67.7801 - dense_788_loss: 74.9051 - val_loss: 216.1074 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 51.8968 - val_dense_788_loss: 64.2106\n",
      "Epoch 428/1000\n",
      "317/317 [==============================] - 0s 145us/step - loss: 237.5300 - activation_23_loss: 99.9925 - dense_784_loss: 68.3765 - dense_788_loss: 69.1610 - val_loss: 216.2231 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.1982 - val_dense_788_loss: 64.0249\n",
      "Epoch 429/1000\n",
      "317/317 [==============================] - 0s 164us/step - loss: 233.6745 - activation_23_loss: 99.9865 - dense_784_loss: 68.3735 - dense_788_loss: 65.3145 - val_loss: 216.1143 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.5730 - val_dense_788_loss: 63.5413\n",
      "Epoch 430/1000\n",
      "317/317 [==============================] - 0s 152us/step - loss: 240.4409 - activation_23_loss: 99.9941 - dense_784_loss: 63.0225 - dense_788_loss: 77.4242 - val_loss: 215.7367 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.3606 - val_dense_788_loss: 63.3761\n",
      "Epoch 431/1000\n",
      "317/317 [==============================] - 0s 153us/step - loss: 236.2586 - activation_23_loss: 99.9757 - dense_784_loss: 65.2468 - dense_788_loss: 71.0360 - val_loss: 215.7665 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.1844 - val_dense_788_loss: 63.5821\n",
      "Epoch 432/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 240.8125 - activation_23_loss: 99.9921 - dense_784_loss: 65.6882 - dense_788_loss: 75.1322 - val_loss: 216.0493 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.1193 - val_dense_788_loss: 63.9300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 433/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 233.2131 - activation_23_loss: 99.9210 - dense_784_loss: 67.1194 - dense_788_loss: 66.1727 - val_loss: 216.9084 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.3969 - val_dense_788_loss: 64.5115\n",
      "Epoch 434/1000\n",
      "317/317 [==============================] - 0s 163us/step - loss: 249.9940 - activation_23_loss: 99.9919 - dense_784_loss: 66.9045 - dense_788_loss: 83.0976 - val_loss: 218.0103 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.8956 - val_dense_788_loss: 65.1148\n",
      "Epoch 435/1000\n",
      "317/317 [==============================] - 0s 157us/step - loss: 230.1679 - activation_23_loss: 99.9935 - dense_784_loss: 67.0448 - dense_788_loss: 63.1296 - val_loss: 218.3570 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 53.0756 - val_dense_788_loss: 65.2815\n",
      "Epoch 436/1000\n",
      "317/317 [==============================] - 0s 163us/step - loss: 238.8332 - activation_23_loss: 99.9542 - dense_784_loss: 62.5120 - dense_788_loss: 76.3669 - val_loss: 218.4111 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.9503 - val_dense_788_loss: 65.4608\n",
      "Epoch 437/1000\n",
      "317/317 [==============================] - 0s 161us/step - loss: 236.6967 - activation_23_loss: 99.9751 - dense_784_loss: 67.1802 - dense_788_loss: 69.5413 - val_loss: 218.3497 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.7695 - val_dense_788_loss: 65.5802\n",
      "Epoch 438/1000\n",
      "317/317 [==============================] - 0s 160us/step - loss: 240.9548 - activation_23_loss: 99.9901 - dense_784_loss: 71.6948 - dense_788_loss: 69.2699 - val_loss: 218.0089 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.6517 - val_dense_788_loss: 65.3573\n",
      "Epoch 439/1000\n",
      "317/317 [==============================] - 0s 157us/step - loss: 233.8083 - activation_23_loss: 99.9905 - dense_784_loss: 66.4795 - dense_788_loss: 67.3383 - val_loss: 217.9934 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.8468 - val_dense_788_loss: 65.1467\n",
      "Epoch 440/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 238.2837 - activation_23_loss: 100.0729 - dense_784_loss: 68.2432 - dense_788_loss: 69.9676 - val_loss: 217.8765 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.9551 - val_dense_788_loss: 64.9214\n",
      "Epoch 441/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 230.2567 - activation_23_loss: 99.9935 - dense_784_loss: 63.5715 - dense_788_loss: 66.6917 - val_loss: 217.5457 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.6099 - val_dense_788_loss: 64.9358\n",
      "Epoch 442/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 238.2065 - activation_23_loss: 99.9909 - dense_784_loss: 65.4850 - dense_788_loss: 72.7306 - val_loss: 217.2959 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.2783 - val_dense_788_loss: 65.0176\n",
      "Epoch 443/1000\n",
      "317/317 [==============================] - 0s 157us/step - loss: 243.7230 - activation_23_loss: 99.9926 - dense_784_loss: 63.6912 - dense_788_loss: 80.0392 - val_loss: 217.2813 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.1751 - val_dense_788_loss: 65.1062\n",
      "Epoch 444/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 232.0820 - activation_23_loss: 99.9941 - dense_784_loss: 62.7818 - dense_788_loss: 69.3062 - val_loss: 217.4236 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.1467 - val_dense_788_loss: 65.2770\n",
      "Epoch 445/1000\n",
      "317/317 [==============================] - 0s 163us/step - loss: 234.3044 - activation_23_loss: 99.9904 - dense_784_loss: 64.3659 - dense_788_loss: 69.9481 - val_loss: 217.3552 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 51.9419 - val_dense_788_loss: 65.4134\n",
      "Epoch 446/1000\n",
      "317/317 [==============================] - 0s 160us/step - loss: 245.1204 - activation_23_loss: 99.9915 - dense_784_loss: 67.4296 - dense_788_loss: 77.6994 - val_loss: 216.7894 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 51.6319 - val_dense_788_loss: 65.1575\n",
      "Epoch 447/1000\n",
      "317/317 [==============================] - 0s 160us/step - loss: 240.3833 - activation_23_loss: 99.9936 - dense_784_loss: 63.5262 - dense_788_loss: 76.8635 - val_loss: 217.1516 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 51.6481 - val_dense_788_loss: 65.5036\n",
      "Epoch 448/1000\n",
      "317/317 [==============================] - 0s 161us/step - loss: 239.2898 - activation_23_loss: 99.9867 - dense_784_loss: 70.8269 - dense_788_loss: 68.4762 - val_loss: 217.0724 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 51.6355 - val_dense_788_loss: 65.4369\n",
      "Epoch 449/1000\n",
      "317/317 [==============================] - 0s 177us/step - loss: 229.0360 - activation_23_loss: 99.9837 - dense_784_loss: 62.5891 - dense_788_loss: 66.4632 - val_loss: 216.7155 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 51.5924 - val_dense_788_loss: 65.1231\n",
      "Epoch 450/1000\n",
      "317/317 [==============================] - 0s 152us/step - loss: 235.7037 - activation_23_loss: 99.9088 - dense_784_loss: 64.2779 - dense_788_loss: 71.5170 - val_loss: 216.8074 - val_activation_23_loss: 99.9999 - val_dense_784_loss: 51.8158 - val_dense_788_loss: 64.9917\n",
      "Epoch 451/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 237.1593 - activation_23_loss: 99.9806 - dense_784_loss: 68.5743 - dense_788_loss: 68.6045 - val_loss: 216.7845 - val_activation_23_loss: 99.9998 - val_dense_784_loss: 52.1448 - val_dense_788_loss: 64.6399\n",
      "Epoch 452/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 231.7917 - activation_23_loss: 99.9828 - dense_784_loss: 61.9626 - dense_788_loss: 69.8462 - val_loss: 216.7618 - val_activation_23_loss: 99.9997 - val_dense_784_loss: 52.1786 - val_dense_788_loss: 64.5835\n",
      "Epoch 453/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 229.5558 - activation_23_loss: 99.9958 - dense_784_loss: 63.2825 - dense_788_loss: 66.2775 - val_loss: 216.7312 - val_activation_23_loss: 99.9997 - val_dense_784_loss: 52.0417 - val_dense_788_loss: 64.6898\n",
      "Epoch 454/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 230.5910 - activation_23_loss: 99.9387 - dense_784_loss: 64.7732 - dense_788_loss: 65.8791 - val_loss: 216.6233 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.1989 - val_dense_788_loss: 64.4244\n",
      "Epoch 455/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 232.2988 - activation_23_loss: 99.9884 - dense_784_loss: 63.3938 - dense_788_loss: 68.9166 - val_loss: 216.2333 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.0993 - val_dense_788_loss: 64.1340\n",
      "Epoch 456/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 234.2646 - activation_23_loss: 99.9892 - dense_784_loss: 63.1759 - dense_788_loss: 71.0994 - val_loss: 215.8555 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 51.9261 - val_dense_788_loss: 63.9295\n",
      "Epoch 457/1000\n",
      "317/317 [==============================] - 0s 164us/step - loss: 242.7023 - activation_23_loss: 99.9933 - dense_784_loss: 65.5840 - dense_788_loss: 77.1250 - val_loss: 215.8206 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 51.8973 - val_dense_788_loss: 63.9234\n",
      "Epoch 458/1000\n",
      "317/317 [==============================] - 0s 161us/step - loss: 241.1481 - activation_23_loss: 99.9917 - dense_784_loss: 66.3488 - dense_788_loss: 74.8076 - val_loss: 216.0306 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 51.9967 - val_dense_788_loss: 64.0340\n",
      "Epoch 459/1000\n",
      "317/317 [==============================] - 0s 161us/step - loss: 236.1234 - activation_23_loss: 99.9832 - dense_784_loss: 64.7284 - dense_788_loss: 71.4117 - val_loss: 216.6995 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.3127 - val_dense_788_loss: 64.3869\n",
      "Epoch 460/1000\n",
      "317/317 [==============================] - 0s 161us/step - loss: 238.1752 - activation_23_loss: 99.9428 - dense_784_loss: 61.6891 - dense_788_loss: 76.5433 - val_loss: 216.9682 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.3861 - val_dense_788_loss: 64.5821\n",
      "Epoch 461/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "317/317 [==============================] - 0s 155us/step - loss: 239.3584 - activation_23_loss: 99.9926 - dense_784_loss: 64.5878 - dense_788_loss: 74.7780 - val_loss: 217.1926 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.2860 - val_dense_788_loss: 64.9067\n",
      "Epoch 462/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 220.1656 - activation_23_loss: 99.9764 - dense_784_loss: 57.6954 - dense_788_loss: 62.4937 - val_loss: 217.2704 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.2898 - val_dense_788_loss: 64.9806\n",
      "Epoch 463/1000\n",
      "317/317 [==============================] - 0s 164us/step - loss: 240.4259 - activation_23_loss: 99.9890 - dense_784_loss: 69.9020 - dense_788_loss: 70.5349 - val_loss: 217.0270 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.3657 - val_dense_788_loss: 64.6613\n",
      "Epoch 464/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 237.5342 - activation_23_loss: 99.9550 - dense_784_loss: 64.6043 - dense_788_loss: 72.9749 - val_loss: 216.8209 - val_activation_23_loss: 100.0000 - val_dense_784_loss: 52.4483 - val_dense_788_loss: 64.3726\n",
      "Epoch 465/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 237.4906 - activation_23_loss: 99.9821 - dense_784_loss: 64.7633 - dense_788_loss: 72.7453 - val_loss: 216.4451 - val_activation_23_loss: 99.9999 - val_dense_784_loss: 52.2200 - val_dense_788_loss: 64.2252\n",
      "Epoch 466/1000\n",
      "317/317 [==============================] - 0s 152us/step - loss: 250.7369 - activation_23_loss: 99.9882 - dense_784_loss: 63.6958 - dense_788_loss: 87.0529 - val_loss: 216.4361 - val_activation_23_loss: 99.9999 - val_dense_784_loss: 51.9799 - val_dense_788_loss: 64.4564\n",
      "Epoch 467/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 229.1831 - activation_23_loss: 99.9789 - dense_784_loss: 62.1488 - dense_788_loss: 67.0554 - val_loss: 216.6862 - val_activation_23_loss: 99.9998 - val_dense_784_loss: 51.9784 - val_dense_788_loss: 64.7080\n",
      "Epoch 468/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 246.3481 - activation_23_loss: 99.9825 - dense_784_loss: 62.3930 - dense_788_loss: 83.9726 - val_loss: 216.8165 - val_activation_23_loss: 99.9998 - val_dense_784_loss: 51.8660 - val_dense_788_loss: 64.9507\n",
      "Epoch 469/1000\n",
      "317/317 [==============================] - 0s 168us/step - loss: 235.7321 - activation_23_loss: 99.9947 - dense_784_loss: 67.9688 - dense_788_loss: 67.7687 - val_loss: 216.8633 - val_activation_23_loss: 99.9998 - val_dense_784_loss: 52.0986 - val_dense_788_loss: 64.7649\n",
      "Epoch 470/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 238.6478 - activation_23_loss: 99.9954 - dense_784_loss: 68.1663 - dense_788_loss: 70.4860 - val_loss: 216.4472 - val_activation_23_loss: 99.9998 - val_dense_784_loss: 52.1473 - val_dense_788_loss: 64.3000\n",
      "Epoch 471/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 228.9849 - activation_23_loss: 99.9939 - dense_784_loss: 63.7012 - dense_788_loss: 65.2898 - val_loss: 215.5060 - val_activation_23_loss: 99.9998 - val_dense_784_loss: 51.8656 - val_dense_788_loss: 63.6406\n",
      "Epoch 472/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 228.9109 - activation_23_loss: 99.9952 - dense_784_loss: 62.3117 - dense_788_loss: 66.6039 - val_loss: 214.8270 - val_activation_23_loss: 99.9998 - val_dense_784_loss: 51.9540 - val_dense_788_loss: 62.8733\n",
      "Epoch 473/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 232.1057 - activation_23_loss: 99.9938 - dense_784_loss: 63.8120 - dense_788_loss: 68.2999 - val_loss: 214.3970 - val_activation_23_loss: 99.9997 - val_dense_784_loss: 52.1550 - val_dense_788_loss: 62.2423\n",
      "Epoch 474/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 228.6657 - activation_23_loss: 99.9936 - dense_784_loss: 63.2634 - dense_788_loss: 65.4087 - val_loss: 213.8756 - val_activation_23_loss: 99.9997 - val_dense_784_loss: 52.1057 - val_dense_788_loss: 61.7703\n",
      "Epoch 475/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 229.8159 - activation_23_loss: 99.9880 - dense_784_loss: 64.2859 - dense_788_loss: 65.5420 - val_loss: 213.6193 - val_activation_23_loss: 99.9997 - val_dense_784_loss: 52.1022 - val_dense_788_loss: 61.5174\n",
      "Epoch 476/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 245.6825 - activation_23_loss: 99.9820 - dense_784_loss: 65.9801 - dense_788_loss: 79.7203 - val_loss: 214.1625 - val_activation_23_loss: 99.9997 - val_dense_784_loss: 52.3566 - val_dense_788_loss: 61.8063\n",
      "Epoch 477/1000\n",
      "317/317 [==============================] - 0s 164us/step - loss: 239.5888 - activation_23_loss: 99.9923 - dense_784_loss: 64.6182 - dense_788_loss: 74.9782 - val_loss: 214.8635 - val_activation_23_loss: 99.9997 - val_dense_784_loss: 52.3950 - val_dense_788_loss: 62.4688\n",
      "Epoch 478/1000\n",
      "317/317 [==============================] - 0s 161us/step - loss: 238.6456 - activation_23_loss: 99.9892 - dense_784_loss: 64.2926 - dense_788_loss: 74.3638 - val_loss: 214.5366 - val_activation_23_loss: 99.9997 - val_dense_784_loss: 51.9686 - val_dense_788_loss: 62.5683\n",
      "Epoch 479/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 241.3773 - activation_23_loss: 99.9901 - dense_784_loss: 69.9678 - dense_788_loss: 71.4194 - val_loss: 215.4842 - val_activation_23_loss: 99.9997 - val_dense_784_loss: 52.3963 - val_dense_788_loss: 63.0883\n",
      "Epoch 480/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 240.0885 - activation_23_loss: 99.9838 - dense_784_loss: 72.5564 - dense_788_loss: 67.5483 - val_loss: 216.6283 - val_activation_23_loss: 99.9997 - val_dense_784_loss: 53.2199 - val_dense_788_loss: 63.4088\n",
      "Epoch 481/1000\n",
      "317/317 [==============================] - ETA: 0s - loss: 221.9243 - activation_23_loss: 99.9973 - dense_784_loss: 62.3238 - dense_788_loss: 59.603 - 0s 155us/step - loss: 238.8819 - activation_23_loss: 99.9952 - dense_784_loss: 66.8631 - dense_788_loss: 72.0236 - val_loss: 216.4517 - val_activation_23_loss: 99.9997 - val_dense_784_loss: 53.4275 - val_dense_788_loss: 63.0246\n",
      "Epoch 482/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 236.9903 - activation_23_loss: 99.9916 - dense_784_loss: 64.8321 - dense_788_loss: 72.1666 - val_loss: 216.5267 - val_activation_23_loss: 99.9996 - val_dense_784_loss: 53.5175 - val_dense_788_loss: 63.0096\n",
      "Epoch 483/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 244.1720 - activation_23_loss: 99.9847 - dense_784_loss: 63.7817 - dense_788_loss: 80.4055 - val_loss: 216.8293 - val_activation_23_loss: 99.9996 - val_dense_784_loss: 53.3376 - val_dense_788_loss: 63.4920\n",
      "Epoch 484/1000\n",
      "317/317 [==============================] - 0s 161us/step - loss: 235.4059 - activation_23_loss: 99.8911 - dense_784_loss: 66.3567 - dense_788_loss: 69.1581 - val_loss: 216.8144 - val_activation_23_loss: 99.9997 - val_dense_784_loss: 53.1777 - val_dense_788_loss: 63.6371\n",
      "Epoch 485/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 236.1908 - activation_23_loss: 99.9801 - dense_784_loss: 71.3622 - dense_788_loss: 64.8485 - val_loss: 216.9434 - val_activation_23_loss: 99.9997 - val_dense_784_loss: 53.3359 - val_dense_788_loss: 63.6078\n",
      "Epoch 486/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 241.6160 - activation_23_loss: 99.9841 - dense_784_loss: 74.1931 - dense_788_loss: 67.4387 - val_loss: 216.3929 - val_activation_23_loss: 99.9997 - val_dense_784_loss: 53.1618 - val_dense_788_loss: 63.2314\n",
      "Epoch 487/1000\n",
      "317/317 [==============================] - 0s 161us/step - loss: 228.5331 - activation_23_loss: 99.9888 - dense_784_loss: 64.0441 - dense_788_loss: 64.5002 - val_loss: 216.2282 - val_activation_23_loss: 99.9997 - val_dense_784_loss: 53.3096 - val_dense_788_loss: 62.9189\n",
      "Epoch 488/1000\n",
      "317/317 [==============================] - 0s 161us/step - loss: 232.3351 - activation_23_loss: 99.9835 - dense_784_loss: 62.3958 - dense_788_loss: 69.9558 - val_loss: 215.7926 - val_activation_23_loss: 99.9997 - val_dense_784_loss: 53.0646 - val_dense_788_loss: 62.7283\n",
      "Epoch 489/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "317/317 [==============================] - 0s 157us/step - loss: 233.1585 - activation_23_loss: 99.9916 - dense_784_loss: 67.7782 - dense_788_loss: 65.3887 - val_loss: 215.2487 - val_activation_23_loss: 99.9997 - val_dense_784_loss: 52.8263 - val_dense_788_loss: 62.4227\n",
      "Epoch 490/1000\n",
      "317/317 [==============================] - 0s 166us/step - loss: 236.7236 - activation_23_loss: 99.9650 - dense_784_loss: 61.0242 - dense_788_loss: 75.7343 - val_loss: 214.8240 - val_activation_23_loss: 99.9997 - val_dense_784_loss: 52.4542 - val_dense_788_loss: 62.3701\n",
      "Epoch 491/1000\n",
      "317/317 [==============================] - 0s 160us/step - loss: 235.0841 - activation_23_loss: 99.9735 - dense_784_loss: 60.0582 - dense_788_loss: 75.0524 - val_loss: 214.8566 - val_activation_23_loss: 99.9997 - val_dense_784_loss: 52.2122 - val_dense_788_loss: 62.6447\n",
      "Epoch 492/1000\n",
      "317/317 [==============================] - 0s 157us/step - loss: 228.0889 - activation_23_loss: 99.9721 - dense_784_loss: 63.3967 - dense_788_loss: 64.7201 - val_loss: 214.6325 - val_activation_23_loss: 99.9996 - val_dense_784_loss: 52.1412 - val_dense_788_loss: 62.4917\n",
      "Epoch 493/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 239.0016 - activation_23_loss: 99.9499 - dense_784_loss: 62.0544 - dense_788_loss: 76.9972 - val_loss: 214.7486 - val_activation_23_loss: 99.9996 - val_dense_784_loss: 52.2361 - val_dense_788_loss: 62.5129\n",
      "Epoch 494/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 235.6367 - activation_23_loss: 99.9853 - dense_784_loss: 65.5664 - dense_788_loss: 70.0851 - val_loss: 214.5483 - val_activation_23_loss: 99.9995 - val_dense_784_loss: 52.1960 - val_dense_788_loss: 62.3528\n",
      "Epoch 495/1000\n",
      "317/317 [==============================] - 0s 160us/step - loss: 237.0304 - activation_23_loss: 99.9932 - dense_784_loss: 61.7204 - dense_788_loss: 75.3167 - val_loss: 214.3920 - val_activation_23_loss: 99.9995 - val_dense_784_loss: 52.0493 - val_dense_788_loss: 62.3432\n",
      "Epoch 496/1000\n",
      "317/317 [==============================] - 0s 157us/step - loss: 235.1566 - activation_23_loss: 99.9931 - dense_784_loss: 61.4738 - dense_788_loss: 73.6897 - val_loss: 214.8461 - val_activation_23_loss: 99.9995 - val_dense_784_loss: 52.0193 - val_dense_788_loss: 62.8273\n",
      "Epoch 497/1000\n",
      "317/317 [==============================] - 0s 160us/step - loss: 237.7189 - activation_23_loss: 99.9924 - dense_784_loss: 62.8237 - dense_788_loss: 74.9028 - val_loss: 215.1106 - val_activation_23_loss: 99.9995 - val_dense_784_loss: 51.9354 - val_dense_788_loss: 63.1757\n",
      "Epoch 498/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 236.8150 - activation_23_loss: 99.9946 - dense_784_loss: 68.6825 - dense_788_loss: 68.1379 - val_loss: 215.0551 - val_activation_23_loss: 99.9995 - val_dense_784_loss: 52.0804 - val_dense_788_loss: 62.9752\n",
      "Epoch 499/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 234.9822 - activation_23_loss: 99.9805 - dense_784_loss: 66.1924 - dense_788_loss: 68.8094 - val_loss: 215.2077 - val_activation_23_loss: 99.9994 - val_dense_784_loss: 52.4693 - val_dense_788_loss: 62.7389\n",
      "Epoch 500/1000\n",
      "317/317 [==============================] - 0s 152us/step - loss: 242.2583 - activation_23_loss: 99.9770 - dense_784_loss: 63.2656 - dense_788_loss: 79.0156 - val_loss: 215.7264 - val_activation_23_loss: 99.9994 - val_dense_784_loss: 52.9005 - val_dense_788_loss: 62.8265\n",
      "Epoch 501/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 234.5992 - activation_23_loss: 99.9814 - dense_784_loss: 65.3857 - dense_788_loss: 69.2321 - val_loss: 215.8859 - val_activation_23_loss: 99.9994 - val_dense_784_loss: 52.9997 - val_dense_788_loss: 62.8868\n",
      "Epoch 502/1000\n",
      "317/317 [==============================] - 0s 157us/step - loss: 234.7696 - activation_23_loss: 99.9022 - dense_784_loss: 67.3852 - dense_788_loss: 67.4822 - val_loss: 216.0099 - val_activation_23_loss: 99.9993 - val_dense_784_loss: 52.9955 - val_dense_788_loss: 63.0151\n",
      "Epoch 503/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 233.0120 - activation_23_loss: 99.9599 - dense_784_loss: 63.4246 - dense_788_loss: 69.6275 - val_loss: 216.0466 - val_activation_23_loss: 99.9992 - val_dense_784_loss: 53.1109 - val_dense_788_loss: 62.9366\n",
      "Epoch 504/1000\n",
      "317/317 [==============================] - 0s 157us/step - loss: 232.8784 - activation_23_loss: 99.9801 - dense_784_loss: 62.1765 - dense_788_loss: 70.7218 - val_loss: 216.3369 - val_activation_23_loss: 99.9991 - val_dense_784_loss: 53.4869 - val_dense_788_loss: 62.8509\n",
      "Epoch 505/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 235.6954 - activation_23_loss: 99.9897 - dense_784_loss: 61.6326 - dense_788_loss: 74.0731 - val_loss: 216.6842 - val_activation_23_loss: 99.9991 - val_dense_784_loss: 53.6206 - val_dense_788_loss: 63.0645\n",
      "Epoch 506/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 231.5318 - activation_23_loss: 99.9913 - dense_784_loss: 64.0697 - dense_788_loss: 67.4708 - val_loss: 216.5722 - val_activation_23_loss: 99.9991 - val_dense_784_loss: 53.5101 - val_dense_788_loss: 63.0630\n",
      "Epoch 507/1000\n",
      "317/317 [==============================] - 0s 157us/step - loss: 238.4931 - activation_23_loss: 99.9861 - dense_784_loss: 62.4252 - dense_788_loss: 76.0817 - val_loss: 216.6652 - val_activation_23_loss: 99.9991 - val_dense_784_loss: 53.3117 - val_dense_788_loss: 63.3545\n",
      "Epoch 508/1000\n",
      "317/317 [==============================] - 0s 160us/step - loss: 236.1427 - activation_23_loss: 99.9901 - dense_784_loss: 67.0118 - dense_788_loss: 69.1408 - val_loss: 216.5095 - val_activation_23_loss: 99.9990 - val_dense_784_loss: 53.0198 - val_dense_788_loss: 63.4906\n",
      "Epoch 509/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 234.8052 - activation_23_loss: 99.9818 - dense_784_loss: 63.6379 - dense_788_loss: 71.1855 - val_loss: 216.0891 - val_activation_23_loss: 99.9990 - val_dense_784_loss: 52.7026 - val_dense_788_loss: 63.3874\n",
      "Epoch 510/1000\n",
      "317/317 [==============================] - 0s 163us/step - loss: 233.6019 - activation_23_loss: 99.9896 - dense_784_loss: 62.1106 - dense_788_loss: 71.5018 - val_loss: 215.6431 - val_activation_23_loss: 99.9990 - val_dense_784_loss: 52.5851 - val_dense_788_loss: 63.0590\n",
      "Epoch 511/1000\n",
      "317/317 [==============================] - 0s 164us/step - loss: 226.1401 - activation_23_loss: 100.1132 - dense_784_loss: 63.1248 - dense_788_loss: 62.9021 - val_loss: 215.3923 - val_activation_23_loss: 99.9990 - val_dense_784_loss: 52.5723 - val_dense_788_loss: 62.8210\n",
      "Epoch 512/1000\n",
      "317/317 [==============================] - 0s 152us/step - loss: 235.5978 - activation_23_loss: 99.9662 - dense_784_loss: 63.8834 - dense_788_loss: 71.7482 - val_loss: 214.7929 - val_activation_23_loss: 99.9989 - val_dense_784_loss: 52.3953 - val_dense_788_loss: 62.3987\n",
      "Epoch 513/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 231.2315 - activation_23_loss: 99.9917 - dense_784_loss: 63.1261 - dense_788_loss: 68.1136 - val_loss: 214.9434 - val_activation_23_loss: 99.9989 - val_dense_784_loss: 52.2792 - val_dense_788_loss: 62.6653\n",
      "Epoch 514/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 228.0742 - activation_23_loss: 99.9798 - dense_784_loss: 59.8349 - dense_788_loss: 68.2595 - val_loss: 214.5902 - val_activation_23_loss: 99.9988 - val_dense_784_loss: 52.0573 - val_dense_788_loss: 62.5340\n",
      "Epoch 515/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 236.1313 - activation_23_loss: 99.9736 - dense_784_loss: 64.0512 - dense_788_loss: 72.1065 - val_loss: 214.5817 - val_activation_23_loss: 99.9988 - val_dense_784_loss: 52.0885 - val_dense_788_loss: 62.4944\n",
      "Epoch 516/1000\n",
      "317/317 [==============================] - 0s 164us/step - loss: 236.5932 - activation_23_loss: 99.9695 - dense_784_loss: 61.9883 - dense_788_loss: 74.6354 - val_loss: 214.6587 - val_activation_23_loss: 99.9988 - val_dense_784_loss: 52.1518 - val_dense_788_loss: 62.5082\n",
      "Epoch 517/1000\n",
      "317/317 [==============================] - 0s 161us/step - loss: 233.0341 - activation_23_loss: 99.9898 - dense_784_loss: 63.7276 - dense_788_loss: 69.3167 - val_loss: 214.8943 - val_activation_23_loss: 99.9988 - val_dense_784_loss: 52.1110 - val_dense_788_loss: 62.7845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 518/1000\n",
      "317/317 [==============================] - 0s 161us/step - loss: 236.8342 - activation_23_loss: 99.9636 - dense_784_loss: 62.9235 - dense_788_loss: 73.9471 - val_loss: 215.2578 - val_activation_23_loss: 99.9987 - val_dense_784_loss: 52.2219 - val_dense_788_loss: 63.0372\n",
      "Epoch 519/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 232.6599 - activation_23_loss: 99.9810 - dense_784_loss: 66.1876 - dense_788_loss: 66.4913 - val_loss: 215.9401 - val_activation_23_loss: 99.9987 - val_dense_784_loss: 52.5290 - val_dense_788_loss: 63.4124\n",
      "Epoch 520/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 231.1129 - activation_23_loss: 99.9885 - dense_784_loss: 65.4180 - dense_788_loss: 65.7064 - val_loss: 215.6355 - val_activation_23_loss: 99.9987 - val_dense_784_loss: 52.4008 - val_dense_788_loss: 63.2361\n",
      "Epoch 521/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 233.8528 - activation_23_loss: 99.9885 - dense_784_loss: 61.4188 - dense_788_loss: 72.4455 - val_loss: 215.3140 - val_activation_23_loss: 99.9987 - val_dense_784_loss: 52.1633 - val_dense_788_loss: 63.1521\n",
      "Epoch 522/1000\n",
      "317/317 [==============================] - 0s 155us/step - loss: 226.8656 - activation_23_loss: 99.9830 - dense_784_loss: 64.3111 - dense_788_loss: 62.5715 - val_loss: 215.3687 - val_activation_23_loss: 99.9987 - val_dense_784_loss: 52.3775 - val_dense_788_loss: 62.9925\n",
      "Epoch 523/1000\n",
      "317/317 [==============================] - 0s 158us/step - loss: 231.7658 - activation_23_loss: 100.0178 - dense_784_loss: 62.9794 - dense_788_loss: 68.7686 - val_loss: 215.3485 - val_activation_23_loss: 99.9986 - val_dense_784_loss: 52.5067 - val_dense_788_loss: 62.8432\n",
      "Epoch 524/1000\n",
      "317/317 [==============================] - 0s 149us/step - loss: 228.5383 - activation_23_loss: 99.9847 - dense_784_loss: 60.8065 - dense_788_loss: 67.7472 - val_loss: 215.1667 - val_activation_23_loss: 99.9986 - val_dense_784_loss: 52.2900 - val_dense_788_loss: 62.8781\n",
      "Epoch 525/1000\n",
      "317/317 [==============================] - 0s 161us/step - loss: 234.4769 - activation_23_loss: 99.9887 - dense_784_loss: 64.3854 - dense_788_loss: 70.1028 - val_loss: 215.0849 - val_activation_23_loss: 99.9985 - val_dense_784_loss: 52.1146 - val_dense_788_loss: 62.9718\n",
      "Epoch 00525: early stopping\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "reg = None # l1_l2(0.01,0.02)\n",
    "\n",
    "stack_layers = lambda layers: reduce(lambda stack, e: e(stack), layers)\n",
    "\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "def exp_act(x):\n",
    "    return (keras.backend.exp(x)-1)\n",
    "\n",
    "get_custom_objects().update({'custom_activation': Activation(exp_act)})\n",
    "\n",
    "weather_layers = \\\n",
    "  [Input((3,))] \\\n",
    "+ [Dense(8, activation='relu',kernel_regularizer=reg)] \\\n",
    "+ [Dropout(0.5)] \\\n",
    "+ [Dense(10, activation='relu',kernel_regularizer=reg)] \\\n",
    "+ [Dropout(0.5)] \\\n",
    "+ [Dense(7, activation='relu',kernel_regularizer=reg)] \\\n",
    "+ [Dropout(0.5)] \\\n",
    "+ [Dense(4, activation='relu',kernel_regularizer=reg)]\n",
    "\n",
    "weather_out = stack_layers(weather_layers)\n",
    "\n",
    "day_layers = \\\n",
    "  [Input((7,))] \\\n",
    "+ [Dense(10, activation='relu',kernel_regularizer=reg)] \\\n",
    "+ [Dropout(0.5)] \\\n",
    "+ [Dense(10, activation='relu',kernel_regularizer=reg)] \\\n",
    "+ [Dropout(0.5)] \\\n",
    "+ [Dense(10, activation='relu',kernel_regularizer=reg)] \\\n",
    "+ [Dropout(0.5)] \\\n",
    "+ [Dense(4, activation='relu',kernel_regularizer=reg)]\n",
    "\n",
    "day_out = stack_layers(day_layers)\n",
    "\n",
    "date_layers = \\\n",
    "  [Input((1,))] \\\n",
    "+ [Dense(10, activation='relu',kernel_regularizer=reg)] \\\n",
    "+ [Dropout(0.5)] \\\n",
    "+ [Dense(20, activation='relu',kernel_regularizer=reg)] \\\n",
    "+ [Dropout(0.5)] \\\n",
    "+ [Dense(15, activation='relu',kernel_regularizer=reg)] \\\n",
    "+ [Dropout(0.5)] \\\n",
    "+ [Dense(5, activation='relu',kernel_regularizer=reg)]\n",
    "\n",
    "date_out = stack_layers(date_layers)\n",
    "\n",
    "main_layers = \\\n",
    "  [concatenate([weather_out, day_out, date_out])] \\\n",
    "+ [Dense(50, activation='relu',kernel_regularizer=reg)] \\\n",
    "+ [Dropout(0.5)] \\\n",
    "+ [Dense(50, activation='relu',kernel_regularizer=reg)] \\\n",
    "+ [Dropout(0.5)] \\\n",
    "+ [Dense(50, activation='relu',kernel_regularizer=reg)] \\\n",
    "+ [Dropout(0.5)] \\\n",
    "+ [Dense(50, activation='relu',kernel_regularizer=reg)] \\\n",
    "+ [Dropout(0.5)] \\\n",
    "+ [Dense(50, activation='relu',kernel_regularizer=reg)] \\\n",
    "+ [Dropout(0.5)]\n",
    "\n",
    "main_out = stack_layers(main_layers)\n",
    "\n",
    "usage_layers = \\\n",
    "  [main_out] \\\n",
    "+ [Dense(40, activation='relu',kernel_regularizer=reg)] \\\n",
    "+ [Dropout(0.5)] \\\n",
    "+ [Dense(20, activation='relu',kernel_regularizer=reg)] \\\n",
    "+ [Dropout(0.5)] \\\n",
    "+ [Dense(12, activation='relu',kernel_regularizer=reg)] \\\n",
    "+ [Dropout(0.5)] \\\n",
    "+ [Dense(8, activation='relu',kernel_regularizer=reg)] \\\n",
    "+ [Dropout(0.5)] \\\n",
    "+ [Dense(6, activation='relu',kernel_regularizer=reg)] \\\n",
    "+ [Activation(exp_act)]\n",
    "\n",
    "usage_out=stack_layers(usage_layers)\n",
    "\n",
    "clients_layers = \\\n",
    "  [main_out] \\\n",
    "+ [Dense(25, activation='relu',kernel_regularizer=reg)] \\\n",
    "+ [Dropout(0.5)] \\\n",
    "+ [Dense(10, activation='relu',kernel_regularizer=reg)] \\\n",
    "+ [Dropout(0.5)] \\\n",
    "+ [Dense(5, activation='relu',kernel_regularizer=reg)] \\\n",
    "+ [Dropout(0.5)] \\\n",
    "+ [Dense(1, activation='relu',kernel_regularizer=reg)]\n",
    "\n",
    "clients_out = stack_layers(clients_layers)\n",
    "\n",
    "sessions_layers = \\\n",
    "  [main_out] \\\n",
    "+ [Dense(25, activation='relu',kernel_regularizer=reg)] \\\n",
    "+ [Dropout(0.5)] \\\n",
    "+ [Dense(10, activation='relu',kernel_regularizer=reg)] \\\n",
    "+ [Dropout(0.5)] \\\n",
    "+ [Dense(5, activation='relu',kernel_regularizer=reg)] \\\n",
    "+ [Dropout(0.5)] \\\n",
    "+ [Dense(1, activation='relu',kernel_regularizer=reg)]\n",
    "\n",
    "sessions_out = stack_layers(sessions_layers)\n",
    "\n",
    "model = Model(inputs=[weather_layers[0], day_layers[0], date_layers[0]], outputs=[usage_out,clients_out,sessions_out])\n",
    "model.compile(loss='mean_absolute_percentage_error',\n",
    "              optimizer=Adam(0.001))\n",
    "\n",
    "model.summary()\n",
    "print(usage_y.shape)\n",
    "print(clients_y.shape)\n",
    "print(sessions_y.shape)\n",
    "early_stopping = EarlyStopping(patience=50, verbose=1)\n",
    "history = model.fit([weather_x,day_x,date_x], [usage_y,clients_y,sessions_y],\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1,\n",
<<<<<<< Updated upstream
    "                    callbacks=[early_stopping])\n",
    "model_path = os.path.join('..','models','model.h5')\n",
    "model.save(model_path)"
=======
    "                    callbacks=[early_stopping])\n"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36, 3)\n",
      "36/36 [==============================] - 0s 195us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[-8.971651 , -9.       , -8.804964 , -8.374179 , -8.889538 ,\n",
       "         -8.375239 ],\n",
       "        [-8.976203 , -9.       , -8.780501 , -8.358985 , -8.855598 ,\n",
       "         -8.3443365],\n",
       "        [-8.975781 , -9.       , -8.786451 , -8.361496 , -8.863133 ,\n",
       "         -8.350464 ],\n",
       "        [-8.977427 , -9.       , -8.79532  , -8.368809 , -8.871242 ,\n",
       "         -8.359883 ],\n",
       "        [-8.972646 , -9.       , -8.804157 , -8.37426  , -8.887315 ,\n",
       "         -8.373688 ],\n",
       "        [-8.977157 , -9.       , -8.791334 , -8.365446 , -8.867083 ,\n",
       "         -8.355315 ],\n",
       "        [-8.975045 , -9.       , -8.790389 , -8.36142  , -8.868731 ,\n",
       "         -8.353901 ],\n",
       "        [-8.97577  , -9.       , -8.801188 , -8.372827 , -8.880424 ,\n",
       "         -8.367863 ],\n",
       "        [-8.975518 , -9.       , -8.792377 , -8.364122 , -8.870529 ,\n",
       "         -8.356624 ],\n",
       "        [-8.973645 , -9.       , -8.795862 , -8.360792 , -8.880167 ,\n",
       "         -8.360992 ],\n",
       "        [-8.975945 , -9.       , -8.7921095, -8.364733 , -8.870383 ,\n",
       "         -8.356981 ],\n",
       "        [-8.974399 , -9.       , -8.796072 , -8.363782 , -8.877749 ,\n",
       "         -8.3610735],\n",
       "        [-8.974521 , -9.       , -8.803354 , -8.377274 , -8.882136 ,\n",
       "         -8.372132 ],\n",
       "        [-8.975443 , -9.       , -8.785878 , -8.358799 , -8.862155 ,\n",
       "         -8.348241 ],\n",
       "        [-8.975679 , -9.       , -8.798931 , -8.369941 , -8.878457 ,\n",
       "         -8.365032 ],\n",
       "        [-8.973361 , -9.       , -8.798577 , -8.363541 , -8.8837   ,\n",
       "         -8.364774 ],\n",
       "        [-8.975634 , -9.       , -8.788962 , -8.361841 , -8.866381 ,\n",
       "         -8.352713 ],\n",
       "        [-8.977152 , -9.       , -8.794842 , -8.368636 , -8.870925 ,\n",
       "         -8.359545 ],\n",
       "        [-8.976057 , -9.       , -8.793375 , -8.364858 , -8.871702 ,\n",
       "         -8.357899 ],\n",
       "        [-8.976999 , -9.       , -8.796577 , -8.369472 , -8.873353 ,\n",
       "         -8.36157  ],\n",
       "        [-8.971291 , -9.       , -8.802998 , -8.370332 , -8.888911 ,\n",
       "         -8.372621 ],\n",
       "        [-8.978086 , -9.       , -8.787718 , -8.365404 , -8.8618145,\n",
       "         -8.352069 ],\n",
       "        [-8.97253  , -9.       , -8.804351 , -8.374442 , -8.887714 ,\n",
       "         -8.374098 ],\n",
       "        [-8.9761505, -9.       , -8.801765 , -8.374605 , -8.879889 ,\n",
       "         -8.36859  ],\n",
       "        [-8.977777 , -9.       , -8.788418 , -8.365055 , -8.863296 ,\n",
       "         -8.352802 ],\n",
       "        [-8.977598 , -9.       , -8.786173 , -8.363422 , -8.860731 ,\n",
       "         -8.350235 ],\n",
       "        [-8.972576 , -9.       , -8.806198 , -8.37719  , -8.888699 ,\n",
       "         -8.37638  ],\n",
       "        [-8.972729 , -9.       , -8.799532 , -8.365334 , -8.884061 ,\n",
       "         -8.366027 ],\n",
       "        [-8.973766 , -9.       , -8.796871 , -8.363371 , -8.879744 ,\n",
       "         -8.362067 ],\n",
       "        [-8.97303  , -9.       , -8.800598 , -8.368142 , -8.884165 ,\n",
       "         -8.367751 ],\n",
       "        [-8.975499 , -9.       , -8.7878475, -8.361884 , -8.865115 ,\n",
       "         -8.351904 ],\n",
       "        [-8.975153 , -9.       , -8.786297 , -8.360319 , -8.863588 ,\n",
       "         -8.350014 ],\n",
       "        [-8.9758   , -9.       , -8.7862625, -8.361499 , -8.862899 ,\n",
       "         -8.350319 ],\n",
       "        [-8.974514 , -9.       , -8.791888 , -8.360659 , -8.87322  ,\n",
       "         -8.356466 ],\n",
       "        [-8.9753685, -9.       , -8.785988 , -8.358983 , -8.862498 ,\n",
       "         -8.348564 ],\n",
       "        [-8.973021 , -9.       , -8.800677 , -8.36828  , -8.884223 ,\n",
       "         -8.367873 ]], dtype=float32), array([[62.012764],\n",
       "        [61.977333],\n",
       "        [61.964417],\n",
       "        [61.966927],\n",
       "        [62.033043],\n",
       "        [62.014366],\n",
       "        [62.01693 ],\n",
       "        [62.016907],\n",
       "        [62.011345],\n",
       "        [62.059273],\n",
       "        [62.045876],\n",
       "        [62.056637],\n",
       "        [61.977253],\n",
       "        [61.96432 ],\n",
       "        [62.00969 ],\n",
       "        [62.047447],\n",
       "        [61.985386],\n",
       "        [61.94463 ],\n",
       "        [62.04527 ],\n",
       "        [61.949394],\n",
       "        [62.030468],\n",
       "        [61.963276],\n",
       "        [62.04579 ],\n",
       "        [61.988327],\n",
       "        [61.96997 ],\n",
       "        [61.95304 ],\n",
       "        [61.996075],\n",
       "        [62.055794],\n",
       "        [62.044785],\n",
       "        [62.06061 ],\n",
       "        [61.95571 ],\n",
       "        [61.961914],\n",
       "        [61.96078 ],\n",
       "        [62.048004],\n",
       "        [61.963463],\n",
       "        [62.06022 ]], dtype=float32), array([[373.38037],\n",
       "        [382.76978],\n",
       "        [380.77298],\n",
       "        [377.95654],\n",
       "        [374.12604],\n",
       "        [378.6355 ],\n",
       "        [378.91785],\n",
       "        [375.8569 ],\n",
       "        [378.51117],\n",
       "        [375.50937],\n",
       "        [378.11786],\n",
       "        [376.61813],\n",
       "        [376.10696],\n",
       "        [381.63107],\n",
       "        [376.21887],\n",
       "        [374.8438 ],\n",
       "        [379.66895],\n",
       "        [377.97223],\n",
       "        [377.67813],\n",
       "        [377.14874],\n",
       "        [373.86276],\n",
       "        [379.6568 ],\n",
       "        [373.96826],\n",
       "        [376.14923],\n",
       "        [379.60086],\n",
       "        [380.36917],\n",
       "        [373.36526],\n",
       "        [375.02325],\n",
       "        [376.3531 ],\n",
       "        [375.03528],\n",
       "        [380.19067],\n",
       "        [380.844  ],\n",
       "        [380.8099 ],\n",
       "        [377.30795],\n",
       "        [381.48596],\n",
       "        [375.0308 ]], dtype=float32)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evalSeg=317\n",
    "print (weather_x[evalSeg:].shape)\n",
    "model.evaluate(x=[weather_x[evalSeg:],day_x[evalSeg:],date_x[evalSeg:]],y=[usage_y[evalSeg:],clients_y[evalSeg:],sessions_y[evalSeg:]])\n",
    "model.predict(x=[weather_x[evalSeg:],day_x[evalSeg:],date_x[evalSeg:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.71828183])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.eval(keras.backend.exp(np.array([1.0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
